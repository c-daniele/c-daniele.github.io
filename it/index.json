[{"content":"Intro Nel precedente articolo ho fatto una brevissima panoramica di LangChain, descrivendone i concetti principali e raccontando un esempio di caso d\u0026rsquo;uso con dati non strutturati in formato pdf.\nSeguendo lo stesso approccio, in questo articolo faremo una breve introduzione sugli Agenti e procederemo provando a rispondere ad una domanda ambiziosa:\nè possibile, tramite l\u0026rsquo;AI, fare analisi sui dati presenti in un DB senza alcuna conoscenza di SQL né tantomeno del modello dati, a partire semplicemente da un prompt testuale in lingua naturale?\nAgenti I LLM sono estremamente potenti, ma possono rivelarsi inefficaci nel rispondere a domande che richiedono una conoscenza di dettaglio non strettamente integrata nel training del modello. In rete esistono decine di esempi che riescono a cogliere in fallo ChatGPT tramite allucinazioni o mancata risposta (es: previsioni meteo, ultime notizie, gossip o anche operazioni matematiche particolari).\nI framework come LangChain possono superare queste limitazioni tramite la definizione di componenti specifici e \u0026ldquo;data-aware\u0026rdquo;, ma solitamente le azioni eseguite dal framework sono predeterminate. In altre parole, il framework utilizza un Language Model per eseguire delle azioni, ma esse sono \u0026ldquo;hardcoded\u0026rdquo; e in moltissimi casi questo può rendere del tutto inefficaci i modelli di AI, perché non si riesce ad aggiungere quel livello di dinamismo tale da pilotare le specifiche azioni sulla base dell\u0026rsquo;input utente.\nE\u0026rsquo; qui che entrano in gioco gli \u0026ldquo;Agent\u0026rdquo;. Gli Agent sono dei componenti che hanno a disposizione una serie di Tool per svolgere delle azioni specifiche, come ad esempio fare una ricerca su Wikipedia o su Google, o eseguire codice Python o addirittura accedere al file system locale.\nGli Agent utilizzano un LLM per interpretare l\u0026rsquo;input dell\u0026rsquo;utente e decidere di volta come procedere, cioè:\nQuale tool utilizzare tra gli N a disposizione Cosa passare come input al tool Decidere se si è riusciti ad ottenere una risposta al quesito iniziale oppure ripetere gli step 1 e 2 Questo approccio prende ispirazione da un framework denominato ReAct che è stato definito a fine 2022 da un team congiunto di ricercatori di Google e della Princeton University, che trovate descritto qui. In LangChain, ne esistono diverse implementazioni, ma la più comune prende il nome di \u0026ldquo;Zero-shot ReAct\u0026rdquo; e può essere schematizzata secondo il workflow in Figura 1.\nFigura 1 - Workflow semplificato per gli agenti di tipo \u0026#34;Zero-shot ReAct\u0026#34; Un aspetto particolarmente rilevante da tenere in considerazione è relativo al fatto che gli agenti di questo tipo non hanno memoria e discriminano le loro azioni unicamente sulla base del testo in input e della descrizione del tool. E\u0026rsquo; dunque importante che i tool includano anche una descrizione efficace ai fini di una corretta interpretazione da parte del LLM.\nPer semplicità, i tool di LangChain sono talvolta raggruppati in gruppi denominati \u0026ldquo;Toolkits\u0026rdquo;. Nella documentazione ufficiale troverete un toolkit predefinito che si chiama \u0026ldquo;SQLDatabaseToolkit\u0026rdquo;, per configurare appunto un agente SQL.\nLa descrizione dello scenario Come preannunciato all\u0026rsquo;inizio dell\u0026rsquo;articolo, vogliamo fare una vera e propria analisi in maniera automatica sui dati presenti in un DB relazionale, supponendo di non avere alcuna conoscenza del modello dati né tantomeno competenze SQL. Il punto di partenza sarà infatti un prompt testuale in lingua naturale.\nDa un punto di vista tecnico, l\u0026rsquo;esercizio è facilissimo perché, oltre al toolkit, LangChain mette a disposizione un metodo di utility per la definizione di un SqlAgent in cui dobbiamo passare solo alcuni parametri come i puntamenti al DB, il tipo di LLM, etc..\nA prima vista, gli esempi riportati nella documentazione ufficiale sembrano già molto interessanti. Oltre ai casi banali (es: DESCRIBE di una tabella), viene infatti mostrato come l\u0026rsquo;agente sia in grado di fare inferenza sui metadati per capire come aggregare i dati o mettere in JOIN 2 o più tabelle. Tutto ciò in totale autonomia.\nPer non ripetere lo stesso identico esempio presente nella documentazione ed introdurre qualche complicazione in più, decido di creare una versione potenziata del toolkit standard, che sia in grado di fare anche ricerche su Google.\nIl dataset La documentazione ufficiale include degli esempi che fanno uso di un DB di prova basato su SqlLite e denominato \u0026ldquo;Chinook\u0026rdquo;, che simula un media store e che trovate anche nel sito ufficiale di SqlLite.\nDando un occhio al modello dati e ai dati stessi, ho guardato con sospetto gli entusiasmanti risultati che hanno riportato, perché il DB è a mio avviso non è rappresentativo di un caso reale:\ni nomi delle tabelle e delle colonne sono tutti abbastanza parlanti ed in lingua inglese, inoltre non viene fatto uso di una naming convention il DB sembra praticamente in terza forma normale: improbabile laddove si voglia fare pura analisi dati file .db SqlLite in locale? Si tratta di un caso molto lontano dalla realtà! Fortunatamente ho a disposizione un DB Athena su un mio account AWS con alcune strutture dati più vicine ad un caso reale e di cui conosco un po\u0026rsquo; la semantica del dato. Si tratta di OpenData del Comune di Milano, relativi ai transiti all\u0026rsquo;interno dei varchi di AreaC. In realtà Athena non è un vero DB, quanto piuttosto un SQL-Engine basato su Presto, ma con le opportune configurazioni, AWS mette a disposizione un endpoint che permette di accedervi come se fosse un vero DBMS.\nIl Data Model è semplicissimo: si tratta di 2 tabelle dei fatti, che fanno riferimento al conteggio degli ingressi (dettaglio+aggregato), legate entrambe ad una tabella di decodifica dei varchi, in cui sono indicati alcuni attributi tra cui la posizione geografica esatta del varco. In tutti e 3 i casi, si tratta di tabelle Iceberg memorizzate su S3 e mappate su Athena tramite il catalogo di Glue.\nI dataset originari li trovate sul portale OpenData ufficiale. Si tratta di circa 4 anni di dati (circa 101 milioni di record nella tabella di cardinalità massima).\nDi seguito le DDL delle tabelle con qualche commento che ho aggiunto qui per semplicità (e che dunque l\u0026rsquo;agente non aveva a disposizione\u0026hellip;).\nFigura 2 - DDL tabella di dettaglio Figura 3 - DDL tabella aggregata Figura 2 - DDL tabella di decodifica dei varchi Nella tabella aggregata, oltre a rimuovere qualche attributo, ho fatto una sorta di pivot sul tipo di alimentazione, calcolando i diversi transiti in COLONNA anziché in RIGA, riducendo la cardinalità di circa il 92%. A parte questo, le 2 tabelle dei fatti sono praticamente identiche.\nLa tabella di decodifica dei varchi, oltre al nome descrittivo, contiene anche le coordinate geografiche.\nCome si può vedere, ho usato una naming convention, ma essa è volutamente imperfetta, ad esempio è un mix di inglese ed italiano.\nLa configurazione software Riporto di seguito gli import e le configurazioni di base del codice python:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 from langchain.agents import create_sql_agent from langchain.sql_database import SQLDatabase from langchain.llms.openai import OpenAI from langchain.agents.agent_types import AgentType import os from urllib.parse import quote_plus from ExtendedSqlDatabaseToolkit import * # carico le variabili di ambiente from dotenv import load_dotenv load_dotenv() # connection string conn_str = ( \u0026#34;awsathena+rest://{aws_access_key_id}:{aws_secret_access_key}@\u0026#34; \u0026#34;athena.{region_name}.amazonaws.com:443/\u0026#34; \u0026#34;{schema_name}?s3_staging_dir={s3_staging_dir}\u0026amp;work_group={wg}\u0026#34; ) # inizializzazione del DB db = SQLDatabase.from_uri(conn_str.format( aws_access_key_id=quote_plus(os.environ[\u0026#39;AWS_AK\u0026#39;]), aws_secret_access_key=quote_plus(os.environ[\u0026#39;AWS_SAK\u0026#39;]), region_name=os.environ[\u0026#39;AWS_REGION\u0026#39;], schema_name=os.environ[\u0026#39;AWS_ATHENA_SCHEMA\u0026#39;], s3_staging_dir=quote_plus(os.environ[\u0026#39;AWS_S3_OUT\u0026#39;]), wg=os.environ[\u0026#39;AWS_ATHENA_WG\u0026#39;] ) , include_tables=[\u0026#39;xtdpl1_ingressi_detailed\u0026#39;, \u0026#39;xtdpl1_ingressi_aggregated\u0026#39;, \u0026#39;xtdpl1_varchi\u0026#39;] , sample_rows_in_table_info=2) # definizione del toolkit tramite classe Custom toolkit = ExtendedSqlDatabaseToolkit(db=db, llm=OpenAI(temperature=0)) # inizializzazione dell\u0026#39;Agent agent_executor = create_sql_agent( llm=OpenAI(temperature=0), toolkit=toolkit, verbose=True, agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION ) LangChain utilizza SQLAlchemy quindi garantisce già l\u0026rsquo;accesso a un gran numero di DBMS senza la necessità di inventarsi nulla.\nDa notare che oltre alle variabili di ambiente relative ad AWS ed esplicitamente referenziate sopra, occorre anche settare le variabili:\nOPENAI_API_KEY: associata all\u0026rsquo;account OpenAI, essenziale per l\u0026rsquo;utilizzo del LLM SERPAPI_API_KEY: associata all\u0026rsquo;account SerpApi, al fine di fare programmaticamente ricerche su Google. Esiste una versione FREE che supporta un numero di chiamate mensili \u0026lt; 100 Le opzioni indicate in riga 29 e 30 servono per limitare il raggio d\u0026rsquo;azione dell\u0026rsquo;agente ed evitare che faccia ragionamenti troppo estesi su tutto il catalogo o su un sample di dati troppo ampio. Il rischio è infatti quello di saturare molto facilmente i token disponibili dal LLM.\nIl toolkit istanziato in riga 34 è una mia classe custom, che estende il SQLToolkit standard messo a disposizione da LangChain. Trattandosi di poche righe di codice, aggiungo anche questo:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 \u0026#34;\u0026#34;\u0026#34;Enhanced Toolkit for interacting with SQL databases and search over the internet\u0026#34;\u0026#34;\u0026#34; from typing import List from langchain.agents.agent_toolkits import SQLDatabaseToolkit from langchain.tools import BaseTool from langchain.agents import load_tools class ExtendedSqlDatabaseToolkit(SQLDatabaseToolkit): \u0026#34;\u0026#34;\u0026#34;Enhanced Toolkit for interacting with SQL databases and search over the internet\u0026#34;\u0026#34;\u0026#34; def get_tools(self) -\u0026gt; List[BaseTool]: sqlTools = super().get_tools() additionalTools = load_tools([\u0026#34;serpapi\u0026#34;], llm=self.llm) return additionalTools+sqlTools Oltre alle librerie esplicitamente referenziate, occorre anche installare le librerie \u0026ldquo;openai\u0026rdquo; e \u0026ldquo;pyathena\u0026rdquo;.\nLe challenges Ho sottoposto all\u0026rsquo;agente diverse domande, cercando di stressare le diverse componenti (es: capacità di individuare la semantica del dato, capire cosa cercare su google, quando/se andare nella tabella di dettaglio, etc etc).\nMi limiterò nel seguito a descrivere un paio di esempi, ma prima faccio qualche considerazione generale.\nIl modello standard utilizzato dalle librerie OpenAI è Text-davinci-003. Questo modello, è molto più ampio e più costoso (circa 10 volte di più!) di quello usato da ChatGPT (GPT-3.5-Turbo). Esiste molta letteratura che descrive l\u0026rsquo;efficacia di entrambi e di come il secondo, pur essendo sulla carta più piccolo (6 vs 175 miliardi di parametri), possa comunque avere risultati uguali o in alcuni casi addirittura migliori.\nPersonalmente ho usato quasi esclusivamente il primo dei 2 e le poche prove che ho fatto con GPT-3.5-Turbo hanno avuto risultati nettamente peggiori, ma non ho approfondito molto questo aspetto, a cui magari dedicherò un altro articolo.\nCaso A - Calcolo di un semplice KPI trova le coordinate e il nome descrittivo del varco in cui ci sono più transiti di veicoli diesel nel periodo di Agosto 2020. Considera gli ingressi effettivamente areac ed escludi i mezzi di servizio\nL\u0026rsquo;output restituito è indicato in Figura 3 e se date un occhio alle righe che iniziano per \u0026ldquo;Action\u0026rdquo;, \u0026ldquo;Observation\u0026rdquo; e \u0026ldquo;Thought\u0026rdquo;, vedrete che esso rispetta quanto previsto nel modello \u0026ldquo;Zero-shot ReAct\u0026rdquo;.\nFigura 3 - Output caso A In particolare, l\u0026rsquo;agente parte con l\u0026rsquo;identificazione della Action (sql_db_list_tables) e dell\u0026rsquo;input (nessun in input in questo caso), ottenendo (Observation) le 3 tabelle su cui abbiamo programmaticamente ristretto la sua visibilità. In teoria il tool potrebbe esplorare tutto il catalogo ma, come anticipato sopra, ho voluto restringere il campo per evitare di saturare i token messi a disposizione del modello.\nA questo punto l\u0026rsquo;agente passa il controllo al LLM (Thought) per decidere la prossima azione e tramite esso determina che le uniche 2 tabelle di interesse sono la tabella dei fatti aggregata e la tabella di decodifica dei varchi.\nE\u0026rsquo; interessante che già in questa fase abbia dedotto che sia meglio fare la query sulla tabella aggregata rispetto a quella di dettaglio, ma mi stranisce un po\u0026rsquo; il fatto che abbia fatto questa deduzione basandosi unicamente sulla naming della tabella, poiché l\u0026rsquo;inferenza sui metadati e sui dati viene fatta solo in un momento successivo. In tal senso, il risultato finale potrebbe non essere quello corretto qualora le 2 tabelle avessero avuto un perimetro dati diverso (ad esempio qualora la tabella aggregata contenesse solo l\u0026rsquo;ultimo anno).\nDopo aver letto i metadati e fatto un carotaggio dei dati, il LLM costruisce la query. In questo caso specifico si vede chiaramente che il modello indovina la sintassi della query al primo tentativo, ma ho sperimentato diversi casi in cui esso va a tentativi, correggendo di volta in volta la sintassi fino ad arrivare alla query definitiva (vedi casi B e C).\nIl resto è autodescritto nell\u0026rsquo;immagine.\nUn paio di commenti:\nil modello è riuscito a implementare perfettamente i filtri che avevo in mente nel prompt, tramite inferenza sulla naming e/o sui dati ho fatto altri tentativi rimuovendo la tabella aggregata e lasciando solo quella di dettaglio e ho ottenuto lo stesso risultato. Da notare però che la tabella di dettaglio ha il KPI in riga anziché in colonna, dunque in quel caso il modello ha capito che andava applicato il filtro \u0026ldquo;des_tipo_alimentazione = \u0026lsquo;diesel\u0026rsquo;\u0026rdquo; come da attese, non è stata fatta alcuna ricerca su google, perché ovviamente non serviva Caso B - informazioni aggiuntive trova il varco in cui ci sono più transiti di veicoli diesel nel periodo di Agosto 2020, includendo solo ingressi areac ed escludendo i mezzi di servizio. Restituiscimi anche i 3 varchi più vicini ad esso\nQui, il LLM mi ha sorpreso: ho aggiunto la frase finale per costringerlo a fare una ricerca su Google, ma non avevo pensato che partendo dalle coordinate geografiche fosse possibile calcolare la distanza con delle operazioni matematiche, dunque il tool (e cioè il modello LLM sottostante) ha eseguito l\u0026rsquo;intero task all\u0026rsquo;interno del DB tramite le funzioni ST_POINT ed ST_DISTANCE come mostrato in Figura 8.\nHo omesso la prima parte dell\u0026rsquo;output perché identica al caso precedente.\nFigura 4 - Output caso B Come si vede dai vari messaggi di errore, in questo caso il modello ha avuto diverse \u0026ldquo;allucinazioni\u0026rdquo; nella costruzione della query SQL, ma è riuscito a correggerli fino ad arrivare alla query definitiva perché l\u0026rsquo;agente ha restituito al modello LLM i feedback di tali errori tramite i loop Action-Observation-Thought.\nCaso C - esecuzione combinata SQL+Ricerca L\u0026rsquo;estrema semplicità del modello dati non mi ha aiutato molto nel creare una richiesta sufficientemente articolata, dunque ho dovuto fare un po\u0026rsquo; di prompt engineering per costringerlo a fare una ricerca sul web. Alla fine sono riuscito ad ottenere qualcosa con una richiesta di questo tipo:\ntrova le coordinate e il nome descrittivo del varco in cui ci sono più transiti di veicoli diesel nel mese di Agosto 2020. Cerca la fermata dei mezzi pubblici più vicina a quel varco\nQui sono accadute 2 cose strane:\nnonostante la prima parte del prompt fosse quasi identica al caso A (ho usato \u0026ldquo;nel mese di\u0026rdquo; anziché \u0026ldquo;nel periodo di\u0026rdquo;), il LLM esegue l\u0026rsquo;operazione di MAX anziché di SUM come da attese, l\u0026rsquo;agente ha eseguito la ricerca tramite SerpApi per individuare la fermata dei mezzi ma, anziché usare le coordinate, ha usato il nome descrittivo del varco. Il risultato chiaramente non è in linea con le aspettative, perché viene restituita una fermata dei mezzi della città di Venezia Figura 4 - Output caso C Conclusioni Come già ho scritto nel precedente articolo, la curva di apprendimento per adottare LangChain è piuttosto bassa. Bastano poche righe di codice per ottenere un effetto \u0026ldquo;wow\u0026rdquo; e consentire a chiunque di implementare una propria soluzione custom, magari integrata con il resto dell\u0026rsquo;ecosistema aziendale (repository documentali, Data APIs, mail server, shared file systems, \u0026hellip;) e/o con il proprio LLM (ad esempio, è possibile integrare una propria installazione di Llama 2 on-premise) laddove non si vogliano condividere dati al di fuori dell\u0026rsquo;organizzazione aziendale.\nD\u0026rsquo;altro canto, gli esempi che ho riportato sopra sono da considerarsi come tutorial semplificati per prendere dimestichezza con il framework.\nPer mettere a terra delle soluzioni reali, serve un approccio più strutturato, che sfrutti meglio le caratteristiche del framework e tenga conto delle peculiarità dei modelli.\nAd esempio, mi sono reso conto che non è stata una scelta saggia quella di unire le funzionalità SQL e di ricerca SerpApi in un unico toolkit e che sarebbe stato meglio integrare le 2 funzionalità tramite agent/chain separati.\nCome altro esempio, ho notato che nel pacchetto \u0026ldquo;experimental\u0026rdquo; è presente una classe che si chiama \u0026ldquo;SQLDatabaseChain\u0026rdquo; che con poche righe di codice permette di sviluppare un Tool Sql from scratch, bypassando completamente il toolkit standard:\n1 2 3 4 5 6 7 8 9 10 11 12 sql_chain = SQLDatabaseChain.from_llm(llm=llm, db=db, verbose=True) sql_tool = Tool( name=\u0026#39;Areac DB\u0026#39;, func=sql_chain.run, description=\u0026#34;Database che contiene i dati relativi agli ingressi nei varchi dell\u0026#39;area C di Milano.\u0026#34; \u0026#34; Le tabelle principali sono xtdpl1_ingressi_aggregated e xtdpl1_varchi.\u0026#34; \u0026#34; La tabella xtdpl1_ingressi_aggregated contiene le principali misure, come ad esempio il conteggio del numero di accessi per ciascuno dei varchi e per ciascun giorno dell\u0026#39;anno.\u0026#34; \u0026#34; Il campo relativo alla dimensione tempo si chiama dat_year_month ed è di tipo numerico, nel classico formato YYYYMM.\u0026#34; \u0026#34; Il campo flg_areac è di tipo BOOLEAN (true/false) ed indica se si tratta di un ingresso effettivamente conteggiato come areac.\u0026#34; \u0026#34; La tabella xtdpl1_varchi contiene la decodifica dei varchi. La chiave principale di questa tabella è il campo \u0026#39;id\u0026#39;, che identifica il varco. Gli altri attributi sono descrittivi.\u0026#34; ) Poiché l\u0026rsquo;agente utilizza il LLM per decidere QUALE tool utilizzare e COME utilizzarlo unicamente in base alla descrizione del tool, questo approccio ha il grande vantaggio di migliorare le performance semplicemente aggiungendo una descrizione efficace del DB all\u0026rsquo;interno del tool, senza modificare in alcun modo il modello LLM. Nel mio caso, ad esempio, ho aggiunto incrementalmente un gran numero di dettagli e ho notato un progressivo miglioramento delle performance.\n","permalink":"https://c-daniele.github.io/it/posts/2023-08-13-langchain-agents/","summary":"Intro Nel precedente articolo ho fatto una brevissima panoramica di LangChain, descrivendone i concetti principali e raccontando un esempio di caso d\u0026rsquo;uso con dati non strutturati in formato pdf.\nSeguendo lo stesso approccio, in questo articolo faremo una breve introduzione sugli Agenti e procederemo provando a rispondere ad una domanda ambiziosa:\nè possibile, tramite l\u0026rsquo;AI, fare analisi sui dati presenti in un DB senza alcuna conoscenza di SQL né tantomeno del modello dati, a partire semplicemente da un prompt testuale in lingua naturale?","title":"Langchain pt. 2 - Analisi dati tramite Agenti"},{"content":"Intro Per chi non lo conoscesse, LangChain è un framework per lo sviluppo di applicazioni che fanno uso di LLMs.\nCome si evince dal nome stesso, LangChain si basa sul concetto di Catena LLM, la quale combina 3 elementi:\nI Prompt Templates: fanno riferimento ad un modo riproducibile per generare un prompt. Contiene una stringa di testo (\u0026ldquo;il modello\u0026rdquo;), che può accettare una serie di parametri dall\u0026rsquo;utente finale e genera il prompt definitivo che viene passato in input al modello Il modello linguistico (LLM): in particolare, LangChain si integra con i provider più importanti (OpenAI, Cohere, Hugging Face, etc) Gli Output Parsers: consentono di estrarre dati in forma strutturata dalle risposte restituite dal modello linguistico I Prompt Templates: fanno riferimento ad un modo riproducibile per generare un prompt. Contiene una stringa di testo (\u0026ldquo;il modello\u0026rdquo;), che può accettare una serie di parametri dall\u0026rsquo;utente finale e genera il prompt definitivo che viene passato in input al modello Il modello linguistico (LLM): in particolare, LangChain si integra con i provider più importanti (OpenAI, Cohere, Hugging Face, etc) Gli Output Parsers: consentono di estrarre dati in forma strutturata dalle risposte restituite dal modello linguistico\nIl framework ha 2 caratteristiche molto interessanti:\npuò integrare le capability dei LLM con una propria base dati, partendo da dati strutturati e non permette di implementare il concetto di \u0026ldquo;Agente\u0026rdquo;, attraverso cui la sequenza di azioni da compiere è determinata anch\u0026rsquo;essa come output del language model Relativamente al punto 1, ero piuttosto curioso così ho deciso di fare alcuni test. L\u0026rsquo;obiettivo non è tanto quello di fare un\u0026rsquo;analisi critica relativa alle performance dei modelli, ma piuttosto verificare la facilità con cui è possibile integrare il framework all\u0026rsquo;interno di una propria base dati.\nIntegrazione con i dati non strutturati Non sapendo da dove partire, ho dato un occhio ai casi d\u0026rsquo;uso più documentati su internet. Nella fattispecie, ho trovato molta documentazione relativa all\u0026rsquo;analisi dei file PDF. Se c\u0026rsquo;è una cosa che non manca su Internet, sono appunto i file PDF, quindi mi sembrava un ambito su cui avrei potuto sperimentare parecchio.\nNella documentazione ufficiale c\u0026rsquo;è una sezione apposita relativa alla \u0026ldquo;Data Connection\u0026rdquo;, che ho trovato incredibilmente chiara ed intuitiva e di cui provo a riassumere qui i punti principali.\nI building blocks messi a disposizione da LangChain sono i seguenti:\nDocument: è un\u0026rsquo;astrazione contenente i dati in forma testuale e i metadati associati Document loaders: Sono delle classi che consentono di estrarre il testo e i metadati da una specifica tipologia di dati per costruire il \u0026ldquo;Document\u0026rdquo; Document transformers: utilizzato per processare i Document. Poiché i LLM solitamente hanno delle limitazioni importanti in termini di token processabili, la trasformazione più comune è lo splitting in chunk, attraverso cui è possibile sottomettere le chiamate verso il provider del LLM in serie o in parallelo. Esistono anche altre tipologie di transformer, per esempio: riduzione di ridondandanza, traduzione, estrazione di metadati etc\u0026hellip; Text embedding: L\u0026rsquo;embedding è l\u0026rsquo;operazione di traduzione di una porzione di testo in un modello vettoriale N-dimensionale, che poi è alla base delle operazioni di ricerca semantica basate su indici di similarità ed implementate tramite calcolo delle distanze vettoriali Vector stores: memorizza gli embedding all\u0026rsquo;interno di un DB Engine in grado di restituire efficientemente i \u0026ldquo;vettori\u0026rdquo; più \u0026ldquo;vicini\u0026rdquo; (e dunque le porzioni di testo più simili) al testo passato in input (anch\u0026rsquo;esso opportunamente vettorializzato tramite embedding). In particolare, è possibile sfruttare alcuni engine open source per far girare tutto in locale, oppure integrarsi con alcuni prodotti di mercato che ovviamente offrono performance molto migliori (es: Pinecone) Retrievers: è un\u0026rsquo;interfaccia che restituisce documenti a partire da una query non strutturata. È un concetto un po\u0026rsquo; più generale di un Vector Store, ma a differenza di quest\u0026rsquo;ultimo, consente unicamente di restituire i documenti e non necessariamente di memorizzarli Chains E adesso veniamo ai componenti principali: le catene.\nLangChain introduce questo concetto che rappresenta un\u0026rsquo;astrazione utile per implementare in maniera semplice e modulare le applicazioni che fanno uso di LLMs. Esistono molte Chain predefinite, le più comuni sono:\nRetrievalQA: risponde ad un input utente partendo dal\u0026rsquo;output restituito da un retriever ConversationalRetrievalChain: simile a RetrievalQA, aggiunge la capacità di costruire una esperienza conversazionale attraverso la storia dei messaggi scambiati Summarize: come si evince dal nome, fa una sintesi dei documenti passati in input L\u0026rsquo;esperimento Ho preso un paper di ricerca del 2017, scritto da alcuni ricercatori dell\u0026rsquo;Oak Ridge National Laboratory (ORNL) e di altri istituti universitari, che propone una implementazione di un algoritmo di quantum computing per un problema di Portfolio Optimization.\nIn particolare, l\u0026rsquo;articolo descrive i vantaggi derivanti dall\u0026rsquo;utilizzo di una variante del modello di Markowitz (QUBO) su device quantistici di tipo D-Wave.\nL\u0026rsquo;articolo completo lo trovate a questo link.\nEssendo appassionato di questi temi, ma non avendo una solida base teorica, riesco a capire i punti principali del paper, ma non ho alcuna competenza per valutarne l\u0026rsquo;attendibilità o la bontà dei risultati, così decido di chiedere un\u0026rsquo;analisi critica ad OpenAI, passando attraverso LangChain.\nSorprendentemente, ho impiegato solo qualche ora e meno di 20 righe di codice per ottenere un prototipo funzionante, con un risultato che reputo soddisfacente.\nIl codice Ecco di seguito il codice prodotto.\nIl codice è praticamente autoesplicativo, ma aggiungo nel seguito alcune note e commenti a margine.\nfrom langchain.llms import OpenAI from langchain.document_loaders import PyPDFLoader from langchain.chains.summarize import load_summarize_chain from langchain import OpenAI, PromptTemplate from dotenv import load_dotenv load_dotenv() loader = PyPDFLoader(\u0026#34;docs/pdf/102.pdf\u0026#34;) docs = [] docs.extend(loader.load()) prompt_template = \u0026#34;\u0026#34;\u0026#34;Scrivi una critica negativa di questo articolo di ricerca, mettendone in dubbio i risultati e l\u0026#39;applicabilità: {text} CRITICA:\u0026#34;\u0026#34;\u0026#34; PROMPT = PromptTemplate(template=prompt_template, input_variables=[\u0026#34;text\u0026#34;]) chain = load_summarize_chain(OpenAI(temperature=0.5), chain_type=\u0026#34;map_reduce\u0026#34;, return_intermediate_steps=False, map_prompt=PROMPT, combine_prompt=PROMPT) chain({\u0026#34;input_documents\u0026#34;: docs}, return_only_outputs=True) La chiamata a load_dotenv() serve unicamente per caricare la variabile d\u0026rsquo;ambiente OPENAI_API_KEY, associata al mio account di OpenAI. Le prime istruzioni servono per inizializzare il loader e creare i \u0026ldquo;Document\u0026rdquo; associati. Subito dopo, definisco il template di Prompt e chiamo la Chain di \u0026ldquo;Summarize\u0026rdquo;. Il primo parametro passato alla Chain è una istanziazione di LLM di tipo OpenAI, che ho volutamente configurato con una temperatura di 0.5, per spingere un po\u0026rsquo; con la creatività. La chain_type è di tipo \u0026ldquo;map_reduce\u0026rdquo; perché il testo è sufficientemente lungo da superare il limite dei 4096 token imposti dal provider che sto usando (ChatGPT 3.5).\nRisultati Questi alcuni dei risultati che ho ottenuto:\nQuesto articolo di ricerca presenta una varietà di metodologie per l\u0026rsquo;analisi della selezione del portafoglio in presenza di più criteri. Tuttavia, i risultati sono basati su teorie matematiche e modelli di simulazione piuttosto che su dati reali. Inoltre, l\u0026rsquo;uso di un budget fisso di $100 non è realistico e non rappresenta le condizioni del mondo reale. Inoltre, la maggior parte delle tecniche descritte in questo articolo non sono applicabili ai mercati finanziari reali, poiché non prendono in considerazione fattori come la volatilità dei mercati, i cicli economici, ecc. Di conseguenza, i risultati e l\u0026rsquo;applicabilità di questo articolo di ricerca sono messi in dubbio.\nQuesto articolo di ricerca non fornisce sufficienti prove dei risultati ottenuti, né dimostra l\u0026rsquo;applicabilità del modello proposto nella pratica. La teoria di Markowitz sulla selezione del portafoglio non è applicabile a tutti gli investitori, poiché non considera le complicazioni che si possono verificare nella pratica, come le tasse o le commissioni. Inoltre, l\u0026rsquo;uso dell\u0026rsquo;ottimizzatore quantico D-Wave non è stato adeguatamente dimostrato. La formulazione rilassata non prevede l\u0026rsquo;acquisto di quote di azioni, il che significa che il risultato ottimale non è realistico. Inoltre, l\u0026rsquo;abbondanza di zeri nei dati non è realistica. La maggior parte dei codici di esempio e delle applicazioni menzionate nell\u0026rsquo;articolo sono relativi al calcolo\nConclusioni Anche dopo aver letto il responso di OpenAI, non riesco ad entrare nel merito per capire se le conclusioni tratte hanno delle basi solide o meno, ma in prima analisi sembrerebbero condivisibili dal lettore medio (cioè me stesso in questo caso).\nIl caso emblematico è, ad esempio, il commento sui soli 100$ di investimento. Se andate a leggere l\u0026rsquo;articolo, vedrete che questo è effettivamente lo scenario semplificato che è stato considerato dai ricercatori, ma oggettivamente non ho elementi per dire se effettivamente questo fattore può mettere in discussione i risultati.\nIn generale, la cosa che mi ha stupito è la facilità con cui il framework mette a disposizione dello sviluppatore i building block per sviluppare applicazioni di AI, senza reinventare la ruota ed integrandosi molto bene con i principali provider e prodotti di mercato.\nMi rendo conto che l\u0026rsquo;esempio illustrato è veramente banale, ma apre un mondo di possibilità. Sto facendo delle altre prove ampliando il dataset e cercando di rispondere a domande un po\u0026rsquo; più articolate. Stay tuned\n","permalink":"https://c-daniele.github.io/it/posts/2023-07-24-langchain-helloworld-pdf/","summary":"Intro Per chi non lo conoscesse, LangChain è un framework per lo sviluppo di applicazioni che fanno uso di LLMs.\nCome si evince dal nome stesso, LangChain si basa sul concetto di Catena LLM, la quale combina 3 elementi:\nI Prompt Templates: fanno riferimento ad un modo riproducibile per generare un prompt. Contiene una stringa di testo (\u0026ldquo;il modello\u0026rdquo;), che può accettare una serie di parametri dall\u0026rsquo;utente finale e genera il prompt definitivo che viene passato in input al modello Il modello linguistico (LLM): in particolare, LangChain si integra con i provider più importanti (OpenAI, Cohere, Hugging Face, etc) Gli Output Parsers: consentono di estrarre dati in forma strutturata dalle risposte restituite dal modello linguistico I Prompt Templates: fanno riferimento ad un modo riproducibile per generare un prompt.","title":"LLM - Esperimenti con LangChain - Parte 1"}]