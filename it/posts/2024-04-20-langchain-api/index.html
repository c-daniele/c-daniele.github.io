<!doctype html><html lang=it dir=ltr><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Langchain pt. 3 - Come invocare API Rest in linguaggio naturale | Cdani's Blog</title><meta name=keywords content="ai,langchain,api,rest,swagger,openapi"><meta name=description content="Intro L&rsquo;anno scorso, Gartner ha inserito la Generative AI nella fase di picco di aspettative all&rsquo;interno del suo modello di Hype Cycle per il mondo della AI.
Recentemente alcuni nomi importanti tra le grandi aziende del settore hanno paragonato l&rsquo;entusiasmo della GenAI alla bolla dotcom. Inoltre sono circolate delle indiscrezioni intorno ai principali Cloud Providers, secondo le quali essi stiano addirittura dando indicazioni ai loro Sales Team di rallentare l&rsquo;entusiasmo dimostrato verso i clienti nei confronti delle iniziative di GenAI, o comunque di utilizzare un approccio cauto e consapevole dei costi e dei reali benefici."><meta name=author content="Me"><link rel=canonical href=https://c-daniele.github.io/it/posts/2024-04-20-langchain-api/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.3598bbf45621a4ad34d093926efeb15d6df27175e085d2f069483f14ad39d7fa.css integrity="sha256-NZi79FYhpK000JOSbv6xXW3ycXXghdLwaUg/FK051/o=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://c-daniele.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://c-daniele.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://c-daniele.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://c-daniele.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://c-daniele.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=it href=https://c-daniele.github.io/it/posts/2024-04-20-langchain-api/><link rel=alternate hreflang=en href=https://c-daniele.github.io/en/posts/2024-04-20-langchain-api/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-8NZQZ3Z1RN"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-8NZQZ3Z1RN",{anonymize_ip:!1})}</script><meta property="og:title" content="Langchain pt. 3 - Come invocare API Rest in linguaggio naturale"><meta property="og:description" content="Intro L&rsquo;anno scorso, Gartner ha inserito la Generative AI nella fase di picco di aspettative all&rsquo;interno del suo modello di Hype Cycle per il mondo della AI.
Recentemente alcuni nomi importanti tra le grandi aziende del settore hanno paragonato l&rsquo;entusiasmo della GenAI alla bolla dotcom. Inoltre sono circolate delle indiscrezioni intorno ai principali Cloud Providers, secondo le quali essi stiano addirittura dando indicazioni ai loro Sales Team di rallentare l&rsquo;entusiasmo dimostrato verso i clienti nei confronti delle iniziative di GenAI, o comunque di utilizzare un approccio cauto e consapevole dei costi e dei reali benefici."><meta property="og:type" content="article"><meta property="og:url" content="https://c-daniele.github.io/it/posts/2024-04-20-langchain-api/"><meta property="og:image" content="https://c-daniele.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-04-20T19:00:00+02:00"><meta property="article:modified_time" content="2024-04-20T19:00:00+02:00"><meta property="og:site_name" content="Cdani's Blog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://c-daniele.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Langchain pt. 3 - Come invocare API Rest in linguaggio naturale"><meta name=twitter:description content="Intro L&rsquo;anno scorso, Gartner ha inserito la Generative AI nella fase di picco di aspettative all&rsquo;interno del suo modello di Hype Cycle per il mondo della AI.
Recentemente alcuni nomi importanti tra le grandi aziende del settore hanno paragonato l&rsquo;entusiasmo della GenAI alla bolla dotcom. Inoltre sono circolate delle indiscrezioni intorno ai principali Cloud Providers, secondo le quali essi stiano addirittura dando indicazioni ai loro Sales Team di rallentare l&rsquo;entusiasmo dimostrato verso i clienti nei confronti delle iniziative di GenAI, o comunque di utilizzare un approccio cauto e consapevole dei costi e dei reali benefici."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":4,"name":"Langchain pt. 3 - Come invocare API Rest in linguaggio naturale","item":"https://c-daniele.github.io/it/posts/2024-04-20-langchain-api/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Langchain pt. 3 - Come invocare API Rest in linguaggio naturale","name":"Langchain pt. 3 - Come invocare API Rest in linguaggio naturale","description":"Intro L\u0026rsquo;anno scorso, Gartner ha inserito la Generative AI nella fase di picco di aspettative all\u0026rsquo;interno del suo modello di Hype Cycle per il mondo della AI.\nRecentemente alcuni nomi importanti tra le grandi aziende del settore hanno paragonato l\u0026rsquo;entusiasmo della GenAI alla bolla dotcom. Inoltre sono circolate delle indiscrezioni intorno ai principali Cloud Providers, secondo le quali essi stiano addirittura dando indicazioni ai loro Sales Team di rallentare l\u0026rsquo;entusiasmo dimostrato verso i clienti nei confronti delle iniziative di GenAI, o comunque di utilizzare un approccio cauto e consapevole dei costi e dei reali benefici.","keywords":["ai","langchain","api","rest","swagger","openapi"],"articleBody":"Intro L’anno scorso, Gartner ha inserito la Generative AI nella fase di picco di aspettative all’interno del suo modello di Hype Cycle per il mondo della AI.\nRecentemente alcuni nomi importanti tra le grandi aziende del settore hanno paragonato l’entusiasmo della GenAI alla bolla dotcom. Inoltre sono circolate delle indiscrezioni intorno ai principali Cloud Providers, secondo le quali essi stiano addirittura dando indicazioni ai loro Sales Team di rallentare l’entusiasmo dimostrato verso i clienti nei confronti delle iniziative di GenAI, o comunque di utilizzare un approccio cauto e consapevole dei costi e dei reali benefici. E’ già iniziata la discesa verso la “fossa della disillusione”?\nFigura 1 - Modello Hype cycle D’altro canto, è anche possibile che questa volta il classico modello Hype Cycle non sia applicabile . Rispetto ad altri trend trasformativi e tecnologici, si sta andando molto velocemente a regime verso una fase di consapevolezza e maturazione. Si iniziano infatti a vedere alcuni trend del mercato, che vanno oltre la semplice corsa al modello più potente in termini di “forza bruta”.\nAlcuni esempi:\nMolte aziende stanno lavorando su modelli relativamente piccoli ed eseguibili anche in locale, esempi: Meta e Qualcomm hanno appena annunciato un accordo di collaborazione per l’ottimizzazione dei modelli Llama3 al fine di essere eseguiti direttamente sui dispositivi equipaggiati con le future piattaforme top di gamma Snapdragon H2O ha lanciato un modello linguistico superleggero denominato Danube, derivato da Llama2 e pensato per essere eseguito su device mobili Apple sembra stia lavorando ad un modello linguistico “on-device” e disponibile offline sui dispositivi di Cupertino Più o meno tutti i grandi sviluppatori di LLM stanno introducendo delle soluzioni multi-modali Stanno nascendo diversi framework e prodotti che permettono di costruire soluzioni complesse e modulari e che utilizzano i modelli LLM come building block per costruire applicazioni “AI-powered” complesse e vendor-agnostic In altre parole, per fare un parallelo con quello che è successo ormai molti anni fa con la nascita dell’ingegneria del software, tali prodotti stanno spianando la strada alla “Ingegneria dell’AI” Relativamente a quest’ultimo punto, LangChain va proprio in questa strada. E’ uno dei framework per l’AI Open Source al momento più completi e potenti. Fornisce un grande controllo e adattabilità per vari casi d’uso e offre una maggiore granularità rispetto ad altri framework come, ad esempio, LlamaIndex. Una delle features che ho testato in questi giorni è l’integrazione del framework con API Rest esterne, specificate secondo uno standard preciso (es: Swagger, OpenApi) o anche descritte in linguaggio naturale.\nIn questo articolo, mostrerò come sia possibile integrare direttamente “a runtime” una API di terze parti all’interno di un banalissimo chatbot, ed interrogare l’API in linguaggio naturale senza alcuna preventiva conoscenza delle specifiche di tale API.\nPreambolo tecnico Il codice mostrato nel seguito e che condivido su GitHub si basa sull’utilizzo di OpenAI e di Bedrock. Quest’ultimo, per chi non lo conoscesse, è il servizio di AWS che dà accesso a diversi modelli tra cui Llama2, Claude, Mistral e il modello proprietario di AWS denominato Titan. Il codice è estremamente semplice e segue i seguenti step logici:\nInizializzazione delle variabili di ambiente Creazione del modello LLM Retrieve dinamico delle specifiche di una determinata API Inizializzazione e invocazione del componente APIChain. Questo componente, nello specifico, applica alcune semplici tecniche di Prompt Engineering per eseguire le seguenti 3 azioni: Prendere in input la domanda dell’utente in linguaggio naturale e costruire, tramite il LLM, l’URL da invocare Invocare l’URL così costruito tramite una chiamata HTTP Inglobare la risposta ottenuta dalla chiamata HTTP all’interno di una nuova invocazione del LLM ed estrapolare l’informazione richiesta dall’utente Il processo è riassunto nel seguente diagramma di flusso:\nFigura 2 - Diagramma di Flusso Nel codice che seguirà, ho cablato per semplicità anche le interazioni utente nel codice sotto forma di stringhe statiche, ma nulla vieta di ottenere dinamicamente questi input dall’utente, ad esempio tramite una interfaccia a Chatbot, oppure configurare le API da una apposita sezione applicativa e fare poi plug\u0026play direttamente nel chatbot per aggiungere funzionalità a runtime.\nIn altre parole, con pochissimo sforzo, è possibile realizzare un chatbot completamente agnostico rispetto alle specifiche API e adeguarsi dinamicamente alle esigenze, inserendo liberamente i riferimenti a nuove API o recependo al volo le eventuali modifiche alla interfaccia.\nIl caso d’uso più immediato potrebbe essere, ad esempio, quello di uno strumento di customer care che si integra con le API aziendali per restituire direttamente al cliente informazioni relative ai suoi ordini, prodotti, segnalazioni etc. La disponibilità di queste informazioni potrebbe essere infatti sviluppata in maniera incrementale, potenziando le funzionalità esposte dal chatbot senza tuttavia toccare una riga di codice ed utilizzando un approccio plug\u0026play delle API all’interno del processo dialogico esistente.\nAllargando il discorso e andando verso un contesto più Enterprise, possiamo immaginare lo scenario di una moderna Data Platform che metta a disposizione i principali KPI aziendali sotto forma di Data-APIs a beneficio di chiunque in azienda voglia consultarli rapidamente tramite il chatbot aziendale.\nIn altre parole, le possibilità sono tantissime.\nLe API Le API che ho utilizzato per fare le prove sono le seguenti:\nklarna.com per chi non la conoscesse, KLARNA è una fintech svedese che offre una piattaforma di pagamento a rate per chi fa shopping online. E’ integrata in tantissime piattaforme di shopping online. L’API in questione è accessibile gratuitamente, mette a disposizione un metodo per cercare prodotti sulla base di attributi descrittivi, prezzo, brand etc ed è operativa solo in alcuni mercati (US, GB, DE, SE, DK). open-meteo E’ una API gratuita che mette a disposizione dati meteoreologici. Il caso più comune è quello in cui interroghiamo l’API per ottenere le condizioni meteo in una determinata città, in termini di temperatura, precipitazioni, visibilità, etc. APIChain Il componente che andremo ad utilizzare all’interno della suite di LangChain si chiama APIChain ed è banalmente un wrapper che contiene:\nUna istanza di un LLMChain, che serve per costruire l’URL e i parametri HTTP a partire dalla domanda in linguaggio naturale Un wrapper del componente request, che viene utilizzato per inviare la chiamata HTTP Una istanza di un LLMChain che serve per costruire la response in linguaggio naturale a partire dal payload della Response HTTP Alcuni prompt che servono per creare il contesto corretto e implementare efficacemente le chiamate al LLM Per quanto riguarda i Prompt che mette a disposizione il componente APIChain, durante i test mi sono reso conto che essi non funzionavano correttamente con tutti i LLM (ad es: funzionavano con OpenAI, ma non con Llama2, Claude, etc). Pertanto, ho costruito una versione leggermente migliore del prompt e ho proposto la modifica sul repo ufficiale (vedremo se l’accetteranno 😃 ).\nIl Test Nella prima parte del codice facciamo l’inizializzazione dei componenti di base e creiamo i modelli.\nAlcune note:\nle variabili di ambiente relative alla integrazione con OPEN_AI e AWS devono essere configurate nel file .env all’interno del file “libs.py” ho creato un wrapper per l’istanziazione del modello LLM. Troverete tutto nel repository GitHub I modelli di Bedrock che ho utilizzato si trovano al momento solo in alcune Region. Dunque occorre fare attenzione alle impostazioni della region e dei costi associati all’utilizzo 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 from langchain.chains import APIChain from dotenv import load_dotenv import httpx import logging as logger import sys # see \"libs.py\" file from libs import * # see \"prompt_improved.py\" file from prompt_improved import * # Set WARNING Logger levels help print only meaningful text logger.basicConfig(stream=sys.stdout, level=logger.WARNING) logger.getLogger('botocore').setLevel(logger.WARNING) logger.getLogger('httpx').setLevel(logger.WARNING) # loading ENV variables load_dotenv() # Initialize Models gpt35 = create_llm(model={\"provider\":\"OpenAI\", \"value\": \"gpt-3.5-turbo\"}, model_kwargs={\"temperature\": 0.1}) gpt4 = create_llm(model={\"provider\":\"OpenAI\", \"value\": \"gpt-4\"}, model_kwargs={\"temperature\": 0.1}) claude3 = create_llm(model={\"provider\":\"Anthropic\", \"value\": \"anthropic.claude-3-sonnet-20240229-v1:0\"}, model_kwargs={\"temperature\": 0.1}) llama2 = create_llm(model={\"provider\":\"Meta\", \"value\": \"meta.llama2-70b-chat-v1\"}, model_kwargs=None) Ok, adesso vediamo come integrare dinamicamente il file descrittore della interfaccia e passarlo al componente APIChain. La variabile “limit_to_domains” è utilizzata per introdurre un meccanismo di sicurezza che limita i domini verso cui indirizzare le richieste. In teoria potrebbe essere impostato a “None” per non impostare alcun vincolo, ma è sempre preferibile evitarlo. Le 2 variabili api_url_prompt e api_response_prompt consentono di customizzare i prompt da passare all’LLM. Come ho anticipato in precedenza, ho impostato 2 prompt che si sono dimostrati più robusti di quelli di default.\n26 27 28 29 30 31 32 33 34 35 36 37 38 # Dynamically retrieve swagger output = httpx.get(\"https://www.klarna.com/us/shopping/public/openai/v0/api-docs/\") swagger = output.text # build the APIChain chain = APIChain.from_llm_and_api_docs( llm=gpt4, api_docs=swagger, verbose=False, limit_to_domains=[\"klarna.com\", \"https://www.klarna.com/\", \"https://www.klarna.com\"], api_url_prompt=FINE_TUNED_API_URL_PROMPT, api_response_prompt=FINE_TUNED_API_RESPONSE_PROMPT ) A questo punto è tutto impostato. Possiamo fare una domanda e passarla al framework per poi restituire l’output all’utente finale. Ho chiesto di ricercare 3 magliette con un tetto massimo di 50 dollari e di ritornare prezzo, descrizione e link.\n39 40 41 42 43 44 45 # Ask a question to the Chain response = chain.invoke( \"Find 3 t-shirts, max 50 USD. For each Product print the Description, the Price and the corresponding URL\" ) # Print the Chain Output print(response['output']) Questo è l’output che ho ottenuto al primo tentativo:\n1. *Product: Polo Ralph Lauren Men's Slim Fit Wicking Crew Undershirts 3-pack - White* *Price: $37.99* *URL: https://www.klarna.com/us/shopping/pl/cl10001/3207134809/Clothing/Polo-Ralph-Lauren-Men-s-Slim-Fit-Wicking-Crew-Undershirts-3-pack-White/?utm_source=openai\u0026ref-site=openai_plugin* 2. *Product: Lacoste Men's T-shirts 3-pack - Black* *Price: $31.90* *URL: https://www.klarna.com/us/shopping/pl/cl10001/3202043025/Clothing/Lacoste-Men-s-T-shirts-3-pack-Black/?utm_source=openai\u0026ref-site=openai_plugin* 3. *Product: SKIMS Cotton Jersey T-shirt* *Price: $48.00* *URL: https://www.klarna.com/us/shopping/pl/cl10001/3202929904/Clothing/SKIMS-Cotton-Jersey-T-shirt/?utm_source=openai\u0026ref-site=openai_plugin* Non male!\nHo fatto parecchie altre prove con gli altri modelli e ho ottenuto performance simili anche se, come mi aspettavo, GPT4 e Claude3 sono mediamente più precisi.\nPer quanto riguarda la seconda API, il codice è praticamente identico, a parte il riferimento all’URL descrittore (swagger), la variabile limit_to_domains che deve essere coerente con l’API e la domanda dell’utente. Riporto dunque solo la seconda e la terza parte dello script python.\nPunto di attenzione: non esiste uno swagger ufficiale per questa API, quindi ho usato il file YAML che si trova su GitHub. A volte le chiamate verso GitHub vanno in errore. In tal caso suggerisco di riprovare un paio di volte.\n26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 # Dynamically retrieve swagger output = httpx.get(\"https://raw.githubusercontent.com/open-meteo/open-meteo/main/openapi.yml\") meteo_swagger = output.text # build the APIChain chain = APIChain.from_llm_and_api_docs( llm=claude3, api_docs=meteo_swagger, verbose=True, limit_to_domains=None, api_url_prompt=FINE_TUNED_API_URL_PROMPT, api_response_prompt=FINE_TUNED_API_RESPONSE_PROMPT ) # Ask a question to the Chain response = chain.invoke( \"What is the weather like right now in Munich, Germany in degrees Fahrenheit?\" ) # Print the Chain Output print(response['output']) Il risultato con Claude, e con GPT 3,5 e GPT4 è in linea con le aspettative. Le 2 chiamate di Langchain hanno costruito correttamente l’URL ed interpretato il risultato, trasformandolo in linguaggio naturale.\nThe current weather in Munich, Germany is 45.7°F with a wind speed of 17.7 km/h coming from 264° direction. Il test con Llama2 non è andato a buon fine. In particolare, ha evidentemente avuto allucinazioni nella prima chiamata, in cui LangChain crea l’URL, inventando alcuni parametri non specificati nello swagger.\nDietro le quinte Un altro tool molto interessante della suite di LangChain si chiama LangSmith, che consente di fare monitoraggio e profiling su tutte le invocazioni del modello. Oltre a questo, consente di fare tante altre cose, come ad esempio:\nil debugging avanzato la continua valutazione dei task tramite la definizione di dataset predefiniti e di criteri di valutazione l’annotazione dei modelli per aggiungere metadati o feedback utente molte altre features relative al monitoraggio e al miglioramento delle applicazioni basate su LangChain Utilizzando LangSmith, è possibile vedere graficamente il macroprocesso le chiamate ai modelli sottostanti.\nFigura 3 - Struttura delle chiamate di LangChain In particolare, in Figura 3, si vede chiaramente la struttura ad albero delle chiamate, identificata dalla sigla “APIChain”, che è composta da 2 chain figlie di tipo LLM, a cui corrispondono altrettante chiamate verso OpenAI. Altra cosa estremamente utile è il numero di token utilizzati e il costo stimato delle singole chiamate.\nAndando nel dettaglio sulle singole chiamate al LLM, possiamo vedere il prompt realmente passato in input al modello e la response sulla singola invocazione.\nFigura 4 - Step di costruzione dell'URL Figura 5 - Prompt finale e sintesi della risposta all'utente in linguggio naturale Conclusioni Dando un occhio al codice sorgente di LangChain e alle chiamate che vengono fatte verso i modelli, tramite LangSmith, si vede chiaramente che l’integrazioni di API Rest in una applicazione basata su LLM è veramente banale e basata su tecniche molto semplici di Prompt Engineering, che però consentono una integrazione estremamente potente tra le nuove applicazioni AI e i sistemi tradizionali.\nA mio avviso, è uno degli esempi più chiari e cristallini di come oggi si possa (e forse si debba) reinterpretare l’interazione uomo/macchina in termini di integrazione tra sistemi formali ben specificati con comportamento predicibile (es: qualunque sistema software tradizionale in azienda) e il linguaggio naturale.\nLangChain ed altri framework consentono di fare qualcosa di simile anche a livello più basso, ad esempio interrogando un DB in linguaggio naturale e utilizzando un LLM per generare le query sottostanti. Al di là delle questioni squisitamente tecniche e di performance, questo approccio è bello in teoria ma, sulla base della mia esperienza, ci sono diversi elementi che mi fanno pensare che esso non sia realmente applicabile se non in alcuni casi specifici poiché nella stragrande maggioranza dei casi ci sono stratificazioni applicative che durano anni e difficoltà a mantenere un data catalog auto-descrittivo di buon livello. Al contrario, le API enterprise introducono un layer che quasi sempre parla una lingua più vicina al Business ed in generale hanno dei metadati auto-descrittivi.\n","wordCount":"2238","inLanguage":"it","datePublished":"2024-04-20T19:00:00+02:00","dateModified":"2024-04-20T19:00:00+02:00","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://c-daniele.github.io/it/posts/2024-04-20-langchain-api/"},"publisher":{"@type":"Organization","name":"Cdani's Blog","logo":{"@type":"ImageObject","url":"https://c-daniele.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://c-daniele.github.io/it/ accesskey=h title="Home (Alt + H)"><img src=https://c-daniele.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://c-daniele.github.io/en/ title=English aria-label=English>En</a></li></ul></div></div><ul id=menu><li><a href=https://c-daniele.github.io/it/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://c-daniele.github.io/it/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://c-daniele.github.io/it/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://c-daniele.github.io/it/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://c-daniele.github.io/it/>Home</a></div><h1 class=post-title>Langchain pt. 3 - Come invocare API Rest in linguaggio naturale</h1><div class=post-meta><span title='2024-04-20 19:00:00 +0200 +0200'>aprile 20, 2024</span>&nbsp;·&nbsp;11 minuti&nbsp;·&nbsp;2238 parole&nbsp;·&nbsp;Me&nbsp;|&nbsp;Traduzioni:<ul class=i18n_list><li><a href=https://c-daniele.github.io/en/posts/2024-04-20-langchain-api/>En</a></li></ul></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Indice contenuti</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#intro>Intro</a></li><li><a href=#preambolo-tecnico>Preambolo tecnico</a></li><li><a href=#le-api>Le API</a></li><li><a href=#apichain>APIChain</a></li><li><a href=#il-test>Il Test</a></li><li><a href=#dietro-le-quinte>Dietro le quinte</a></li><li><a href=#conclusioni>Conclusioni</a></li></ul></nav></div></details></div><div class=post-content><h2 id=intro>Intro<a hidden class=anchor aria-hidden=true href=#intro>#</a></h2><p>L&rsquo;anno scorso, Gartner ha inserito la Generative AI nella fase di picco di aspettative all&rsquo;interno del suo modello di <a href=https://www.gartner.com/en/articles/what-s-new-in-artificial-intelligence-from-the-2023-gartner-hype-cycle>Hype Cycle</a> per il mondo della AI.</p><p>Recentemente alcuni nomi importanti tra le grandi aziende del settore <a href=https://www.wired.com/story/amazons-cloud-boss-selipsky-generative-ai-hype/>hanno paragonato l&rsquo;entusiasmo della GenAI alla bolla <em>dotcom</em></a>.
Inoltre sono <a href="https://www.theinformation.com/articles/generative-ai-providers-quietly-tamp-down-expectations?ref=wheresyoured.at">circolate delle indiscrezioni</a> intorno ai principali Cloud Providers, secondo le quali essi stiano addirittura dando indicazioni ai loro Sales Team di rallentare l&rsquo;entusiasmo dimostrato verso i clienti nei confronti delle iniziative di GenAI, o comunque di utilizzare un approccio cauto e consapevole dei costi e dei reali benefici.
E&rsquo; già iniziata la discesa verso la <a href=https://it.wikipedia.org/wiki/Hype_cycle>&ldquo;fossa della disillusione&rdquo;</a>?</p><p><figure><a href=/images/20240420/Hype-Cycle-General.png><img src=/images/20240420/Hype-Cycle-General.png alt="Figura 1 - Modello Hype cycle"></a><figcaption>Figura 1 - Modello Hype cycle</figcaption></figure></p><p>D&rsquo;altro canto, è anche possibile che questa volta il classico modello Hype Cycle non sia applicabile .
Rispetto ad altri trend trasformativi e tecnologici, si sta andando molto velocemente a regime verso una fase di consapevolezza e maturazione.
Si iniziano infatti a vedere alcuni trend del mercato, che vanno oltre la semplice corsa al modello più potente in termini di &ldquo;forza bruta&rdquo;.</p><p>Alcuni esempi:</p><ul><li>Molte aziende stanno lavorando su modelli relativamente piccoli ed eseguibili anche in locale, esempi:<ul><li>Meta e Qualcomm hanno appena annunciato un <a href=https://www.qualcomm.com/news/releases/2024/04/qualcomm-enables-meta-llama-3-to-run-on-devices-powered-by-snapd>accordo di collaborazione</a> per l&rsquo;ottimizzazione dei modelli <em>Llama3</em> al fine di essere eseguiti direttamente sui dispositivi equipaggiati con le future piattaforme top di gamma Snapdragon</li><li>H2O ha lanciato un modello linguistico superleggero denominato <a href=https://venturebeat.com/ai/h2o-ai-releases-danube-a-super-tiny-llm-for-mobile-applications/><em>Danube</em></a>, derivato da <em>Llama2</em> e pensato per essere eseguito su device mobili</li><li>Apple <a href=https://www.bloomberg.com/news/newsletters/2024-04-21/apple-aapl-growth-opportunities-southeast-asia-and-africa-lower-end-iphone-lv9itkna>sembra stia lavorando</a> ad un modello linguistico &ldquo;on-device&rdquo; e disponibile offline sui dispositivi di Cupertino</li></ul></li><li>Più o meno tutti i grandi sviluppatori di LLM stanno introducendo delle soluzioni multi-modali</li><li>Stanno nascendo diversi framework e prodotti che permettono di costruire soluzioni complesse e modulari e che utilizzano i modelli LLM come building block per costruire applicazioni <em>&ldquo;AI-powered&rdquo;</em> complesse e vendor-agnostic<ul><li>In altre parole, per fare un parallelo con quello che è successo ormai molti anni fa con la nascita dell&rsquo;ingegneria del software, tali prodotti stanno spianando la strada alla <strong>&ldquo;Ingegneria dell&rsquo;AI&rdquo;</strong></li></ul></li></ul><p>Relativamente a quest&rsquo;ultimo punto, LangChain va proprio in questa strada.
E&rsquo; uno dei framework per l&rsquo;AI Open Source al momento più completi e potenti. Fornisce un grande controllo e adattabilità per vari casi d&rsquo;uso e offre una maggiore granularità rispetto ad altri framework come, ad esempio, <em>LlamaIndex</em>.
Una delle features che ho testato in questi giorni è l&rsquo;integrazione del framework con API Rest esterne, specificate secondo uno standard preciso (es: <em>Swagger</em>, <em>OpenApi</em>) o anche descritte in linguaggio naturale.</p><p>In questo articolo, mostrerò come sia possibile integrare direttamente &ldquo;a runtime&rdquo; una API di terze parti all&rsquo;interno di un banalissimo chatbot, ed interrogare l&rsquo;API in linguaggio naturale <em>senza alcuna preventiva conoscenza</em> delle specifiche di tale API.</p><h2 id=preambolo-tecnico>Preambolo tecnico<a hidden class=anchor aria-hidden=true href=#preambolo-tecnico>#</a></h2><p>Il codice mostrato nel seguito e che <a href=https://github.com/c-daniele/langchain_tests/tree/main/01.APIChain>condivido su GitHub</a> si basa sull&rsquo;utilizzo di <em>OpenAI</em> e di <em>Bedrock</em>. Quest&rsquo;ultimo, per chi non lo conoscesse, è il servizio di AWS che dà accesso a diversi modelli tra cui <em>Llama2</em>, <em>Claude</em>, <em>Mistral</em> e il modello proprietario di AWS denominato <em>Titan</em>.
Il codice è estremamente semplice e segue i seguenti step logici:</p><ol><li>Inizializzazione delle variabili di ambiente</li><li>Creazione del modello LLM</li><li>Retrieve dinamico delle specifiche di una determinata API</li><li>Inizializzazione e invocazione del componente <em>APIChain</em>. Questo componente, nello specifico, applica alcune semplici tecniche di Prompt Engineering per eseguire le seguenti 3 azioni:<ol><li>Prendere in input la domanda dell&rsquo;utente in linguaggio naturale e costruire, tramite il LLM, l&rsquo;URL da invocare</li><li>Invocare l&rsquo;URL così costruito tramite una chiamata HTTP</li><li>Inglobare la risposta ottenuta dalla chiamata HTTP all&rsquo;interno di una nuova invocazione del LLM ed estrapolare l&rsquo;informazione richiesta dall&rsquo;utente</li></ol></li></ol><p>Il processo è riassunto nel seguente diagramma di flusso:</p><p><figure><a href=/images/20240420/FlowChart-APIChain-v2.png><img src=/images/20240420/FlowChart-APIChain-v2.png alt="Figura 2 - Diagramma di Flusso"></a><figcaption>Figura 2 - Diagramma di Flusso</figcaption></figure></p><p>Nel codice che seguirà, ho cablato per semplicità anche le interazioni utente nel codice sotto forma di stringhe statiche, ma nulla vieta di ottenere dinamicamente questi input dall&rsquo;utente, ad esempio tramite una interfaccia a Chatbot, oppure configurare le API da una apposita sezione applicativa e fare poi plug&amp;play direttamente nel chatbot per <strong>aggiungere funzionalità a runtime</strong>.</p><p>In altre parole, con pochissimo sforzo, è possibile realizzare un <strong>chatbot completamente agnostico</strong> rispetto alle specifiche API e adeguarsi dinamicamente alle esigenze, inserendo liberamente i riferimenti a nuove API o recependo al volo le eventuali modifiche alla interfaccia.</p><p>Il caso d&rsquo;uso più immediato potrebbe essere, ad esempio, quello di uno strumento di <strong>customer care</strong> che si integra con le API aziendali per restituire direttamente al cliente informazioni relative ai suoi ordini, prodotti, segnalazioni etc. La disponibilità di queste informazioni potrebbe essere infatti sviluppata in maniera incrementale, potenziando le funzionalità esposte dal chatbot senza tuttavia toccare una riga di codice ed utilizzando un <strong>approccio plug&amp;play</strong> delle API all&rsquo;interno del processo dialogico esistente.</p><p>Allargando il discorso e andando verso un contesto più Enterprise, possiamo immaginare lo scenario di una moderna Data Platform che metta a disposizione i principali KPI aziendali sotto forma di <em>Data-APIs</em> a beneficio di chiunque in azienda voglia consultarli rapidamente tramite il chatbot aziendale.</p><p>In altre parole, le possibilità sono tantissime.</p><h2 id=le-api>Le API<a hidden class=anchor aria-hidden=true href=#le-api>#</a></h2><p>Le API che ho utilizzato per fare le prove sono le seguenti:</p><ul><li><em><a href=https://www.klarna.com/us/shopping/public/openai/v0/api-docs/>klarna.com</a></em><ul><li>per chi non la conoscesse, KLARNA è una fintech svedese che offre una piattaforma di pagamento a rate per chi fa shopping online. E&rsquo; integrata in tantissime piattaforme di shopping online. L&rsquo;API in questione è accessibile gratuitamente, mette a disposizione un metodo per cercare prodotti sulla base di attributi descrittivi, prezzo, brand etc ed è operativa solo in alcuni mercati (US, GB, DE, SE, DK).</li></ul></li><li><em><a href=https://raw.githubusercontent.com/open-meteo/open-meteo/main/openapi.yml>open-meteo</a></em><ul><li>E&rsquo; una API gratuita che mette a disposizione dati meteoreologici. Il caso più comune è quello in cui interroghiamo l&rsquo;API per ottenere le condizioni meteo in una determinata città, in termini di temperatura, precipitazioni, visibilità, etc.</li></ul></li></ul><h2 id=apichain>APIChain<a hidden class=anchor aria-hidden=true href=#apichain>#</a></h2><p>Il componente che andremo ad utilizzare all&rsquo;interno della suite di LangChain si chiama <em>APIChain</em> ed è banalmente un wrapper che contiene:</p><ul><li>Una istanza di un <em>LLMChain</em>, che serve per costruire l&rsquo;URL e i parametri HTTP a partire dalla domanda in linguaggio naturale</li><li>Un wrapper del componente <em>request</em>, che viene utilizzato per inviare la chiamata HTTP</li><li>Una istanza di un <em>LLMChain</em> che serve per costruire la response in linguaggio naturale a partire dal payload della Response HTTP</li><li>Alcuni prompt che servono per creare il contesto corretto e implementare efficacemente le chiamate al LLM</li></ul><p>Per quanto riguarda i Prompt che mette a disposizione il componente APIChain, durante i test mi sono reso conto che essi non funzionavano correttamente con tutti i LLM (ad es: funzionavano con OpenAI, ma non con Llama2, Claude, etc). Pertanto, ho costruito una versione leggermente migliore del prompt e ho proposto la modifica sul repo ufficiale (vedremo se l&rsquo;accetteranno 😃 ).</p><h2 id=il-test>Il Test<a hidden class=anchor aria-hidden=true href=#il-test>#</a></h2><p>Nella prima parte del codice facciamo l&rsquo;inizializzazione dei componenti di base e creiamo i modelli.</p><p>Alcune note:</p><ul><li>le variabili di ambiente relative alla integrazione con OPEN_AI e AWS devono essere configurate nel file <em>.env</em></li><li>all&rsquo;interno del file &ldquo;libs.py&rdquo; ho creato un wrapper per l&rsquo;istanziazione del modello LLM. Troverete tutto nel <a href=https://github.com/c-daniele/langchain_tests/tree/main/01.APIChain>repository GitHub</a></li><li>I modelli di Bedrock che ho utilizzato si trovano al momento solo in alcune Region. Dunque occorre fare attenzione alle impostazioni della region e dei costi associati all&rsquo;utilizzo</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>langchain.chains</span> <span class=kn>import</span> <span class=n>APIChain</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>dotenv</span> <span class=kn>import</span> <span class=n>load_dotenv</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>httpx</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>logging</span> <span class=k>as</span> <span class=nn>logger</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>sys</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># see &#34;libs.py&#34; file</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>libs</span> <span class=kn>import</span> <span class=o>*</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># see &#34;prompt_improved.py&#34; file</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>prompt_improved</span> <span class=kn>import</span> <span class=o>*</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Set WARNING Logger levels help print only meaningful text</span>
</span></span><span class=line><span class=cl><span class=n>logger</span><span class=o>.</span><span class=n>basicConfig</span><span class=p>(</span><span class=n>stream</span><span class=o>=</span><span class=n>sys</span><span class=o>.</span><span class=n>stdout</span><span class=p>,</span> <span class=n>level</span><span class=o>=</span><span class=n>logger</span><span class=o>.</span><span class=n>WARNING</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>logger</span><span class=o>.</span><span class=n>getLogger</span><span class=p>(</span><span class=s1>&#39;botocore&#39;</span><span class=p>)</span><span class=o>.</span><span class=n>setLevel</span><span class=p>(</span><span class=n>logger</span><span class=o>.</span><span class=n>WARNING</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>logger</span><span class=o>.</span><span class=n>getLogger</span><span class=p>(</span><span class=s1>&#39;httpx&#39;</span><span class=p>)</span><span class=o>.</span><span class=n>setLevel</span><span class=p>(</span><span class=n>logger</span><span class=o>.</span><span class=n>WARNING</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># loading ENV variables</span>
</span></span><span class=line><span class=cl><span class=n>load_dotenv</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Initialize Models</span>
</span></span><span class=line><span class=cl><span class=n>gpt35</span> <span class=o>=</span> <span class=n>create_llm</span><span class=p>(</span><span class=n>model</span><span class=o>=</span><span class=p>{</span><span class=s2>&#34;provider&#34;</span><span class=p>:</span><span class=s2>&#34;OpenAI&#34;</span><span class=p>,</span> <span class=s2>&#34;value&#34;</span><span class=p>:</span> <span class=s2>&#34;gpt-3.5-turbo&#34;</span><span class=p>},</span> <span class=n>model_kwargs</span><span class=o>=</span><span class=p>{</span><span class=s2>&#34;temperature&#34;</span><span class=p>:</span> <span class=mf>0.1</span><span class=p>})</span>
</span></span><span class=line><span class=cl><span class=n>gpt4</span> <span class=o>=</span> <span class=n>create_llm</span><span class=p>(</span><span class=n>model</span><span class=o>=</span><span class=p>{</span><span class=s2>&#34;provider&#34;</span><span class=p>:</span><span class=s2>&#34;OpenAI&#34;</span><span class=p>,</span> <span class=s2>&#34;value&#34;</span><span class=p>:</span> <span class=s2>&#34;gpt-4&#34;</span><span class=p>},</span> <span class=n>model_kwargs</span><span class=o>=</span><span class=p>{</span><span class=s2>&#34;temperature&#34;</span><span class=p>:</span> <span class=mf>0.1</span><span class=p>})</span>
</span></span><span class=line><span class=cl><span class=n>claude3</span> <span class=o>=</span> <span class=n>create_llm</span><span class=p>(</span><span class=n>model</span><span class=o>=</span><span class=p>{</span><span class=s2>&#34;provider&#34;</span><span class=p>:</span><span class=s2>&#34;Anthropic&#34;</span><span class=p>,</span> <span class=s2>&#34;value&#34;</span><span class=p>:</span> <span class=s2>&#34;anthropic.claude-3-sonnet-20240229-v1:0&#34;</span><span class=p>},</span> <span class=n>model_kwargs</span><span class=o>=</span><span class=p>{</span><span class=s2>&#34;temperature&#34;</span><span class=p>:</span> <span class=mf>0.1</span><span class=p>})</span>
</span></span><span class=line><span class=cl><span class=n>llama2</span> <span class=o>=</span> <span class=n>create_llm</span><span class=p>(</span><span class=n>model</span><span class=o>=</span><span class=p>{</span><span class=s2>&#34;provider&#34;</span><span class=p>:</span><span class=s2>&#34;Meta&#34;</span><span class=p>,</span> <span class=s2>&#34;value&#34;</span><span class=p>:</span> <span class=s2>&#34;meta.llama2-70b-chat-v1&#34;</span><span class=p>},</span> <span class=n>model_kwargs</span><span class=o>=</span><span class=kc>None</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>Ok, adesso vediamo come integrare dinamicamente il file descrittore della interfaccia e passarlo al componente APIChain.
La variabile <em>&ldquo;limit_to_domains&rdquo;</em> è utilizzata per introdurre un meccanismo di sicurezza che limita i domini verso cui indirizzare le richieste. In teoria potrebbe essere impostato a &ldquo;None&rdquo; per non impostare alcun vincolo, ma è sempre preferibile evitarlo.
Le 2 variabili <em>api_url_prompt</em> e <em>api_response_prompt</em> consentono di customizzare i prompt da passare all&rsquo;LLM. Come ho anticipato in precedenza, ho impostato 2 prompt che si sono dimostrati più robusti di quelli di default.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Dynamically retrieve swagger</span>
</span></span><span class=line><span class=cl><span class=n>output</span> <span class=o>=</span> <span class=n>httpx</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;https://www.klarna.com/us/shopping/public/openai/v0/api-docs/&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>swagger</span> <span class=o>=</span> <span class=n>output</span><span class=o>.</span><span class=n>text</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># build the APIChain </span>
</span></span><span class=line><span class=cl><span class=n>chain</span> <span class=o>=</span> <span class=n>APIChain</span><span class=o>.</span><span class=n>from_llm_and_api_docs</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>llm</span><span class=o>=</span><span class=n>gpt4</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>api_docs</span><span class=o>=</span><span class=n>swagger</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>verbose</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>limit_to_domains</span><span class=o>=</span><span class=p>[</span><span class=s2>&#34;klarna.com&#34;</span><span class=p>,</span> <span class=s2>&#34;https://www.klarna.com/&#34;</span><span class=p>,</span> <span class=s2>&#34;https://www.klarna.com&#34;</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=n>api_url_prompt</span><span class=o>=</span><span class=n>FINE_TUNED_API_URL_PROMPT</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>api_response_prompt</span><span class=o>=</span><span class=n>FINE_TUNED_API_RESPONSE_PROMPT</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>A questo punto è tutto impostato. Possiamo fare una domanda e passarla al framework per poi restituire l&rsquo;output all&rsquo;utente finale.
Ho chiesto di ricercare 3 magliette con un tetto massimo di 50 dollari e di ritornare prezzo, descrizione e link.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Ask a question to the Chain</span>
</span></span><span class=line><span class=cl><span class=n>response</span> <span class=o>=</span> <span class=n>chain</span><span class=o>.</span><span class=n>invoke</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;Find 3 t-shirts, max 50 USD. For each Product print the Description, the Price and the corresponding URL&#34;</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Print the Chain Output</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>response</span><span class=p>[</span><span class=s1>&#39;output&#39;</span><span class=p>])</span>
</span></span></code></pre></td></tr></table></div></div><p>Questo è l&rsquo;output che ho ottenuto al primo tentativo:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>1. *Product: Polo Ralph Lauren Men&#39;s Slim Fit Wicking Crew Undershirts 3-pack - White*
</span></span><span class=line><span class=cl>   *Price: $37.99*
</span></span><span class=line><span class=cl>   *URL: https://www.klarna.com/us/shopping/pl/cl10001/3207134809/Clothing/Polo-Ralph-Lauren-Men-s-Slim-Fit-Wicking-Crew-Undershirts-3-pack-White/?utm_source=openai&amp;ref-site=openai_plugin*
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>2. *Product: Lacoste Men&#39;s T-shirts 3-pack - Black*
</span></span><span class=line><span class=cl>   *Price: $31.90*
</span></span><span class=line><span class=cl>   *URL: https://www.klarna.com/us/shopping/pl/cl10001/3202043025/Clothing/Lacoste-Men-s-T-shirts-3-pack-Black/?utm_source=openai&amp;ref-site=openai_plugin*
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>3. *Product: SKIMS Cotton Jersey T-shirt*
</span></span><span class=line><span class=cl>   *Price: $48.00*
</span></span><span class=line><span class=cl>   *URL: https://www.klarna.com/us/shopping/pl/cl10001/3202929904/Clothing/SKIMS-Cotton-Jersey-T-shirt/?utm_source=openai&amp;ref-site=openai_plugin*
</span></span></code></pre></div><p>Non male!</p><p>Ho fatto parecchie altre prove con gli altri modelli e ho ottenuto performance simili anche se, come mi aspettavo, GPT4 e Claude3 sono mediamente più precisi.</p><p>Per quanto riguarda la seconda API, il codice è praticamente identico, a parte il riferimento all&rsquo;URL descrittore (swagger), la variabile <em>limit_to_domains</em> che deve essere coerente con l&rsquo;API e la domanda dell&rsquo;utente. Riporto dunque solo la seconda e la terza parte dello script python.</p><p><em>Punto di attenzione</em>: non esiste uno swagger ufficiale per questa API, quindi ho usato il file YAML che si trova su GitHub. A volte le chiamate verso GitHub vanno in errore. In tal caso suggerisco di riprovare un paio di volte.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Dynamically retrieve swagger</span>
</span></span><span class=line><span class=cl><span class=n>output</span> <span class=o>=</span> <span class=n>httpx</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;https://raw.githubusercontent.com/open-meteo/open-meteo/main/openapi.yml&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>meteo_swagger</span> <span class=o>=</span> <span class=n>output</span><span class=o>.</span><span class=n>text</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># build the APIChain </span>
</span></span><span class=line><span class=cl><span class=n>chain</span> <span class=o>=</span> <span class=n>APIChain</span><span class=o>.</span><span class=n>from_llm_and_api_docs</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>llm</span><span class=o>=</span><span class=n>claude3</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>api_docs</span><span class=o>=</span><span class=n>meteo_swagger</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>verbose</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>limit_to_domains</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>api_url_prompt</span><span class=o>=</span><span class=n>FINE_TUNED_API_URL_PROMPT</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>api_response_prompt</span><span class=o>=</span><span class=n>FINE_TUNED_API_RESPONSE_PROMPT</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># Ask a question to the Chain</span>
</span></span><span class=line><span class=cl><span class=n>response</span> <span class=o>=</span> <span class=n>chain</span><span class=o>.</span><span class=n>invoke</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;What is the weather like right now in Munich, Germany in degrees Fahrenheit?&#34;</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Print the Chain Output</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>response</span><span class=p>[</span><span class=s1>&#39;output&#39;</span><span class=p>])</span>
</span></span></code></pre></td></tr></table></div></div><p>Il risultato con Claude, e con GPT 3,5 e GPT4 è in linea con le aspettative. Le 2 chiamate di Langchain hanno costruito correttamente l&rsquo;URL ed interpretato il risultato, trasformandolo in linguaggio naturale.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>The current weather in Munich, Germany is 45.7°F with a wind speed of 17.7 km/h coming from 264° direction.
</span></span></code></pre></div><p>Il test con Llama2 non è andato a buon fine. In particolare, ha evidentemente avuto allucinazioni nella prima chiamata, in cui LangChain crea l&rsquo;URL, inventando alcuni parametri non specificati nello swagger.</p><h2 id=dietro-le-quinte>Dietro le quinte<a hidden class=anchor aria-hidden=true href=#dietro-le-quinte>#</a></h2><p>Un altro tool molto interessante della suite di LangChain si chiama <a href=https://www.langchain.com/langsmith><em>LangSmith</em></a>, che consente di fare monitoraggio e profiling su tutte le invocazioni del modello.
Oltre a questo, consente di fare tante altre cose, come ad esempio:</p><ul><li>il debugging avanzato</li><li>la continua valutazione dei task tramite la definizione di dataset predefiniti e di criteri di valutazione</li><li>l&rsquo;annotazione dei modelli per aggiungere metadati o feedback utente</li><li>molte altre features relative al monitoraggio e al miglioramento delle applicazioni basate su LangChain</li></ul><p>Utilizzando LangSmith, è possibile vedere graficamente il macroprocesso le chiamate ai modelli sottostanti.</p><p><figure><a href=/images/20240420/langsmith_input_root_calls.png><img src=/images/20240420/langsmith_input_root_calls.png alt="Figura 3 - Struttura delle chiamate di LangChain"></a><figcaption>Figura 3 - Struttura delle chiamate di LangChain</figcaption></figure></p><p>In particolare, in Figura 3, si vede chiaramente la struttura ad albero delle chiamate, identificata dalla sigla <em>&ldquo;APIChain&rdquo;</em>, che è composta da 2 chain figlie di tipo LLM, a cui corrispondono altrettante chiamate verso OpenAI.
Altra cosa estremamente utile è il <em>numero di token utilizzati</em> e il <em>costo stimato delle singole chiamate</em>.</p><p>Andando nel dettaglio sulle singole chiamate al LLM, possiamo vedere il prompt realmente passato in input al modello e la response sulla singola invocazione.</p><p><figure><a href=/images/20240420/langsmith_input_00.png><img src=/images/20240420/langsmith_input_00.png alt="Figura 4 - Step di costruzione dell&amp;rsquo;URL"></a><figcaption>Figura 4 - Step di costruzione dell'URL</figcaption></figure></p><p><figure><a href=/images/20240420/langsmith_input_01.png><img src=/images/20240420/langsmith_input_01.png alt="Figura 5 - Prompt finale e sintesi della risposta all&amp;rsquo;utente in linguggio naturale"></a><figcaption>Figura 5 - Prompt finale e sintesi della risposta all'utente in linguggio naturale</figcaption></figure></p><h2 id=conclusioni>Conclusioni<a hidden class=anchor aria-hidden=true href=#conclusioni>#</a></h2><p>Dando un occhio al codice sorgente di LangChain e alle chiamate che vengono fatte verso i modelli, tramite LangSmith, si vede chiaramente che l&rsquo;integrazioni di API Rest in una applicazione basata su LLM è veramente banale e basata su tecniche molto semplici di Prompt Engineering, che però consentono una integrazione estremamente potente tra le nuove applicazioni AI e i sistemi tradizionali.</p><p>A mio avviso, è uno degli esempi più chiari e cristallini di come oggi si possa (e forse si debba) <strong>reinterpretare l&rsquo;interazione uomo/macchina</strong> in termini di integrazione tra sistemi formali ben specificati con comportamento predicibile (es: qualunque sistema software tradizionale in azienda) e il linguaggio naturale.</p><p>LangChain ed altri framework consentono di fare qualcosa di simile anche a livello più basso, ad esempio interrogando un DB in linguaggio naturale e utilizzando un LLM per generare le query sottostanti.
Al di là delle questioni squisitamente tecniche e di performance, questo approccio è bello in teoria ma, sulla base della mia esperienza, ci sono diversi elementi che mi fanno pensare che esso non sia realmente applicabile se non in alcuni casi specifici poiché nella stragrande maggioranza dei casi ci sono stratificazioni applicative che durano anni e difficoltà a mantenere un data catalog auto-descrittivo di buon livello.
Al contrario, le API enterprise introducono un layer che quasi sempre parla una lingua più vicina al Business ed in generale hanno dei metadati auto-descrittivi.</p><section id=comments><script src=https://giscus.app/client.js data-repo=c-daniele/c-daniele.github.io data-repo-id=R_kgDOKIObxg data-category=Announcements data-category-id=DIC_kwDOKIObxs4Cu2th data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=it crossorigin=anonymous async></script></section></div><footer class=post-footer><ul class=post-tags><li><a href=https://c-daniele.github.io/it/tags/ai/>ai</a></li><li><a href=https://c-daniele.github.io/it/tags/langchain/>langchain</a></li><li><a href=https://c-daniele.github.io/it/tags/api/>api</a></li><li><a href=https://c-daniele.github.io/it/tags/rest/>rest</a></li><li><a href=https://c-daniele.github.io/it/tags/swagger/>swagger</a></li><li><a href=https://c-daniele.github.io/it/tags/openapi/>openapi</a></li></ul><nav class=paginav><a class=prev href=https://c-daniele.github.io/it/posts/2025-05-15-policy-puppetry/><span class=title>« Precedente</span><br><span>Policy Puppetry Prompt Injection</span></a>
<a class=next href=https://c-daniele.github.io/it/posts/2023-08-13-langchain-agents/><span class=title>Successivo »</span><br><span>Langchain pt. 2 - Analisi dati tramite Agenti</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Langchain pt. 3 - Come invocare API Rest in linguaggio naturale on twitter" href="https://twitter.com/intent/tweet/?text=Langchain%20pt.%203%20-%20Come%20invocare%20API%20Rest%20in%20linguaggio%20naturale&amp;url=https%3a%2f%2fc-daniele.github.io%2fit%2fposts%2f2024-04-20-langchain-api%2f&amp;hashtags=ai%2clangchain%2capi%2crest%2cswagger%2copenapi"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Langchain pt. 3 - Come invocare API Rest in linguaggio naturale on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fc-daniele.github.io%2fit%2fposts%2f2024-04-20-langchain-api%2f&amp;title=Langchain%20pt.%203%20-%20Come%20invocare%20API%20Rest%20in%20linguaggio%20naturale&amp;summary=Langchain%20pt.%203%20-%20Come%20invocare%20API%20Rest%20in%20linguaggio%20naturale&amp;source=https%3a%2f%2fc-daniele.github.io%2fit%2fposts%2f2024-04-20-langchain-api%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Langchain pt. 3 - Come invocare API Rest in linguaggio naturale on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fc-daniele.github.io%2fit%2fposts%2f2024-04-20-langchain-api%2f&title=Langchain%20pt.%203%20-%20Come%20invocare%20API%20Rest%20in%20linguaggio%20naturale"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Langchain pt. 3 - Come invocare API Rest in linguaggio naturale on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fc-daniele.github.io%2fit%2fposts%2f2024-04-20-langchain-api%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Langchain pt. 3 - Come invocare API Rest in linguaggio naturale on whatsapp" href="https://api.whatsapp.com/send?text=Langchain%20pt.%203%20-%20Come%20invocare%20API%20Rest%20in%20linguaggio%20naturale%20-%20https%3a%2f%2fc-daniele.github.io%2fit%2fposts%2f2024-04-20-langchain-api%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Langchain pt. 3 - Come invocare API Rest in linguaggio naturale on telegram" href="https://telegram.me/share/url?text=Langchain%20pt.%203%20-%20Come%20invocare%20API%20Rest%20in%20linguaggio%20naturale&amp;url=https%3a%2f%2fc-daniele.github.io%2fit%2fposts%2f2024-04-20-langchain-api%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Langchain pt. 3 - Come invocare API Rest in linguaggio naturale on ycombinator" href="https://news.ycombinator.com/submitlink?t=Langchain%20pt.%203%20-%20Come%20invocare%20API%20Rest%20in%20linguaggio%20naturale&u=https%3a%2f%2fc-daniele.github.io%2fit%2fposts%2f2024-04-20-langchain-api%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://c-daniele.github.io/it/>Cdani's Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copia";function s(){t.innerHTML="copiato!",setTimeout(()=>{t.innerHTML="copia"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>