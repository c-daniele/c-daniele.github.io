<!doctype html><html lang=en-IT><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>Langchain pt. 3 - Come invocare API Rest in linguaggio naturale - Cdani's Blog</title><meta name=Description content="Cdani's Blog"><meta property="og:url" content="https://c-daniele.github.io/it/posts/2024-04-20-langchain-api/"><meta property="og:site_name" content="Cdani's Blog"><meta property="og:title" content="Langchain pt. 3 - Come invocare API Rest in linguaggio naturale"><meta property="og:description" content="Intro L’anno scorso, Gartner ha inserito la Generative AI nella fase di picco di aspettative all’interno del suo modello di Hype Cycle per il mondo della AI.
Recentemente alcuni nomi importanti tra le grandi aziende del settore hanno paragonato l’entusiasmo della GenAI alla bolla dotcom. Inoltre sono circolate delle indiscrezioni intorno ai principali Cloud Providers, secondo le quali essi stiano addirittura dando indicazioni ai loro Sales Team di rallentare l’entusiasmo dimostrato verso i clienti nei confronti delle iniziative di GenAI, o comunque di utilizzare un approccio cauto e consapevole dei costi e dei reali benefici. E’ già iniziata la discesa verso la “fossa della disillusione”?"><meta property="og:locale" content="en_IT"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-04-20T19:00:00+02:00"><meta property="article:modified_time" content="2024-04-20T19:00:00+02:00"><meta property="article:tag" content="GenAI"><meta property="article:tag" content="Langchain"><meta property="article:tag" content="Api"><meta property="article:tag" content="Swagger"><meta property="article:tag" content="OpenAPI"><meta property="og:image" content="https://c-daniele.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://c-daniele.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Langchain pt. 3 - Come invocare API Rest in linguaggio naturale"><meta name=twitter:description content="Intro L’anno scorso, Gartner ha inserito la Generative AI nella fase di picco di aspettative all’interno del suo modello di Hype Cycle per il mondo della AI.
Recentemente alcuni nomi importanti tra le grandi aziende del settore hanno paragonato l’entusiasmo della GenAI alla bolla dotcom. Inoltre sono circolate delle indiscrezioni intorno ai principali Cloud Providers, secondo le quali essi stiano addirittura dando indicazioni ai loro Sales Team di rallentare l’entusiasmo dimostrato verso i clienti nei confronti delle iniziative di GenAI, o comunque di utilizzare un approccio cauto e consapevole dei costi e dei reali benefici. E’ già iniziata la discesa verso la “fossa della disillusione”?"><meta name=application-name content="Cdani's Blog"><meta name=apple-mobile-web-app-title content="Cdani's Blog"><meta name=referrer content="no-referrer"><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel=icon href=/logo_cd_v3.svg><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#00872b><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://c-daniele.github.io/it/posts/2024-04-20-langchain-api/><link rel=prev href=https://c-daniele.github.io/it/posts/2023-08-13-langchain-agents/><link rel=next href=https://c-daniele.github.io/it/posts/2025-05-15-policy-puppetry/><link rel=stylesheet href=/css/style.min.css><link rel=preload href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.7.2/css/all.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.7.2/css/all.min.css></noscript><link rel=preload href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Langchain pt. 3 - Come invocare API Rest in linguaggio naturale","inLanguage":"en-IT","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/c-daniele.github.io\/it\/posts\/2024-04-20-langchain-api\/"},"genre":"posts","keywords":"GenAI, Langchain, Api, Swagger, OpenAPI","wordcount":2173,"url":"https:\/\/c-daniele.github.io\/it\/posts\/2024-04-20-langchain-api\/","datePublished":"2024-04-20T19:00:00+02:00","dateModified":"2024-04-20T19:00:00+02:00","publisher":{"@type":"Organization","name":""},"author":{"@type":"Person","name":"Me"},"description":""}</script></head><body data-header-desktop=fixed data-header-mobile=auto><script>const query=window.matchMedia("(prefers-color-scheme: dark)");function applyTheme(){let e=window.localStorage?.getItem("theme")||"auto",t=e==="dark"||e==="auto"&&query.matches;document.body.setAttribute("theme",t?"dark":"light"),document.body.setAttribute("cfg-theme",e)}applyTheme(),query.addEventListener("change",applyTheme)</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/it/ title="Cdani's Blog"><img class="lazyload logo" src=/svg/loading.min.svg data-src=/logo_cd_v3.svg data-srcset="/logo_cd_v3.svg, /logo_cd_v3.svg 1.5x, /logo_cd_v3.svg 2x" data-sizes=auto alt=/logo_cd_v3.svg title=/logo_cd_v3.svg>Cdani's Blog</a></div><div class=menu><div class=menu-inner><a class=menu-item href=/it/posts/>Archive </a><a class=menu-item href=/it/tags/>Tags </a><span class="menu-item delimiter"></span><span class="menu-item search" id=search-desktop>
<input type=text placeholder=Search... id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=Cerca><i class="fas fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=Chiaro><i class="fas fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-desktop><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i>
</span></span><a href=javascript:void(0); class="menu-item theme-switch" title="Cambiare il tema"><i class="fas fa-adjust fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="menu-item language" title="Scegliere la lingua"><i class="fa fa-globe fa-fw" aria-hidden=true></i>
<select class=language-select id=language-select-desktop onchange="location=this.value"><option value=/it/posts/2024-04-20-langchain-api/ selected>Italiano</option><option value=/en/posts/2024-04-20-langchain-api/>English</option></select></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/it/ title="Cdani's Blog"><img class="lazyload logo" src=/svg/loading.min.svg data-src=/logo_cd_v3.svg data-srcset="/logo_cd_v3.svg, /logo_cd_v3.svg 1.5x, /logo_cd_v3.svg 2x" data-sizes=auto alt=/logo_cd_v3.svg title=/logo_cd_v3.svg>Cdani's Blog</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><div class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=Search... id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=Cerca><i class="fas fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=Chiaro><i class="fas fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-mobile><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>Annulla</a></div><a class=menu-item href=/it/posts/ title>Archive</a><a class=menu-item href=/it/tags/ title>Tags</a><a href=javascript:void(0); class="menu-item theme-switch" title="Cambiare il tema">
<i class="fas fa-adjust fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class=menu-item title="Scegliere la lingua"><i class="fa fa-globe fa-fw" aria-hidden=true></i>
<select class=language-select onchange="location=this.value"><option value=/it/posts/2024-04-20-langchain-api/ selected>Italiano</option><option value=/en/posts/2024-04-20-langchain-api/>English</option></select></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>Contenuti</h2><div class=toc-content id=toc-content-auto></div></div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">Langchain pt. 3 - Come invocare API Rest in linguaggio naturale</h1><div class=post-meta><div class=post-meta-line><span class=post-author><a href=/it/ title=Author rel=author class=author><i class="fas fa-user-circle fa-fw" aria-hidden=true></i>Me</a></span>&nbsp;<span class=post-category>incluso in <a href=/it/categories/software-development/><i class="far fa-folder fa-fw" aria-hidden=true></i>Software Development</a>&nbsp;<a href=/it/categories/artificial-intelligence/><i class="far fa-folder fa-fw" aria-hidden=true></i>Artificial Intelligence</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw" aria-hidden=true></i>&nbsp;<time datetime="April 20, 2024">April 20, 2024</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden=true></i>&nbsp;2173 parole&nbsp;
<i class="far fa-clock fa-fw" aria-hidden=true></i>&nbsp;11 minuti&nbsp;</div></div><div class="details toc" id=toc-static data-kept><div class="details-summary toc-title"><span>Contenuti</span>
<span><i class="details-icon fas fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#intro>Intro</a></li><li><a href=#preambolo-tecnico>Preambolo tecnico</a></li><li><a href=#le-api>Le API</a></li><li><a href=#apichain>APIChain</a></li><li><a href=#il-test>Il Test</a></li><li><a href=#dietro-le-quinte>Dietro le quinte</a></li><li><a href=#conclusioni>Conclusioni</a></li></ul></nav></div></div><div class=content id=content><h2 id=intro>Intro</h2><p>L&rsquo;anno scorso, Gartner ha inserito la Generative AI nella fase di picco di aspettative all&rsquo;interno del suo modello di <a href=https://www.gartner.com/en/articles/what-s-new-in-artificial-intelligence-from-the-2023-gartner-hype-cycle target=_blank rel="noopener noreffer">Hype Cycle</a> per il mondo della AI.</p><p>Recentemente alcuni nomi importanti tra le grandi aziende del settore <a href=https://www.wired.com/story/amazons-cloud-boss-selipsky-generative-ai-hype/ target=_blank rel="noopener noreffer">hanno paragonato l&rsquo;entusiasmo della GenAI alla bolla <em>dotcom</em></a>.
Inoltre sono <a href="https://www.theinformation.com/articles/generative-ai-providers-quietly-tamp-down-expectations?ref=wheresyoured.at" target=_blank rel="noopener noreffer">circolate delle indiscrezioni</a> intorno ai principali Cloud Providers, secondo le quali essi stiano addirittura dando indicazioni ai loro Sales Team di rallentare l&rsquo;entusiasmo dimostrato verso i clienti nei confronti delle iniziative di GenAI, o comunque di utilizzare un approccio cauto e consapevole dei costi e dei reali benefici.
E&rsquo; già iniziata la discesa verso la <a href=https://it.wikipedia.org/wiki/Hype_cycle target=_blank rel="noopener noreffer">&ldquo;fossa della disillusione&rdquo;</a>?</p><p><figure><a href=/images/20240420/Hype-Cycle-General.png><img src=/images/20240420/Hype-Cycle-General.png alt="Figura 1 - Modello Hype cycle"></a><figcaption>Figura 1 - Modello Hype cycle</figcaption></figure></p><p>D&rsquo;altro canto, è anche possibile che questa volta il classico modello Hype Cycle non sia applicabile .
Rispetto ad altri trend trasformativi e tecnologici, si sta andando molto velocemente a regime verso una fase di consapevolezza e maturazione.
Si iniziano infatti a vedere alcuni trend del mercato, che vanno oltre la semplice corsa al modello più potente in termini di &ldquo;forza bruta&rdquo;.</p><p>Alcuni esempi:</p><ul><li>Molte aziende stanno lavorando su modelli relativamente piccoli ed eseguibili anche in locale, esempi:<ul><li>Meta e Qualcomm hanno appena annunciato un <a href=https://www.qualcomm.com/news/releases/2024/04/qualcomm-enables-meta-llama-3-to-run-on-devices-powered-by-snapd target=_blank rel="noopener noreffer">accordo di collaborazione</a> per l&rsquo;ottimizzazione dei modelli <em>Llama3</em> al fine di essere eseguiti direttamente sui dispositivi equipaggiati con le future piattaforme top di gamma Snapdragon</li><li>H2O ha lanciato un modello linguistico superleggero denominato <a href=https://venturebeat.com/ai/h2o-ai-releases-danube-a-super-tiny-llm-for-mobile-applications/ target=_blank rel="noopener noreffer"><em>Danube</em></a>, derivato da <em>Llama2</em> e pensato per essere eseguito su device mobili</li><li>Apple <a href=https://www.bloomberg.com/news/newsletters/2024-04-21/apple-aapl-growth-opportunities-southeast-asia-and-africa-lower-end-iphone-lv9itkna target=_blank rel="noopener noreffer">sembra stia lavorando</a> ad un modello linguistico &ldquo;on-device&rdquo; e disponibile offline sui dispositivi di Cupertino</li></ul></li><li>Più o meno tutti i grandi sviluppatori di LLM stanno introducendo delle soluzioni multi-modali</li><li>Stanno nascendo diversi framework e prodotti che permettono di costruire soluzioni complesse e modulari e che utilizzano i modelli LLM come building block per costruire applicazioni <em>&ldquo;AI-powered&rdquo;</em> complesse e vendor-agnostic<ul><li>In altre parole, per fare un parallelo con quello che è successo ormai molti anni fa con la nascita dell&rsquo;ingegneria del software, tali prodotti stanno spianando la strada alla <strong>&ldquo;Ingegneria dell&rsquo;AI&rdquo;</strong></li></ul></li></ul><p>Relativamente a quest&rsquo;ultimo punto, LangChain va proprio in questa strada.
E&rsquo; uno dei framework per l&rsquo;AI Open Source al momento più completi e potenti. Fornisce un grande controllo e adattabilità per vari casi d&rsquo;uso e offre una maggiore granularità rispetto ad altri framework come, ad esempio, <em>LlamaIndex</em>.
Una delle features che ho testato in questi giorni è l&rsquo;integrazione del framework con API Rest esterne, specificate secondo uno standard preciso (es: <em>Swagger</em>, <em>OpenApi</em>) o anche descritte in linguaggio naturale.</p><p>In questo articolo, mostrerò come sia possibile integrare direttamente &ldquo;a runtime&rdquo; una API di terze parti all&rsquo;interno di un banalissimo chatbot, ed interrogare l&rsquo;API in linguaggio naturale <em>senza alcuna preventiva conoscenza</em> delle specifiche di tale API.</p><h2 id=preambolo-tecnico>Preambolo tecnico</h2><p>Il codice mostrato nel seguito e che <a href=https://github.com/c-daniele/langchain_tests/tree/main/01.APIChain target=_blank rel="noopener noreffer">condivido su GitHub</a> si basa sull&rsquo;utilizzo di <em>OpenAI</em> e di <em>Bedrock</em>. Quest&rsquo;ultimo, per chi non lo conoscesse, è il servizio di AWS che dà accesso a diversi modelli tra cui <em>Llama2</em>, <em>Claude</em>, <em>Mistral</em> e il modello proprietario di AWS denominato <em>Titan</em>.
Il codice è estremamente semplice e segue i seguenti step logici:</p><ol><li>Inizializzazione delle variabili di ambiente</li><li>Creazione del modello LLM</li><li>Retrieve dinamico delle specifiche di una determinata API</li><li>Inizializzazione e invocazione del componente <em>APIChain</em>. Questo componente, nello specifico, applica alcune semplici tecniche di Prompt Engineering per eseguire le seguenti 3 azioni:<ol><li>Prendere in input la domanda dell&rsquo;utente in linguaggio naturale e costruire, tramite il LLM, l&rsquo;URL da invocare</li><li>Invocare l&rsquo;URL così costruito tramite una chiamata HTTP</li><li>Inglobare la risposta ottenuta dalla chiamata HTTP all&rsquo;interno di una nuova invocazione del LLM ed estrapolare l&rsquo;informazione richiesta dall&rsquo;utente</li></ol></li></ol><p>Il processo è riassunto nel seguente diagramma di flusso:</p><p><figure><a href=/images/20240420/FlowChart-APIChain-v2.png><img src=/images/20240420/FlowChart-APIChain-v2.png alt="Figura 2 - Diagramma di Flusso"></a><figcaption>Figura 2 - Diagramma di Flusso</figcaption></figure></p><p>Nel codice che seguirà, ho cablato per semplicità anche le interazioni utente nel codice sotto forma di stringhe statiche, ma nulla vieta di ottenere dinamicamente questi input dall&rsquo;utente, ad esempio tramite una interfaccia a Chatbot, oppure configurare le API da una apposita sezione applicativa e fare poi plug&amp;play direttamente nel chatbot per <strong>aggiungere funzionalità a runtime</strong>.</p><p>In altre parole, con pochissimo sforzo, è possibile realizzare un <strong>chatbot completamente agnostico</strong> rispetto alle specifiche API e adeguarsi dinamicamente alle esigenze, inserendo liberamente i riferimenti a nuove API o recependo al volo le eventuali modifiche alla interfaccia.</p><p>Il caso d&rsquo;uso più immediato potrebbe essere, ad esempio, quello di uno strumento di <strong>customer care</strong> che si integra con le API aziendali per restituire direttamente al cliente informazioni relative ai suoi ordini, prodotti, segnalazioni etc. La disponibilità di queste informazioni potrebbe essere infatti sviluppata in maniera incrementale, potenziando le funzionalità esposte dal chatbot senza tuttavia toccare una riga di codice ed utilizzando un <strong>approccio plug&amp;play</strong> delle API all&rsquo;interno del processo dialogico esistente.</p><p>Allargando il discorso e andando verso un contesto più Enterprise, possiamo immaginare lo scenario di una moderna Data Platform che metta a disposizione i principali KPI aziendali sotto forma di <em>Data-APIs</em> a beneficio di chiunque in azienda voglia consultarli rapidamente tramite il chatbot aziendale.</p><p>In altre parole, le possibilità sono tantissime.</p><h2 id=le-api>Le API</h2><p>Le API che ho utilizzato per fare le prove sono le seguenti:</p><ul><li><em><a href=https://www.klarna.com/us/shopping/public/openai/v0/api-docs/ target=_blank rel="noopener noreffer">klarna.com</a></em><ul><li>per chi non la conoscesse, KLARNA è una fintech svedese che offre una piattaforma di pagamento a rate per chi fa shopping online. E&rsquo; integrata in tantissime piattaforme di shopping online. L&rsquo;API in questione è accessibile gratuitamente, mette a disposizione un metodo per cercare prodotti sulla base di attributi descrittivi, prezzo, brand etc ed è operativa solo in alcuni mercati (US, GB, DE, SE, DK).</li></ul></li><li><em><a href=https://raw.githubusercontent.com/open-meteo/open-meteo/main/openapi.yml target=_blank rel="noopener noreffer">open-meteo</a></em><ul><li>E&rsquo; una API gratuita che mette a disposizione dati meteoreologici. Il caso più comune è quello in cui interroghiamo l&rsquo;API per ottenere le condizioni meteo in una determinata città, in termini di temperatura, precipitazioni, visibilità, etc.</li></ul></li></ul><h2 id=apichain>APIChain</h2><p>Il componente che andremo ad utilizzare all&rsquo;interno della suite di LangChain si chiama <em>APIChain</em> ed è banalmente un wrapper che contiene:</p><ul><li>Una istanza di un <em>LLMChain</em>, che serve per costruire l&rsquo;URL e i parametri HTTP a partire dalla domanda in linguaggio naturale</li><li>Un wrapper del componente <em>request</em>, che viene utilizzato per inviare la chiamata HTTP</li><li>Una istanza di un <em>LLMChain</em> che serve per costruire la response in linguaggio naturale a partire dal payload della Response HTTP</li><li>Alcuni prompt che servono per creare il contesto corretto e implementare efficacemente le chiamate al LLM</li></ul><p>Per quanto riguarda i Prompt che mette a disposizione il componente APIChain, durante i test mi sono reso conto che essi non funzionavano correttamente con tutti i LLM (ad es: funzionavano con OpenAI, ma non con Llama2, Claude, etc). Pertanto, ho costruito una versione leggermente migliore del prompt e ho proposto la modifica sul repo ufficiale (vedremo se l&rsquo;accetteranno &#x1f603; ).</p><h2 id=il-test>Il Test</h2><p>Nella prima parte del codice facciamo l&rsquo;inizializzazione dei componenti di base e creiamo i modelli.</p><p>Alcune note:</p><ul><li>le variabili di ambiente relative alla integrazione con OPEN_AI e AWS devono essere configurate nel file <em>.env</em></li><li>all&rsquo;interno del file &ldquo;libs.py&rdquo; ho creato un wrapper per l&rsquo;istanziazione del modello LLM. Troverete tutto nel <a href=https://github.com/c-daniele/langchain_tests/tree/main/01.APIChain target=_blank rel="noopener noreffer">repository GitHub</a></li><li>I modelli di Bedrock che ho utilizzato si trovano al momento solo in alcune Region. Dunque occorre fare attenzione alle impostazioni della region e dei costi associati all&rsquo;utilizzo</li></ul><div class="code-block open" style="counter-reset:code-block 0"><div class="code-header language-python"><span class=code-title><i class="arrow fas fa-angle-right fa-fw" aria-hidden=true></i></span>
<span class=ellipses><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i></span>
<span class=copy title="Copia negli appunti"><i class="far fa-copy fa-fw" aria-hidden=true></i></span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>langchain.chains</span> <span class=kn>import</span> <span class=n>APIChain</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>dotenv</span> <span class=kn>import</span> <span class=n>load_dotenv</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>httpx</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>logging</span> <span class=k>as</span> <span class=nn>logger</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>sys</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># see &#34;libs.py&#34; file</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>libs</span> <span class=kn>import</span> <span class=o>*</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># see &#34;prompt_improved.py&#34; file</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>prompt_improved</span> <span class=kn>import</span> <span class=o>*</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Set WARNING Logger levels help print only meaningful text</span>
</span></span><span class=line><span class=cl><span class=n>logger</span><span class=o>.</span><span class=n>basicConfig</span><span class=p>(</span><span class=n>stream</span><span class=o>=</span><span class=n>sys</span><span class=o>.</span><span class=n>stdout</span><span class=p>,</span> <span class=n>level</span><span class=o>=</span><span class=n>logger</span><span class=o>.</span><span class=n>WARNING</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>logger</span><span class=o>.</span><span class=n>getLogger</span><span class=p>(</span><span class=s1>&#39;botocore&#39;</span><span class=p>)</span><span class=o>.</span><span class=n>setLevel</span><span class=p>(</span><span class=n>logger</span><span class=o>.</span><span class=n>WARNING</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>logger</span><span class=o>.</span><span class=n>getLogger</span><span class=p>(</span><span class=s1>&#39;httpx&#39;</span><span class=p>)</span><span class=o>.</span><span class=n>setLevel</span><span class=p>(</span><span class=n>logger</span><span class=o>.</span><span class=n>WARNING</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># loading ENV variables</span>
</span></span><span class=line><span class=cl><span class=n>load_dotenv</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Initialize Models</span>
</span></span><span class=line><span class=cl><span class=n>gpt35</span> <span class=o>=</span> <span class=n>create_llm</span><span class=p>(</span><span class=n>model</span><span class=o>=</span><span class=p>{</span><span class=s2>&#34;provider&#34;</span><span class=p>:</span><span class=s2>&#34;OpenAI&#34;</span><span class=p>,</span> <span class=s2>&#34;value&#34;</span><span class=p>:</span> <span class=s2>&#34;gpt-3.5-turbo&#34;</span><span class=p>},</span> <span class=n>model_kwargs</span><span class=o>=</span><span class=p>{</span><span class=s2>&#34;temperature&#34;</span><span class=p>:</span> <span class=mf>0.1</span><span class=p>})</span>
</span></span><span class=line><span class=cl><span class=n>gpt4</span> <span class=o>=</span> <span class=n>create_llm</span><span class=p>(</span><span class=n>model</span><span class=o>=</span><span class=p>{</span><span class=s2>&#34;provider&#34;</span><span class=p>:</span><span class=s2>&#34;OpenAI&#34;</span><span class=p>,</span> <span class=s2>&#34;value&#34;</span><span class=p>:</span> <span class=s2>&#34;gpt-4&#34;</span><span class=p>},</span> <span class=n>model_kwargs</span><span class=o>=</span><span class=p>{</span><span class=s2>&#34;temperature&#34;</span><span class=p>:</span> <span class=mf>0.1</span><span class=p>})</span>
</span></span><span class=line><span class=cl><span class=n>claude3</span> <span class=o>=</span> <span class=n>create_llm</span><span class=p>(</span><span class=n>model</span><span class=o>=</span><span class=p>{</span><span class=s2>&#34;provider&#34;</span><span class=p>:</span><span class=s2>&#34;Anthropic&#34;</span><span class=p>,</span> <span class=s2>&#34;value&#34;</span><span class=p>:</span> <span class=s2>&#34;anthropic.claude-3-sonnet-20240229-v1:0&#34;</span><span class=p>},</span> <span class=n>model_kwargs</span><span class=o>=</span><span class=p>{</span><span class=s2>&#34;temperature&#34;</span><span class=p>:</span> <span class=mf>0.1</span><span class=p>})</span>
</span></span><span class=line><span class=cl><span class=n>llama2</span> <span class=o>=</span> <span class=n>create_llm</span><span class=p>(</span><span class=n>model</span><span class=o>=</span><span class=p>{</span><span class=s2>&#34;provider&#34;</span><span class=p>:</span><span class=s2>&#34;Meta&#34;</span><span class=p>,</span> <span class=s2>&#34;value&#34;</span><span class=p>:</span> <span class=s2>&#34;meta.llama2-70b-chat-v1&#34;</span><span class=p>},</span> <span class=n>model_kwargs</span><span class=o>=</span><span class=kc>None</span><span class=p>)</span></span></span></code></pre></div></div><p>Ok, adesso vediamo come integrare dinamicamente il file descrittore della interfaccia e passarlo al componente APIChain.
La variabile <em>&ldquo;limit_to_domains&rdquo;</em> è utilizzata per introdurre un meccanismo di sicurezza che limita i domini verso cui indirizzare le richieste. In teoria potrebbe essere impostato a &ldquo;None&rdquo; per non impostare alcun vincolo, ma è sempre preferibile evitarlo.
Le 2 variabili <em>api_url_prompt</em> e <em>api_response_prompt</em> consentono di customizzare i prompt da passare all&rsquo;LLM. Come ho anticipato in precedenza, ho impostato 2 prompt che si sono dimostrati più robusti di quelli di default.</p><div class="code-block open" style="counter-reset:code-block -1"><div class="code-header language-python"><span class=code-title><i class="arrow fas fa-angle-right fa-fw" aria-hidden=true></i></span>
<span class=ellipses><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i></span>
<span class=copy title="Copia negli appunti"><i class="far fa-copy fa-fw" aria-hidden=true></i></span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Dynamically retrieve swagger</span>
</span></span><span class=line><span class=cl><span class=n>output</span> <span class=o>=</span> <span class=n>httpx</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;https://www.klarna.com/us/shopping/public/openai/v0/api-docs/&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>swagger</span> <span class=o>=</span> <span class=n>output</span><span class=o>.</span><span class=n>text</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># build the APIChain </span>
</span></span><span class=line><span class=cl><span class=n>chain</span> <span class=o>=</span> <span class=n>APIChain</span><span class=o>.</span><span class=n>from_llm_and_api_docs</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>llm</span><span class=o>=</span><span class=n>gpt4</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>api_docs</span><span class=o>=</span><span class=n>swagger</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>verbose</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>limit_to_domains</span><span class=o>=</span><span class=p>[</span><span class=s2>&#34;klarna.com&#34;</span><span class=p>,</span> <span class=s2>&#34;https://www.klarna.com/&#34;</span><span class=p>,</span> <span class=s2>&#34;https://www.klarna.com&#34;</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=n>api_url_prompt</span><span class=o>=</span><span class=n>FINE_TUNED_API_URL_PROMPT</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>api_response_prompt</span><span class=o>=</span><span class=n>FINE_TUNED_API_RESPONSE_PROMPT</span>
</span></span><span class=line><span class=cl><span class=p>)</span></span></span></code></pre></div></div><p>A questo punto è tutto impostato. Possiamo fare una domanda e passarla al framework per poi restituire l&rsquo;output all&rsquo;utente finale.
Ho chiesto di ricercare 3 magliette con un tetto massimo di 50 dollari e di ritornare prezzo, descrizione e link.</p><div class="code-block open" style="counter-reset:code-block -1"><div class="code-header language-python"><span class=code-title><i class="arrow fas fa-angle-right fa-fw" aria-hidden=true></i></span>
<span class=ellipses><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i></span>
<span class=copy title="Copia negli appunti"><i class="far fa-copy fa-fw" aria-hidden=true></i></span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Ask a question to the Chain</span>
</span></span><span class=line><span class=cl><span class=n>response</span> <span class=o>=</span> <span class=n>chain</span><span class=o>.</span><span class=n>invoke</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;Find 3 t-shirts, max 50 USD. For each Product print the Description, the Price and the corresponding URL&#34;</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Print the Chain Output</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>response</span><span class=p>[</span><span class=s1>&#39;output&#39;</span><span class=p>])</span></span></span></code></pre></div></div><p>Questo è l&rsquo;output che ho ottenuto al primo tentativo:</p><div class="code-block code-line-numbers open" style="counter-reset:code-block 0"><div class="code-header language-text"><span class=code-title><i class="arrow fas fa-angle-right fa-fw" aria-hidden=true></i></span>
<span class=ellipses><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i></span>
<span class=copy title="Copia negli appunti"><i class="far fa-copy fa-fw" aria-hidden=true></i></span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>1. *Product: Polo Ralph Lauren Men&#39;s Slim Fit Wicking Crew Undershirts 3-pack - White*
</span></span><span class=line><span class=cl>   *Price: $37.99*
</span></span><span class=line><span class=cl>   *URL: https://www.klarna.com/us/shopping/pl/cl10001/3207134809/Clothing/Polo-Ralph-Lauren-Men-s-Slim-Fit-Wicking-Crew-Undershirts-3-pack-White/?utm_source=openai&amp;ref-site=openai_plugin*
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>2. *Product: Lacoste Men&#39;s T-shirts 3-pack - Black*
</span></span><span class=line><span class=cl>   *Price: $31.90*
</span></span><span class=line><span class=cl>   *URL: https://www.klarna.com/us/shopping/pl/cl10001/3202043025/Clothing/Lacoste-Men-s-T-shirts-3-pack-Black/?utm_source=openai&amp;ref-site=openai_plugin*
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>3. *Product: SKIMS Cotton Jersey T-shirt*
</span></span><span class=line><span class=cl>   *Price: $48.00*
</span></span><span class=line><span class=cl>   *URL: https://www.klarna.com/us/shopping/pl/cl10001/3202929904/Clothing/SKIMS-Cotton-Jersey-T-shirt/?utm_source=openai&amp;ref-site=openai_plugin*</span></span></code></pre></div></div><p>Non male!</p><p>Ho fatto parecchie altre prove con gli altri modelli e ho ottenuto performance simili anche se, come mi aspettavo, GPT4 e Claude3 sono mediamente più precisi.</p><p>Per quanto riguarda la seconda API, il codice è praticamente identico, a parte il riferimento all&rsquo;URL descrittore (swagger), la variabile <em>limit_to_domains</em> che deve essere coerente con l&rsquo;API e la domanda dell&rsquo;utente. Riporto dunque solo la seconda e la terza parte dello script python.</p><p><em>Punto di attenzione</em>: non esiste uno swagger ufficiale per questa API, quindi ho usato il file YAML che si trova su GitHub. A volte le chiamate verso GitHub vanno in errore. In tal caso suggerisco di riprovare un paio di volte.</p><div class="code-block open" style="counter-reset:code-block -1"><div class="code-header language-python"><span class=code-title><i class="arrow fas fa-angle-right fa-fw" aria-hidden=true></i></span>
<span class=ellipses><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i></span>
<span class=copy title="Copia negli appunti"><i class="far fa-copy fa-fw" aria-hidden=true></i></span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Dynamically retrieve swagger</span>
</span></span><span class=line><span class=cl><span class=n>output</span> <span class=o>=</span> <span class=n>httpx</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;https://raw.githubusercontent.com/open-meteo/open-meteo/main/openapi.yml&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>meteo_swagger</span> <span class=o>=</span> <span class=n>output</span><span class=o>.</span><span class=n>text</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># build the APIChain </span>
</span></span><span class=line><span class=cl><span class=n>chain</span> <span class=o>=</span> <span class=n>APIChain</span><span class=o>.</span><span class=n>from_llm_and_api_docs</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>llm</span><span class=o>=</span><span class=n>claude3</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>api_docs</span><span class=o>=</span><span class=n>meteo_swagger</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>verbose</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>limit_to_domains</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>api_url_prompt</span><span class=o>=</span><span class=n>FINE_TUNED_API_URL_PROMPT</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>api_response_prompt</span><span class=o>=</span><span class=n>FINE_TUNED_API_RESPONSE_PROMPT</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># Ask a question to the Chain</span>
</span></span><span class=line><span class=cl><span class=n>response</span> <span class=o>=</span> <span class=n>chain</span><span class=o>.</span><span class=n>invoke</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;What is the weather like right now in Munich, Germany in degrees Fahrenheit?&#34;</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Print the Chain Output</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>response</span><span class=p>[</span><span class=s1>&#39;output&#39;</span><span class=p>])</span></span></span></code></pre></div></div><p>Il risultato con Claude, e con GPT 3,5 e GPT4 è in linea con le aspettative. Le 2 chiamate di Langchain hanno costruito correttamente l&rsquo;URL ed interpretato il risultato, trasformandolo in linguaggio naturale.</p><div class="code-block code-line-numbers open" style="counter-reset:code-block 0"><div class="code-header language-text"><span class=code-title><i class="arrow fas fa-angle-right fa-fw" aria-hidden=true></i></span>
<span class=ellipses><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i></span>
<span class=copy title="Copia negli appunti"><i class="far fa-copy fa-fw" aria-hidden=true></i></span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>The current weather in Munich, Germany is 45.7°F with a wind speed of 17.7 km/h coming from 264° direction.</span></span></code></pre></div></div><p>Il test con Llama2 non è andato a buon fine. In particolare, ha evidentemente avuto allucinazioni nella prima chiamata, in cui LangChain crea l&rsquo;URL, inventando alcuni parametri non specificati nello swagger.</p><h2 id=dietro-le-quinte>Dietro le quinte</h2><p>Un altro tool molto interessante della suite di LangChain si chiama <a href=https://www.langchain.com/langsmith target=_blank rel="noopener noreffer"><em>LangSmith</em></a>, che consente di fare monitoraggio e profiling su tutte le invocazioni del modello.
Oltre a questo, consente di fare tante altre cose, come ad esempio:</p><ul><li>il debugging avanzato</li><li>la continua valutazione dei task tramite la definizione di dataset predefiniti e di criteri di valutazione</li><li>l&rsquo;annotazione dei modelli per aggiungere metadati o feedback utente</li><li>molte altre features relative al monitoraggio e al miglioramento delle applicazioni basate su LangChain</li></ul><p>Utilizzando LangSmith, è possibile vedere graficamente il macroprocesso le chiamate ai modelli sottostanti.</p><p><figure><a href=/images/20240420/langsmith_input_root_calls.png><img src=/images/20240420/langsmith_input_root_calls.png alt="Figura 3 - Struttura delle chiamate di LangChain"></a><figcaption>Figura 3 - Struttura delle chiamate di LangChain</figcaption></figure></p><p>In particolare, in Figura 3, si vede chiaramente la struttura ad albero delle chiamate, identificata dalla sigla <em>&ldquo;APIChain&rdquo;</em>, che è composta da 2 chain figlie di tipo LLM, a cui corrispondono altrettante chiamate verso OpenAI.
Altra cosa estremamente utile è il <em>numero di token utilizzati</em> e il <em>costo stimato delle singole chiamate</em>.</p><p>Andando nel dettaglio sulle singole chiamate al LLM, possiamo vedere il prompt realmente passato in input al modello e la response sulla singola invocazione.</p><p><figure><a href=/images/20240420/langsmith_input_00.png><img src=/images/20240420/langsmith_input_00.png alt="Figura 4 - Step di costruzione dell&rsquo;URL"></a><figcaption>Figura 4 - Step di costruzione dell'URL</figcaption></figure></p><p><figure><a href=/images/20240420/langsmith_input_01.png><img src=/images/20240420/langsmith_input_01.png alt="Figura 5 - Prompt finale e sintesi della risposta all&rsquo;utente in linguggio naturale"></a><figcaption>Figura 5 - Prompt finale e sintesi della risposta all'utente in linguggio naturale</figcaption></figure></p><h2 id=conclusioni>Conclusioni</h2><p>Dando un occhio al codice sorgente di LangChain e alle chiamate che vengono fatte verso i modelli, tramite LangSmith, si vede chiaramente che l&rsquo;integrazioni di API Rest in una applicazione basata su LLM è veramente banale e basata su tecniche molto semplici di Prompt Engineering, che però consentono una integrazione estremamente potente tra le nuove applicazioni AI e i sistemi tradizionali.</p><p>A mio avviso, è uno degli esempi più chiari e cristallini di come oggi si possa (e forse si debba) <strong>reinterpretare l&rsquo;interazione uomo/macchina</strong> in termini di integrazione tra sistemi formali ben specificati con comportamento predicibile (es: qualunque sistema software tradizionale in azienda) e il linguaggio naturale.</p><p>LangChain ed altri framework consentono di fare qualcosa di simile anche a livello più basso, ad esempio interrogando un DB in linguaggio naturale e utilizzando un LLM per generare le query sottostanti.
Al di là delle questioni squisitamente tecniche e di performance, questo approccio è bello in teoria ma, sulla base della mia esperienza, ci sono diversi elementi che mi fanno pensare che esso non sia realmente applicabile se non in alcuni casi specifici poiché nella stragrande maggioranza dei casi ci sono stratificazioni applicative che durano anni e difficoltà a mantenere un data catalog auto-descrittivo di buon livello.
Al contrario, le API enterprise introducono un layer che quasi sempre parla una lingua più vicina al Business ed in generale hanno dei metadati auto-descrittivi.</p></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>Aggiornato il April 20, 2024</span></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span><a href=javascript:void(0); title="Condividi su X" data-sharer=x data-url=https://c-daniele.github.io/it/posts/2024-04-20-langchain-api/ data-title="Langchain pt. 3 - Come invocare API Rest in linguaggio naturale" data-hashtags=GenAI,Langchain,Api,Swagger,OpenAPI><i class="fab fa-x-twitter fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Condividi su Threads" data-sharer=threads data-url=https://c-daniele.github.io/it/posts/2024-04-20-langchain-api/ data-title="Langchain pt. 3 - Come invocare API Rest in linguaggio naturale"><i class="fab fa-threads fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Condividi su Facebook" data-sharer=facebook data-url=https://c-daniele.github.io/it/posts/2024-04-20-langchain-api/ data-hashtag=GenAI><i class="fab fa-facebook-square fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Condividi su Hacker News" data-sharer=hackernews data-url=https://c-daniele.github.io/it/posts/2024-04-20-langchain-api/ data-title="Langchain pt. 3 - Come invocare API Rest in linguaggio naturale"><i class="fab fa-hacker-news fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Condividi su Line" data-sharer=line data-url=https://c-daniele.github.io/it/posts/2024-04-20-langchain-api/ data-title="Langchain pt. 3 - Come invocare API Rest in linguaggio naturale"><i data-svg-src=https://cdn.jsdelivr.net/npm/simple-icons@15.14.0/icons/line.svg aria-hidden=true></i></a><a href=javascript:void(0); title="Condividi su 微博" data-sharer=weibo data-url=https://c-daniele.github.io/it/posts/2024-04-20-langchain-api/ data-title="Langchain pt. 3 - Come invocare API Rest in linguaggio naturale"><i class="fab fa-weibo fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Condividi su Diaspora" data-sharer=diaspora data-url=https://c-daniele.github.io/it/posts/2024-04-20-langchain-api/ data-title="Langchain pt. 3 - Come invocare API Rest in linguaggio naturale" data-description><i class="fab fa-diaspora fa-fw" aria-hidden=true></i></a><a href="https://t.me/share/url?url=https%3a%2f%2fc-daniele.github.io%2fit%2fposts%2f2024-04-20-langchain-api%2f&amp;text=Langchain%20pt.%203%20-%20Come%20invocare%20API%20Rest%20in%20linguaggio%20naturale" target=_blank title="Condividi su Telegram"><i class="fab fa-telegram fa-fw" aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fas fa-tags fa-fw" aria-hidden=true></i>&nbsp;<a href=/it/tags/genai/>GenAI</a>,&nbsp;<a href=/it/tags/langchain/>Langchain</a>,&nbsp;<a href=/it/tags/api/>Api</a>,&nbsp;<a href=/it/tags/swagger/>Swagger</a>,&nbsp;<a href=/it/tags/openapi/>OpenAPI</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>Indietro</a></span>&nbsp;|&nbsp;<span><a href=/it/>Home</a></span></section></div><div class=post-nav><a href=/it/posts/2023-08-13-langchain-agents/ class=prev rel=prev title="Langchain pt. 2 - Analisi dati tramite Agenti"><i class="fas fa-angle-left fa-fw" aria-hidden=true></i>Langchain pt. 2 - Analisi dati tramite Agenti</a>
<a href=/it/posts/2025-05-15-policy-puppetry/ class=next rel=next title="Policy Puppetry Prompt Injection">Policy Puppetry Prompt Injection<i class="fas fa-angle-right fa-fw" aria-hidden=true></i></a></div></div></article></div></main></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title="Torna all'inizio"><i class="fas fa-arrow-up fa-fw" aria-hidden=true></i></a></div><div id=fixed-buttons-hidden><a href=# id=view-comments class=fixed-button title="Vedi commenti"><i class="fas fa-comment fa-fw" aria-hidden=true></i></a></div><script src=https://cdn.jsdelivr.net/npm/autocomplete.js@0.38.1/dist/autocomplete.min.js></script><script src=https://cdn.jsdelivr.net/npm/lunr@2.3.9/lunr.min.js></script><script src=/lib/lunr/lunr.stemmer.support.min.js></script><script src=/lib/lunr/lunr.it.min.js></script><script src=https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js></script><script src=https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js></script><script src=https://cdn.jsdelivr.net/npm/sharer.js@0.5.2/sharer.min.js></script><script>window.config={comment:{},search:{highlightTag:"em",lunrIndexURL:"/it/index.json",lunrLanguageCode:"it",maxResultLength:10,noResultsFound:"Nessun risultato trovato",snippetLength:30,type:"lunr"}}</script><script src=/js/theme.min.js></script><script>var dnt,doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-8NZQZ3Z1RN")}</script><script src="https://www.googletagmanager.com/gtag/js?id=G-8NZQZ3Z1RN" async></script></body></html>