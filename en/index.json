[{"content":"Intro In the previous article I gave a very brief overview of LangChain, describing its main concepts with some examples with unstructured data in pdf format.\nFollowing the same approach, in this article we will give a brief introduction to Agents and proceed by trying to answer an ambitious question:\nleveraging these new AI tools, can we carry out data analysis on our DB without any knowledge of SQL nor of the data model, simply starting from a text prompt in natural language?\nAgents LLMs are extremely powerful, but they seem to be completely ineffective in answering questions that require detailed knowledge not tightly integrated into model training. Over the internet there are dozens of examples that manage to catch ChatGPT out through hallucinations or lack of response (eg: weather forecasts, latest news, gossip or even specific mathematical operations).\nFrameworks like LangChain can overcome these limitations by defining specific and data-aware components, but usually the actions performed by the framework are predetermined. In other words, the framework uses a Language Model to perform some actions, but they are \u0026ldquo;hardcoded\u0026rdquo; and in many cases this can make AI models completely ineffective, because you can\u0026rsquo;t drive specific actions based on user input.\nThat\u0026rsquo;s where agents come into play Agents are components that have a series of tools available to perform specific actions, such as doing a search on Wikipedia or Google, or executing Python code or even accessing the local file system.\nAgents use an LLM to understand user input and decide how to proceed accordingly:\nWhich tool among the provided set should be the most appropriate to use? What\u0026rsquo;s the input text to be passed as input to the tool? Have we reached the goal thus answering the initial question or should we repeat step 1 and 2 again? This approach was inspired from a framework called ReAct which has been defined at the end of 2022 by a joint team of researchers from Google and Princeton University. You can find here the original paper. In LangChain, there are several implementations of such approach, but the most common is called \u0026ldquo;Zero-shot ReAct\u0026rdquo; and can be described at a high level with the following workflow.\nFigure 1 - Simplified workflow for \u0026#34;Zero-shot ReAct\u0026#34; agents Please note that this type of agents have no memory and so discriminate their actions only on the basis of the input text and the description of the tool. It is therefore very important that the tools also include an effective description for the purpose of a correct interpretation by the LLM.\nLangChain tools are sometimes grouped into groups called \u0026ldquo;Toolkits\u0026rdquo;. In the official documentation you will find a default toolkit called \u0026ldquo;SQLDatabaseToolkit\u0026rdquo;, to configure a SQL agent.\nThe scenario As I said at the beginning of the article, we want to do a real analysis on the data present in a relational DB, assuming we have no knowledge of the data model nor SQL skills. The starting point will be a text prompt in natural language.\nFrom a technical standpoint, the task is very easy because, in addition to the toolkit, LangChain provides a utility method for defining a SqlAgent to which we only have to provide some parameters such as the DB connection, the type of LLM, etc..\nAt first sight the examples given in the official documentation look already very interesting and complete. In addition to trivial use cases (eg DESCRIBE a table), it is shown how the agent is able to make inferences on metadata to understand how to aggregate data or JOIN 2 or more tables.\nIn order to not repeat the same example taken from the documentation and introduce some more complications, I\u0026rsquo;ve decided to create an enhanced version of the standard toolkit, which is also able to do searches over the internet.\nThe dataset The official documentation includes examples that make use of a test DB based on SqlLite and called \u0026ldquo;Chinook\u0026rdquo;, which simulates a media store and which you can also download from the official SqlLite site.\nTaking a look at the data model and the data itself, I was suspicious of the exciting results they reported, because the DB is in my opinion not representative of a real case, because:\nthe names of the tables and columns are all defined in English and self-describing, moreover no naming convention has been used the DB seems practically in 3NF and this is pretty unlikely in scenarios where you want to do pure data analysis local SqlLite .db file? This is a case very far from reality! From past personal projects, I have made available an Athena DB on my AWS account with some data structures that in my opinion are more representative of real use-cases. The data is related to OpenData of the Municipality of Milan, relating to transits within the AreaC gates. AreaC is the the main LTZ (Limited Traffic Zone) for the city of Milan. Actually Athena is not a real DB, but rather a SQL-Engine based on Presto, but with the appropriate configurations, AWS provides an endpoint that allows you to access it as if it were a real DBMS.\nThe Data Model is very simple: it\u0026rsquo;s made of 2 fact tables, containing the AreaC crossing events (detail + aggregate), both linked to a decoding table of the entrances, in which some attributes are indicated, including the exact geographical position of the passage. In all 3 cases, these are Iceberg tables stored on S3 and mapped to Athena via the Glue catalog.\nThe original datasets have been taken from the official OpenData portal. This is about 4 years of data (about 101 million records in the biggest fact table).\nPlease find below the DDLs of the tables with some comments that I have added here for simplicity (and which therefore the agent did not have available\u0026hellip;).\nFigure 2 - Detailed fact table DDL Figure 3 - Aggregated fact table DDL Figure 2 - Gate decoding table DDL In the aggregate table, in addition to removing some attributes, I\u0026rsquo;ve made a sort of pivot on the type of power supply, calculating the different transits in COLUMN instead of ROW, reducing the cardinality by about 92%. Other than that, the 2 fact tables are pretty much identical.\nThe Gate decoding table contains the descriptive name and the geographical coordinates.\nAs you can see, I\u0026rsquo;ve used a naming convention, but this is deliberately imperfect, for example it is a mix of English and Italian.\nThe software setup Please find below the basic imports and configurations of the main python code:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 from langchain.agents import create_sql_agent from langchain.sql_database import SQLDatabase from langchain.llms.openai import OpenAI from langchain.agents.agent_types import AgentType import os from urllib.parse import quote_plus from ExtendedSqlDatabaseToolkit import * # set the environment variables from dotenv import load_dotenv load_dotenv() # connection string conn_str = ( \u0026#34;awsathena+rest://{aws_access_key_id}:{aws_secret_access_key}@\u0026#34; \u0026#34;athena.{region_name}.amazonaws.com:443/\u0026#34; \u0026#34;{schema_name}?s3_staging_dir={s3_staging_dir}\u0026amp;work_group={wg}\u0026#34; ) # database initialization db = SQLDatabase.from_uri(conn_str.format( aws_access_key_id=quote_plus(os.environ[\u0026#39;AWS_AK\u0026#39;]), aws_secret_access_key=quote_plus(os.environ[\u0026#39;AWS_SAK\u0026#39;]), region_name=os.environ[\u0026#39;AWS_REGION\u0026#39;], schema_name=os.environ[\u0026#39;AWS_ATHENA_SCHEMA\u0026#39;], s3_staging_dir=quote_plus(os.environ[\u0026#39;AWS_S3_OUT\u0026#39;]), wg=os.environ[\u0026#39;AWS_ATHENA_WG\u0026#39;] ) , include_tables=[\u0026#39;xtdpl1_ingressi_detailed\u0026#39;, \u0026#39;xtdpl1_ingressi_aggregated\u0026#39;, \u0026#39;xtdpl1_varchi\u0026#39;] , sample_rows_in_table_info=2) # toolkit definition through Custom Class toolkit = ExtendedSqlDatabaseToolkit(db=db, llm=OpenAI(temperature=0)) # Agent initialization agent_executor = create_sql_agent( llm=OpenAI(temperature=0), toolkit=toolkit, verbose=True, agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION ) LangChain makes use of SQLAlchemy so it already allows accessing to a large number of DBMSs without the need of reinventing the wheel.\nNote that in addition to the AWS-related environment variables explicitly referenced above, you also need to set the following variables:\nOPENAI_API_KEY: associated with the OpenAI account, which is mandatory to invoke their LLM APIs SERPAPI_API_KEY: associated with the SerpApi account, in order to programmatically search on Google. There is a FREE version with a 100 monthly calls limit The options at lines 29 and 30 has been provided to limit the agent\u0026rsquo;s range of action and prevent it from making too extensive reasoning on the whole catalog and dataset. Without these options, it\u0026rsquo;s pretty easy to overcome the tokens limit in the OpenAI API call.\nThe toolkit instantiated at line 34 is my custom class, extending the standard SQLToolkit made available by LangChain. Being a few lines of code, I\u0026rsquo;m also adding this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 \u0026#34;\u0026#34;\u0026#34;Enhanced Toolkit for interacting with SQL databases and search over the internet\u0026#34;\u0026#34;\u0026#34; from typing import List from langchain.agents.agent_toolkits import SQLDatabaseToolkit from langchain.tools import BaseTool from langchain.agents import load_tools class ExtendedSqlDatabaseToolkit(SQLDatabaseToolkit): \u0026#34;\u0026#34;\u0026#34;Enhanced Toolkit for interacting with SQL databases and search over the internet\u0026#34;\u0026#34;\u0026#34; def get_tools(self) -\u0026gt; List[BaseTool]: sqlTools = super().get_tools() additionalTools = load_tools([\u0026#34;serpapi\u0026#34;], llm=self.llm) return additionalTools+sqlTools Please note that, in addition to the explicitly referenced libraries, you also need to install the \u0026ldquo;openai\u0026rdquo; and \u0026ldquo;pyathena\u0026rdquo; libraries.\nThe challenges I\u0026rsquo;ve asked the agent several questions, trying to stress-test different components (eg: identifying the semantics of the data, understand what to search over the internet, when/if it\u0026rsquo;s better switching from the aggregated table to the detailed one, etc etc).\nHere I\u0026rsquo;m just going to describe a couple of examples, but I will make some general considerations first.\nThe default language model from the OpenAI libraries is Text-davinci-003. This model is much larger and more expensive (about 10 times more!) than the one used by ChatGPT (GPT-3.5-Turbo). There are a lot of articles and papers describing the effectiveness of both in different use cases. Despite being smaller (6 vs 175 billion parameters), the latter one can have the same or in some cases even better performances.\nI\u0026rsquo;ve almost exclusively used the first of the 2 and the few tests I did with GPT-3.5-Turbo had much worse results. I didn\u0026rsquo;t spend any time for trying to understand the reason for such performance gap. Maybe I will dedicate another post to this topic.\nUse case A - Simple KPI Evaluation look for the coordinates and the descriptive name of the Gate (\u0026lsquo;varco\u0026rsquo;) with the hightest sum of diesel vehicle transits during the whole month of August 2020. Only consider the real AreaC transits and exclude the service vehicles\nThe returned output is represented in the following picture. If you take a look at the lines starting with \u0026ldquo;Action\u0026rdquo;, \u0026ldquo;Observation\u0026rdquo; and \u0026ldquo;Thought\u0026rdquo;, you will see that we\u0026rsquo;ve got what is expected according to the \u0026ldquo;Zero-shot ReAct\u0026rdquo; model.\nFigure 3 - Use case A output The agent starts with the identification of the Action (sql_db_list_tables) and of the input (no input in this case), obtaining (Observation) the 3 tables on which we programmatically restricted the its visibility. In theory, the tool could explore the entire catalog but, as mentioned above, I wanted to narrow the scope to avoid exceeding the OpenAI tokens threshold.\nNow the agent gives control to the LLM (Thought) to identify the next action by which it states that the only 2 interesting tables are the aggregate fact table and the gate decoding table.\nPlease note that it\u0026rsquo;s assuming to query the aggregated table over the detail one, but I am a little surprised that this deduction has been made solely on the table naming, since metadata and data fetching will be made later. From this point of view, the final result might not be the correct one if the 2 tables had a different data perimeter (for example if the aggregated table only contained the last year).\nAfter fetching the metedata and extracting some sample data rows, the LLM builds the query. In this specific case you\u0026rsquo;ll see that the model guesses the syntax of the query on the first attempt, but I have experienced several cases in which it tries, correcting the syntax each time until it reaches the definitive query.\nThe rest is self-described in the image.\nA couple of comments:\nthe model was able to perfectly implement the filters I had in mind in the prompt, through naming and/or data inference I\u0026rsquo;ve made other few attempts by removing the aggregate table and leaving only the detail one and I got the same result. However, it should be noted that the detailed table has the KPI represented in a ROW instead of a COLUMN, so in that case the model understood that the filter \u0026ldquo;des_tipo_alimentazione = \u0026lsquo;diesel\u0026rsquo;\u0026rdquo; was to be applied as expected, no google search was done, because it was obviously not needed Use case B - further info requested look for the coordinates and the descriptive name of the Gate (\u0026lsquo;varco\u0026rsquo;) with the hightest sum of diesel vehicle transits during the whole month of August 2020, including only areac entrances and excluding service vehicles. Also give me back the 3 gates with the smallest distance from it\nHere, the LLM surprised me: I\u0026rsquo;ve added the final sentence to force the Agent doing a search over the internet, but I forgot that the distance could be also evaluated with mathematical operations using just geographical coordinates, therefore the tool (namely the LLM model behind it) performed the whole task within the DB as shown in Figure 8.\nI\u0026rsquo;ve removed the first part of the output as this is identical to use case A.\nFigure 4 - Use case B output Use case C - combining query+search The extreme simplicity of the data model didn\u0026rsquo;t help me so much in creating a meaningful request, so I had to do some prompt engineering in order to force a web search. Finally I managed to get something relevant with a prompt like this:\nlook for the coordinates and the descriptive name of the Gate (\u0026lsquo;varco\u0026rsquo;) with the hightest sum of diesel vehicle transits during the whole month of August 2020, including only areac entrances and excluding service vehicles. Also give me back the bus stop closest to this gate\nFigure 4 - Use case C output Here I\u0026rsquo;ve experienced some differences in the Agent behaviour between the Italian and English input prompt, but in general, it\u0026rsquo;s doing the expected job.\nConclusions As I already wrote in the previous article, the learning curve to adopt LangChain is quite shallow. A few lines of code are enough to obtain a \u0026ldquo;wow\u0026rdquo; effect and allow anyone to implement their own custom solution, also integrated with the rest of the enterprise ecosystem (repositories, Data APIs, mail servers, shared file systems, \u0026hellip;) and√ü with their own LLM (for example, you can integrate your own installation of Llama 2 on-premise) where you don\u0026rsquo;t want to share data outside the Organization.\nOn the other hand, the examples I have given above are to be considered as simplified tutorials to familiarize yourself with the framework.\nTo get real solutions, a more structured approach is needed, which better exploits the characteristics of the framework and takes into account the detailed capabilities of the models.\nFor example, I\u0026rsquo;ve realized that it was not a wise choice to combine SQL and SerpApi search functionality in a single toolkit and that it would have been better to integrate those 2 capabilities through separate agents/chains.\nAs another example, I\u0026rsquo;ve noticed that in the \u0026ldquo;experimental\u0026rdquo; package there is a class called \u0026ldquo;SQLDatabaseChain\u0026rdquo; which allows you to develop a Tool Sql from scratch, with a few lines of code. This way, you can completely avoid the usage of the standard toolkit and choose a more tailored solution:\n1 2 3 4 5 6 7 8 9 10 11 12 sql_chain = SQLDatabaseChain.from_llm(llm=llm, db=db, verbose=True) sql_tool = Tool( name=\u0026#39;Areac DB\u0026#39;, func=sql_chain.run, description=\u0026#34;This database contains the data related to transits for all gates of the LTZ of Milan, which is called \\\u0026#34;AreaC\\\u0026#34;\u0026#34; \u0026#34; The most important tables are: xtdpl1_ingressi_aggregated and xtdpl1_varchi.\u0026#34; \u0026#34; The table xtdpl1_ingressi_aggregated contains most important measures, like the number of all transits for each of the gates and for each day of the year.\u0026#34; \u0026#34; The field identifying the Time dimension is \u0026#39;dat_year_month\u0026#39; and it\u0026#39;s NUMERIC with a standard YYYYMM format.\u0026#34; \u0026#34; The field \u0026#39;flg_areac\u0026#39; is BOOLEAN (true/false) and it\u0026#39;s used to identify the actual \\\u0026#34;AreaC\\\u0026#34; payed transits.\u0026#34; \u0026#34; The xtdpl1_varchi table contains the gates transcoding. The primary key is the \u0026#39;id\u0026#39; field, identifying the specific gate. The other fields are descriptive attributes.\u0026#34; ) Since the agent uses the LLM to decide which tool to use and how to use it solely based on the tool description, this approach has the great advantage of improving performance just by adding an effective description of the DB within the tool, without modifying the LLM model at all. In my case, for example, I\u0026rsquo;ve incrementally added a large number of details, experiencing every time a concrete improvement in the tool performance.\n","permalink":"https://c-daniele.github.io/en/posts/2023-08-13-langchain-agents/","summary":"Intro In the previous article I gave a very brief overview of LangChain, describing its main concepts with some examples with unstructured data in pdf format.\nFollowing the same approach, in this article we will give a brief introduction to Agents and proceed by trying to answer an ambitious question:\nleveraging these new AI tools, can we carry out data analysis on our DB without any knowledge of SQL nor of the data model, simply starting from a text prompt in natural language?","title":"Langchain pt. 2 - Data Analysis through Agents"},{"content":"Intro For those unfamiliar with it, LangChain is a framework for developing applications that make use of LLMs.\nAs the name suggests, LangChain is based on the concept of LLM Chain, which combines 3 elements:\nPrompt Templates: they refer to a reproducible way to generate a prompt. Contains a text string (\u0026ldquo;the model\u0026rdquo;), which can accept a series of parameters from the end user and generates the definitive prompt which is passed as input to the model The language model (LLM): LangChain integrates with the most important providers (OpenAI, Cohere, Hugging Face, etc) Output Parsers: allow to extract structure data form from the answers returned by the linguistic model The framework has 2 very interesting features:\nit can extend LLM knowledge with your own database, leveragine structured and unstructured datasets it provides the \u0026ldquo;Agent\u0026rdquo; capabilities by which the action itself is an output returned from the LLM I was quite curious about the first item, so I\u0026rsquo;ve started making some tests. I don\u0026rsquo;t want to make a critical analysis of the model performance, but rather verify how easy is to integrate the framework into one\u0026rsquo;s own database.\nIntegration with unstructured data I didn\u0026rsquo;t know where to start, so I took a look at the most documented use cases on the internet. I\u0026rsquo;ve found a lot of documentation related to parsing PDF files, so it seemed like an area I could experiment with a lot.\nIn the official documentation there\u0026rsquo;s a special section related to the \u0026ldquo;Data Connection\u0026rdquo;, which I found incredibly clear and intuitive. I will try to summarize here the most important points.\nThe building blocks made available by LangChain are the following:\nDocument: it\u0026rsquo;s an abstraction containing both the data in textual form and the associated metadata Document loaders: They are classes that allow you to extract text and metadata from a specific type of data in order to build the \u0026ldquo;Document\u0026rdquo; Document transformers: it\u0026rsquo;s used to process Documents. Since LLMs usually have strong limitations in terms of available tokens, the most common transformation is related to chunk splitting, through which it is possible to submit calls to the LLM provider in series or in parallel. There are also other types of transformers, for example: redundancy reduction, translation, metadata extraction, etc\u0026hellip; Text embedding: it\u0026rsquo;s the operation of translating a portion of text into an N-dimensional vector model, which is the core component for the semantic search operations based on similarity indexes and implemented by calculating vector distances across such N-dimensional space Vector stores: it stores the embeddings inside a vector DB Engine, which is capable of efficiently returning the vectors closest to the input text (and therefore the portions of text that are most similar). It\u0026rsquo;s possible to exploit some open source DB engines to run everything locally, or to integrate with some market products that obviously offer much better performance (eg: Pinecone) Retrievers: it\u0026rsquo;s an interface that returns documents from an unstructured query. It is a slightly more general concept than a Vector Store, but unlike the latter, it only allows you to return documents and not necessarily store them Chains So let\u0026rsquo;s talk about the main components: the chains.\nLangChain introduces this concept which represents a useful abstraction to implement applications that make use of LLMs in a simple and modular way. There are many predefined Chains, the most common are:\nRetrievalQA: it responds to user input from the output returned by a retriever ConversationalRetrievalChain: it\u0026rsquo;s similar to RetrievalQA. It adds the capability to build a conversational experience through the history of exchanged messages Summarize: as the name suggests, it enable text summarization The experiment I took a 2017 research paper, written by some researchers at the Oak Ridge National Laboratory (ORNL) and other university institutes, which proposes an implementation of a quantum computing algorithm for a Portfolio Optimization problem.\nIn particular, the article describes the advantages deriving from the use of a variant of the Markowitz model (QUBO) on D-Wave type quantum devices.\nThe complete article can be found at this link.\nBeing passionate about these topics, but not having a solid theoretical basis, I can understand the main points of the paper, but I have no competence to evaluate the reliability or the goodness of the results, so I decide to ask OpenAI for a critical analysis, passing through LangChain.\nSurprisingly, it only took me a few hours and less than 20 lines of code to get a working prototype with an overall good result.\nThe code Here you can find the source code. It\u0026rsquo;s almost self-describing, but I\u0026rsquo;m adding some further notes and comments below.\nfrom langchain.llms import OpenAI from langchain.document_loaders import PyPDFLoader from langchain.chains.summarize import load_summarize_chain from langchain import OpenAI, PromptTemplate from dotenv import load_dotenv load_dotenv() loader = PyPDFLoader(\u0026#34;docs/pdf/102.pdf\u0026#34;) docs = [] docs.extend(loader.load()) prompt_template = \u0026#34;\u0026#34;\u0026#34;Write a negative critique of this research article, questioning its findings and applicability: {text} CRITICS:\u0026#34;\u0026#34;\u0026#34; PROMPT = PromptTemplate(template=prompt_template, input_variables=[\u0026#34;text\u0026#34;]) chain = load_summarize_chain(OpenAI(temperature=0.5), chain_type=\u0026#34;map_reduce\u0026#34;, return_intermediate_steps=False, map_prompt=PROMPT, combine_prompt=PROMPT) chain({\u0026#34;input_documents\u0026#34;: docs}, return_only_outputs=True) The call to load_dotenv() is for loading the OPENAI_API_KEY environment variable, associated with my OpenAI account. The first instructions are used to initialize the loader and create the associated \u0026ldquo;Documents\u0026rdquo;. Right after that, I define the prompt template and call the \u0026ldquo;Summarize\u0026rdquo; chain. The first parameter passed to the Chain is an OpenAI-type LLM instantiation, which I deliberately configured with a temperature of 0.5, to push the creativity a little bit. The chain_type is \u0026ldquo;map_reduce\u0026rdquo; because the text is long enough to exceed the 4096 token limit imposed by the provider I\u0026rsquo;m using (ChatGPT 3.5).\nOutcomes Please find below a couple of outcomes I got from the above code:\nThis research article presents a variety of methodologies for analyzing portfolio selection in the presence of multiple criteria. However, the results are based on mathematical theories and simulation models rather than real data. Also, using a fixed $100 budget is unrealistic and does not represent real-world conditions. Furthermore, most of the techniques described in this article are not applicable to real financial markets, as they do not take into account factors such as market volatility, economic cycles, etc. Consequently, the findings and applicability of this research article are questioned.\nThis research article does not provide sufficient evidence of the results obtained, nor does it demonstrate the applicability of the proposed model in practice. Markowitz\u0026rsquo;s theory of portfolio selection is not applicable to all investors, as it does not consider the complications that can arise in practice, such as taxes or fees. Also, the use of the D-Wave quantum optimizer has not been adequately demonstrated. The relaxed wording does not involve buying shares, which means that the optimal outcome is not realistic. Also, the abundance of zeros in the data is unrealistic. Most of the sample code and applications mentioned in the article are computation related\nConclusions As I said earlier, I can\u0026rsquo;t claim any strong subject mastership for evaluate if the outcome i got from the LLM have solid foundations or not, but at first sight they seem to be realistic and strong enough to suggest some thoughts.\nFor example, if we take the comment about the investment of only $100, this is actually a simplified scenario they considered in the paper, but to be honest I have no idea if this factor can effectively question the results.\nIn general, the thing that amazed me is the ease with which the framework makes the building blocks available for developing AI applications, without reinventing the wheel and integrating very well with the main providers and market products.\nI realize the example shown is really trivial, but it opens up a world of possibilities. I\u0026rsquo;m doing other tests by expanding the dataset and trying to answer slightly more complex questions. Stay tuned\n","permalink":"https://c-daniele.github.io/en/posts/2023-07-24-langchain-helloworld-pdf/","summary":"Intro For those unfamiliar with it, LangChain is a framework for developing applications that make use of LLMs.\nAs the name suggests, LangChain is based on the concept of LLM Chain, which combines 3 elements:\nPrompt Templates: they refer to a reproducible way to generate a prompt. Contains a text string (\u0026ldquo;the model\u0026rdquo;), which can accept a series of parameters from the end user and generates the definitive prompt which is passed as input to the model The language model (LLM): LangChain integrates with the most important providers (OpenAI, Cohere, Hugging Face, etc) Output Parsers: allow to extract structure data form from the answers returned by the linguistic model The framework has 2 very interesting features:","title":"LLM - Experimenting LangChain - Part 1"}]