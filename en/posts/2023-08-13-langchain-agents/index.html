<!doctype html><html lang=en-US><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>Langchain pt. 2 - Data Analysis through Agents - Cdani's Blog</title><meta name=Description content="Cdani's Blog"><meta property="og:url" content="https://c-daniele.github.io/en/posts/2023-08-13-langchain-agents/"><meta property="og:site_name" content="Cdani's Blog"><meta property="og:title" content="Langchain pt. 2 - Data Analysis through Agents"><meta property="og:description" content="Intro In the previous article I gave a very brief overview of LangChain, describing its main concepts with some examples with unstructured data in pdf format.
Following the same approach, in this article we will give a brief introduction to Agents and proceed by trying to answer an ambitious question:
leveraging these new AI tools, can we carry out data analysis on our DB without any knowledge of SQL nor of the data model, simply starting from a text prompt in natural language?"><meta property="og:locale" content="en_US"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-08-13T19:00:00+02:00"><meta property="article:modified_time" content="2023-08-13T19:00:00+02:00"><meta property="article:tag" content="GenAI"><meta property="article:tag" content="Langchain"><meta property="article:tag" content="Agents"><meta property="og:image" content="https://c-daniele.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://c-daniele.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Langchain pt. 2 - Data Analysis through Agents"><meta name=twitter:description content="Intro In the previous article I gave a very brief overview of LangChain, describing its main concepts with some examples with unstructured data in pdf format.
Following the same approach, in this article we will give a brief introduction to Agents and proceed by trying to answer an ambitious question:
leveraging these new AI tools, can we carry out data analysis on our DB without any knowledge of SQL nor of the data model, simply starting from a text prompt in natural language?"><meta name=application-name content="Cdani's Blog"><meta name=apple-mobile-web-app-title content="Cdani's Blog"><meta name=referrer content="no-referrer"><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel=icon href=/logo_cd_v3.svg><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#00872b><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://c-daniele.github.io/en/posts/2023-08-13-langchain-agents/><link rel=prev href=https://c-daniele.github.io/en/posts/2023-07-24-langchain-helloworld-pdf/><link rel=next href=https://c-daniele.github.io/en/posts/2024-04-20-langchain-api/><link rel=stylesheet href=/css/style.min.css><link rel=preload href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.7.2/css/all.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.7.2/css/all.min.css></noscript><link rel=preload href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Langchain pt. 2 - Data Analysis through Agents","inLanguage":"en-US","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/c-daniele.github.io\/en\/posts\/2023-08-13-langchain-agents\/"},"genre":"posts","keywords":"GenAI, Langchain, Agents","wordcount":2678,"url":"https:\/\/c-daniele.github.io\/en\/posts\/2023-08-13-langchain-agents\/","datePublished":"2023-08-13T19:00:00+02:00","dateModified":"2023-08-13T19:00:00+02:00","publisher":{"@type":"Organization","name":""},"author":{"@type":"Person","name":"Me"},"description":""}</script></head><body data-header-desktop=fixed data-header-mobile=auto><script>const query=window.matchMedia("(prefers-color-scheme: dark)");function applyTheme(){let e=window.localStorage?.getItem("theme")||"auto",t=e==="dark"||e==="auto"&&query.matches;document.body.setAttribute("theme",t?"dark":"light"),document.body.setAttribute("cfg-theme",e)}applyTheme(),query.addEventListener("change",applyTheme)</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/en/ title="Cdani's Blog"><img class="lazyload logo" src=/svg/loading.min.svg data-src=/logo_cd_v3.svg data-srcset="/logo_cd_v3.svg, /logo_cd_v3.svg 1.5x, /logo_cd_v3.svg 2x" data-sizes=auto alt=/logo_cd_v3.svg title=/logo_cd_v3.svg>Cdani's Blog</a></div><div class=menu><div class=menu-inner><a class=menu-item href=/en/posts/>Archive </a><a class=menu-item href=/en/tags/>Tags </a><span class="menu-item delimiter"></span><span class="menu-item search" id=search-desktop>
<input type=text placeholder=Search... id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=Search><i class="fas fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=Clear><i class="fas fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-desktop><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i>
</span></span><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme"><i class="fas fa-adjust fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="menu-item language" title="Select Language"><i class="fa fa-globe fa-fw" aria-hidden=true></i>
<select class=language-select id=language-select-desktop onchange="location=this.value"><option value=/it/posts/2023-08-13-langchain-agents/>Italiano</option><option value=/en/posts/2023-08-13-langchain-agents/ selected>English</option></select></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/en/ title="Cdani's Blog"><img class="lazyload logo" src=/svg/loading.min.svg data-src=/logo_cd_v3.svg data-srcset="/logo_cd_v3.svg, /logo_cd_v3.svg 1.5x, /logo_cd_v3.svg 2x" data-sizes=auto alt=/logo_cd_v3.svg title=/logo_cd_v3.svg>Cdani's Blog</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><div class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=Search... id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=Search><i class="fas fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=Clear><i class="fas fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-mobile><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>Cancel</a></div><a class=menu-item href=/en/posts/ title>Archive</a><a class=menu-item href=/en/tags/ title>Tags</a><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme">
<i class="fas fa-adjust fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class=menu-item title="Select Language"><i class="fa fa-globe fa-fw" aria-hidden=true></i>
<select class=language-select onchange="location=this.value"><option value=/it/posts/2023-08-13-langchain-agents/>Italiano</option><option value=/en/posts/2023-08-13-langchain-agents/ selected>English</option></select></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>Contents</h2><div class=toc-content id=toc-content-auto></div></div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">Langchain pt. 2 - Data Analysis through Agents</h1><div class=post-meta><div class=post-meta-line><span class=post-author><a href=/en/ title=Author rel=author class=author><i class="fas fa-user-circle fa-fw" aria-hidden=true></i>Me</a></span>&nbsp;<span class=post-category>included in <a href=/en/categories/software-development/><i class="far fa-folder fa-fw" aria-hidden=true></i>Software Development</a>&nbsp;<a href=/en/categories/artificial-intelligence/><i class="far fa-folder fa-fw" aria-hidden=true></i>Artificial Intelligence</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw" aria-hidden=true></i>&nbsp;<time datetime="August 13, 2023">August 13, 2023</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden=true></i>&nbsp;2678 words&nbsp;
<i class="far fa-clock fa-fw" aria-hidden=true></i>&nbsp;13 minutes&nbsp;</div></div><div class="details toc" id=toc-static data-kept><div class="details-summary toc-title"><span>Contents</span>
<span><i class="details-icon fas fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#intro>Intro</a></li><li><a href=#agents>Agents</a></li><li><a href=#thats-where-agents-come-into-play>That&rsquo;s where agents come into play</a></li><li><a href=#the-scenario>The scenario</a><ul><li><a href=#the-dataset>The dataset</a></li><li><a href=#the-software-setup>The software setup</a></li></ul></li><li><a href=#the-challenges>The challenges</a><ul><li><a href=#use-case-a---simple-kpi-evaluation>Use case A - Simple KPI Evaluation</a></li><li><a href=#use-case-b---further-info-requested>Use case B - further info requested</a></li><li><a href=#use-case-c---combining-querysearch>Use case C - combining query+search</a></li></ul></li><li><a href=#conclusions>Conclusions</a></li></ul></nav></div></div><div class=content id=content><h2 id=intro>Intro</h2><p>In the previous article I gave a very brief overview of LangChain, describing its main concepts with some examples with unstructured data in pdf format.</p><p>Following the same approach, in this article we will give a brief introduction to Agents and proceed by trying to answer an ambitious question:</p><blockquote><p>leveraging these new AI tools, can we carry out data analysis on our DB without any knowledge of SQL nor of the data model, simply starting from a text prompt in natural language?</p></blockquote><h2 id=agents>Agents</h2><p>LLMs are extremely powerful, but they seem to be completely ineffective in answering questions that require detailed knowledge not tightly integrated into model training. Over the internet there are dozens of examples that manage to catch ChatGPT out through hallucinations or lack of response (eg: weather forecasts, latest news, gossip or even specific mathematical operations).</p><p>Frameworks like LangChain can overcome these limitations by defining specific and data-aware components, but usually the actions performed by the framework are predetermined. In other words, the framework uses a Language Model to perform some actions, <strong>but they are &ldquo;hardcoded&rdquo;</strong> and in many cases this can make AI models completely ineffective, because you can&rsquo;t drive specific actions based on user input.</p><h2 id=thats-where-agents-come-into-play>That&rsquo;s where agents come into play</h2><p>Agents are components that have a series of tools available to perform specific actions, such as doing a search on Wikipedia or Google, or executing Python code or even accessing the local file system.</p><p>Agents use an LLM to understand user input and decide how to proceed accordingly:</p><ol><li>Which tool among the provided set should be the most appropriate to use?</li><li>What&rsquo;s the input text to be passed as input to the tool?</li><li>Have we reached the goal thus answering the initial question or should we repeat step 1 and 2 again?</li></ol><p>This approach was inspired from a framework called <em>ReAct</em> which has been defined at the end of 2022 by a joint team of researchers from Google and Princeton University. You can find <a href=https://arxiv.org/abs/2210.03629 target=_blank rel="noopener noreffer">here</a> the original paper. In LangChain, there are several implementations of such approach, but the most common is called &ldquo;<em>Zero-shot ReAct</em>&rdquo; and can be described at a high level with the following workflow.</p><figure><img src=/images/20230813/Agent_Diagram_v2.png><figcaption><h4>Figure 1 - Simplified workflow for "Zero-shot ReAct" agents</h4></figcaption></figure><p>Please note that this type of agents have no memory and so discriminate their actions <strong>only on the basis of the input text and the description of the tool</strong>. It is therefore very important that the tools also include an effective description for the purpose of a correct interpretation by the LLM.</p><p>LangChain tools are sometimes grouped into groups called &ldquo;Toolkits&rdquo;. In the official documentation you will find a default toolkit called &ldquo;SQLDatabaseToolkit&rdquo;, to configure a SQL agent.</p><h2 id=the-scenario>The scenario</h2><p>As I said at the beginning of the article, we want to do a real analysis on the data present in a relational DB, assuming we have no knowledge of the data model nor SQL skills. The starting point will be a text prompt in natural language.</p><p>From a technical standpoint, the task is very easy because, in addition to the toolkit, LangChain provides a utility method for defining a SqlAgent to which we only have to provide some parameters such as the DB connection, the type of LLM, etc..</p><p>At first sight <a href=https://python.langchain.com/docs/integrations/toolkits/sql_database target=_blank rel="noopener noreffer">the examples given in the official documentation</a> look already very interesting and complete. In addition to trivial use cases (eg <em>DESCRIBE</em> a table), it is shown how the agent is able to make inferences on metadata to understand how to aggregate data or JOIN 2 or more tables.</p><p>In order to not repeat the same example taken from the documentation and introduce some more complications, I&rsquo;ve decided to create an enhanced version of the standard toolkit, which is also able to do searches over the internet.</p><h3 id=the-dataset>The dataset</h3><p>The official documentation includes examples that make use of a test DB based on SqlLite and called &ldquo;Chinook&rdquo;, which simulates a media store and which you can also download from the official SqlLite site.</p><p>Taking a look at the data model and the data itself, I was suspicious of the exciting results they reported, because the DB is in my opinion not representative of a real case, because:</p><ul><li>the names of the tables and columns are all defined in English and self-describing, moreover no naming convention has been used</li><li>the DB seems practically in 3NF and this is pretty unlikely in scenarios where you want to do pure data analysis</li><li>local SqlLite .db file? This is a case very far from reality!</li></ul><p>From past personal projects, I have made available an Athena DB on my AWS account with some data structures that in my opinion are more representative of real use-cases. The data is related to OpenData of the Municipality of Milan, relating to <strong>transits within the AreaC gates</strong>. AreaC is the the main LTZ (<em>Limited Traffic Zone</em>) for the city of Milan. Actually Athena is not a real DB, but rather a SQL-Engine based on Presto, but with the appropriate configurations, AWS provides an endpoint that allows you to access it as if it were a real DBMS.</p><p>The Data Model is very simple: it&rsquo;s made of 2 fact tables, containing the AreaC crossing events (detail + aggregate), both linked to a decoding table of the entrances, in which some attributes are indicated, including the exact geographical position of the passage. In all 3 cases, these are Iceberg tables stored on S3 and mapped to Athena via the Glue catalog.</p><p>The original datasets have been taken from the <a href=https://dati.comune.milano.it/group/tran target=_blank rel="noopener noreffer">official OpenData portal</a>. This is about 4 years of data (about 101 million records in the biggest fact table).</p><p>Please find below the DDLs of the tables with some comments that I have added here for simplicity (and which therefore the agent did not have available&mldr;).</p><figure><img src=/images/20230813/screenshot1.png><figcaption><h4>Figure 2 - Detailed fact table DDL</h4></figcaption></figure><figure><img src=/images/20230813/screenshot2.png><figcaption><h4>Figure 3 - Aggregated fact table DDL</h4></figcaption></figure><figure><img src=/images/20230813/screenshot3.png><figcaption><h4>Figure 2 - Gate decoding table DDL</h4></figcaption></figure><p>In the aggregate table, in addition to removing some attributes, I&rsquo;ve made a sort of pivot on the type of power supply, calculating the different transits in COLUMN instead of ROW, reducing the cardinality by about 92%. Other than that, the 2 fact tables are pretty much identical.</p><p>The Gate decoding table contains the descriptive name and the geographical coordinates.</p><p>As you can see, I&rsquo;ve used a naming convention, but this is <strong>deliberately imperfect</strong>, for example it is a mix of English and Italian.</p><h3 id=the-software-setup>The software setup</h3><p>Please find below the basic imports and configurations of the main python code:</p><div class="code-block open" style="counter-reset:code-block 0"><div class="code-header language-python"><span class=code-title><i class="arrow fas fa-angle-right fa-fw" aria-hidden=true></i></span>
<span class=ellipses><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i></span>
<span class=copy title="Copy to clipboard"><i class="far fa-copy fa-fw" aria-hidden=true></i></span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>langchain.agents</span> <span class=kn>import</span> <span class=n>create_sql_agent</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>langchain.sql_database</span> <span class=kn>import</span> <span class=n>SQLDatabase</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>langchain.llms.openai</span> <span class=kn>import</span> <span class=n>OpenAI</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>langchain.agents.agent_types</span> <span class=kn>import</span> <span class=n>AgentType</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>os</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>urllib.parse</span> <span class=kn>import</span> <span class=n>quote_plus</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>ExtendedSqlDatabaseToolkit</span> <span class=kn>import</span> <span class=o>*</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># set the environment variables</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>dotenv</span> <span class=kn>import</span> <span class=n>load_dotenv</span>
</span></span><span class=line><span class=cl><span class=n>load_dotenv</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># connection string</span>
</span></span><span class=line><span class=cl><span class=n>conn_str</span> <span class=o>=</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;awsathena+rest://</span><span class=si>{aws_access_key_id}</span><span class=s2>:</span><span class=si>{aws_secret_access_key}</span><span class=s2>@&#34;</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;athena.</span><span class=si>{region_name}</span><span class=s2>.amazonaws.com:443/&#34;</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;</span><span class=si>{schema_name}</span><span class=s2>?s3_staging_dir=</span><span class=si>{s3_staging_dir}</span><span class=s2>&amp;work_group=</span><span class=si>{wg}</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># database initialization</span>
</span></span><span class=line><span class=cl><span class=n>db</span> <span class=o>=</span> <span class=n>SQLDatabase</span><span class=o>.</span><span class=n>from_uri</span><span class=p>(</span><span class=n>conn_str</span><span class=o>.</span><span class=n>format</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>aws_access_key_id</span><span class=o>=</span><span class=n>quote_plus</span><span class=p>(</span><span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=s1>&#39;AWS_AK&#39;</span><span class=p>]),</span>
</span></span><span class=line><span class=cl>    <span class=n>aws_secret_access_key</span><span class=o>=</span><span class=n>quote_plus</span><span class=p>(</span><span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=s1>&#39;AWS_SAK&#39;</span><span class=p>]),</span>
</span></span><span class=line><span class=cl>    <span class=n>region_name</span><span class=o>=</span><span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=s1>&#39;AWS_REGION&#39;</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=n>schema_name</span><span class=o>=</span><span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=s1>&#39;AWS_ATHENA_SCHEMA&#39;</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=n>s3_staging_dir</span><span class=o>=</span><span class=n>quote_plus</span><span class=p>(</span><span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=s1>&#39;AWS_S3_OUT&#39;</span><span class=p>]),</span>
</span></span><span class=line><span class=cl>    <span class=n>wg</span><span class=o>=</span><span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=s1>&#39;AWS_ATHENA_WG&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl> <span class=p>,</span> <span class=n>include_tables</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;xtdpl1_ingressi_detailed&#39;</span><span class=p>,</span> <span class=s1>&#39;xtdpl1_ingressi_aggregated&#39;</span><span class=p>,</span> <span class=s1>&#39;xtdpl1_varchi&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl> <span class=p>,</span> <span class=n>sample_rows_in_table_info</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># toolkit definition through Custom Class</span>
</span></span><span class=line><span class=cl><span class=n>toolkit</span> <span class=o>=</span> <span class=n>ExtendedSqlDatabaseToolkit</span><span class=p>(</span><span class=n>db</span><span class=o>=</span><span class=n>db</span><span class=p>,</span> <span class=n>llm</span><span class=o>=</span><span class=n>OpenAI</span><span class=p>(</span><span class=n>temperature</span><span class=o>=</span><span class=mi>0</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Agent initialization</span>
</span></span><span class=line><span class=cl><span class=n>agent_executor</span> <span class=o>=</span> <span class=n>create_sql_agent</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>llm</span><span class=o>=</span><span class=n>OpenAI</span><span class=p>(</span><span class=n>temperature</span><span class=o>=</span><span class=mi>0</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>toolkit</span><span class=o>=</span><span class=n>toolkit</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>verbose</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>agent_type</span><span class=o>=</span><span class=n>AgentType</span><span class=o>.</span><span class=n>ZERO_SHOT_REACT_DESCRIPTION</span>
</span></span><span class=line><span class=cl><span class=p>)</span></span></span></code></pre></div></div><p>LangChain makes use of SQLAlchemy so it already allows accessing to a large number of DBMSs without the need of reinventing the wheel.</p><p>Note that in addition to the AWS-related environment variables explicitly referenced above, you also need to set the following variables:</p><ul><li>OPENAI_API_KEY: associated with the OpenAI account, which is mandatory to invoke their LLM APIs</li><li>SERPAPI_API_KEY: associated with the SerpApi account, in order to programmatically search on Google. There is a FREE version with a 100 monthly calls limit</li></ul><p>The options at lines 29 and 30 has been provided to limit the agent&rsquo;s range of action and prevent it from making too extensive reasoning on the whole catalog and dataset. Without these options, it&rsquo;s pretty easy to overcome the tokens limit in the OpenAI API call.</p><p>The toolkit instantiated at line 34 is my custom class, extending the standard SQLToolkit made available by LangChain. Being a few lines of code, I&rsquo;m also adding this:</p><div class="code-block open" style="counter-reset:code-block 0"><div class="code-header language-python"><span class=code-title><i class="arrow fas fa-angle-right fa-fw" aria-hidden=true></i></span>
<span class=ellipses><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i></span>
<span class=copy title="Copy to clipboard"><i class="far fa-copy fa-fw" aria-hidden=true></i></span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;Enhanced Toolkit for interacting with SQL databases and search over the internet&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>typing</span> <span class=kn>import</span> <span class=n>List</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>langchain.agents.agent_toolkits</span> <span class=kn>import</span> <span class=n>SQLDatabaseToolkit</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>langchain.tools</span> <span class=kn>import</span> <span class=n>BaseTool</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>langchain.agents</span> <span class=kn>import</span> <span class=n>load_tools</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>ExtendedSqlDatabaseToolkit</span><span class=p>(</span><span class=n>SQLDatabaseToolkit</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Enhanced Toolkit for interacting with SQL databases and search over the internet&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>get_tools</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>List</span><span class=p>[</span><span class=n>BaseTool</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>sqlTools</span> <span class=o>=</span> <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=n>get_tools</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>additionalTools</span> <span class=o>=</span> <span class=n>load_tools</span><span class=p>([</span><span class=s2>&#34;serpapi&#34;</span><span class=p>],</span> <span class=n>llm</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>llm</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>additionalTools</span><span class=o>+</span><span class=n>sqlTools</span></span></span></code></pre></div></div><p>Please note that, in addition to the explicitly referenced libraries, you also need to install the &ldquo;openai&rdquo; and &ldquo;pyathena&rdquo; libraries.</p><h2 id=the-challenges>The challenges</h2><p>I&rsquo;ve asked the agent several questions, trying to stress-test different components (eg: identifying the semantics of the data, understand what to search over the internet, when/if it&rsquo;s better switching from the aggregated table to the detailed one, etc etc).</p><p>Here I&rsquo;m just going to describe a couple of examples, but I will make some general considerations first.</p><p>The default language model from the OpenAI libraries is <em>Text-davinci-003</em>. This model is much larger and more expensive (about 10 times more!) than the one used by ChatGPT (<em>GPT-3.5-Turbo</em>). There are a lot of articles and papers describing the effectiveness of both in different use cases.
Despite being smaller (6 vs 175 billion parameters), the latter one can have the same or in some cases even better performances.</p><p>I&rsquo;ve almost exclusively used the first of the 2 and the few tests I did with GPT-3.5-Turbo had much worse results.
I didn&rsquo;t spend any time for trying to understand the reason for such performance gap. Maybe I will dedicate another post to this topic.</p><h3 id=use-case-a---simple-kpi-evaluation>Use case A - Simple KPI Evaluation</h3><blockquote><p><em>look for the coordinates and the descriptive name of the Gate (&lsquo;varco&rsquo;) with the hightest sum of diesel vehicle transits during the whole month of August 2020. Only consider the real AreaC transits and exclude the service vehicles</em></p></blockquote><p>The returned output is represented in the following picture. If you take a look at the lines starting with &ldquo;Action&rdquo;, &ldquo;Observation&rdquo; and &ldquo;Thought&rdquo;, you will see that we&rsquo;ve got what is expected according to the &ldquo;<em>Zero-shot ReAct</em>&rdquo; model.</p><figure><img src=/images/20230813/screenshot6_result_1_en.png><figcaption><h4>Figure 3 - Use case A output</h4></figcaption></figure><p>The agent starts with the identification of the <strong>Action</strong> (sql_db_list_tables) and of the input (no input in this case), obtaining (<strong>Observation</strong>) the 3 tables on which we programmatically restricted the its visibility. In theory, the tool could explore the entire catalog but, as mentioned above, I wanted to narrow the scope to avoid exceeding the OpenAI tokens threshold.</p><p>Now the agent gives control to the LLM (<strong>Thought</strong>) to identify the next action by which it states that the only 2 interesting tables are the aggregate fact table and the gate decoding table.</p><p>Please note that it&rsquo;s assuming to query the aggregated table over the detail one, but I am a little surprised that this deduction has been made <strong>solely on the table naming</strong>, since metadata and data fetching will be made later. From this point of view, the final result might not be the correct one if the 2 tables had a different data perimeter (for example if the aggregated table only contained the last year).</p><p>After fetching the metedata and extracting some sample data rows, the LLM builds the query. In this specific case you&rsquo;ll see that the model guesses the syntax of the query on the first attempt, but I have experienced several cases in which it tries, correcting the syntax each time until it reaches the definitive query.</p><p>The rest is self-described in the image.</p><p>A couple of comments:</p><ul><li>the model was able to perfectly implement the filters I had in mind in the prompt, through naming and/or data inference</li><li>I&rsquo;ve made other few attempts by removing the aggregate table and leaving only the detail one and I got the same result. However, it should be noted that the detailed table has the KPI represented in a ROW instead of a COLUMN, so in that case the model understood that the filter &ldquo;<em>des_tipo_alimentazione = &lsquo;diesel&rsquo;</em>&rdquo; was to be applied</li><li>as expected, no google search was done, because it was obviously not needed</li></ul><h3 id=use-case-b---further-info-requested>Use case B - further info requested</h3><blockquote><p><em>look for the coordinates and the descriptive name of the Gate (&lsquo;varco&rsquo;) with the hightest sum of diesel vehicle transits during the whole month of August 2020, including only areac entrances and excluding service vehicles. Also give me back the 3 gates with the smallest distance from it</em></p></blockquote><p>Here, the LLM surprised me: I&rsquo;ve added the final sentence to force the Agent doing a search over the internet, but I forgot that the distance could be also evaluated with mathematical operations using just geographical coordinates, therefore the tool (namely the LLM model behind it) performed the whole task within the DB as shown in Figure 8.</p><p>I&rsquo;ve removed the first part of the output as this is identical to use case A.</p><figure><img src=/images/20230813/screenshot7_result_6_en.png><figcaption><h4>Figure 4 - Use case B output</h4></figcaption></figure><h3 id=use-case-c---combining-querysearch>Use case C - combining query+search</h3><p>The extreme simplicity of the data model didn&rsquo;t help me so much in creating a meaningful request, so I had to do some prompt engineering in order to force a web search. Finally I managed to get something relevant with a prompt like this:</p><blockquote><p>look for the coordinates and the descriptive name of the Gate (&lsquo;varco&rsquo;) with the hightest sum of diesel vehicle transits during the whole month of August 2020, including only areac entrances and excluding service vehicles. Also give me back the bus stop closest to this gate</p></blockquote><figure><img src=/images/20230813/screenshot8_result_7_en.png><figcaption><h4>Figure 4 - Use case C output</h4></figcaption></figure><p>Here I&rsquo;ve experienced some differences in the Agent behaviour between the Italian and English input prompt, but in general, it&rsquo;s doing the expected job.</p><h2 id=conclusions>Conclusions</h2><p>As I already wrote in the previous article, the learning curve to adopt LangChain is quite shallow. A few lines of code are enough to obtain a &ldquo;wow&rdquo; effect and allow anyone to implement their own custom solution, also integrated with the rest of the enterprise ecosystem (repositories, Data APIs, mail servers, shared file systems, &mldr;) andß with their own LLM (for example, you can integrate your own installation of Llama 2 on-premise) where you don&rsquo;t want to share data outside the Organization.</p><p>On the other hand, the examples I have given above are to be considered as simplified tutorials to familiarize yourself with the framework.</p><p>To get real solutions, a more structured approach is needed, which better exploits the characteristics of the framework and takes into account the detailed capabilities of the models.</p><p>For example, I&rsquo;ve realized that it was not a wise choice to combine SQL and SerpApi search functionality in a single toolkit and that it would have been better to integrate those 2 capabilities through separate agents/chains.</p><p>As another example, I&rsquo;ve noticed that in the &ldquo;experimental&rdquo; package there is a class called &ldquo;<em>SQLDatabaseChain</em>&rdquo; which allows you to develop a Tool Sql from scratch, with a few lines of code. This way, you can completely avoid the usage of the standard toolkit and choose a more tailored solution:</p><div class="code-block open" style="counter-reset:code-block 0"><div class="code-header language-python"><span class=code-title><i class="arrow fas fa-angle-right fa-fw" aria-hidden=true></i></span>
<span class=ellipses><i class="fas fa-ellipsis-h fa-fw" aria-hidden=true></i></span>
<span class=copy title="Copy to clipboard"><i class="far fa-copy fa-fw" aria-hidden=true></i></span></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>sql_chain</span> <span class=o>=</span> <span class=n>SQLDatabaseChain</span><span class=o>.</span><span class=n>from_llm</span><span class=p>(</span><span class=n>llm</span><span class=o>=</span><span class=n>llm</span><span class=p>,</span> <span class=n>db</span><span class=o>=</span><span class=n>db</span><span class=p>,</span> <span class=n>verbose</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>sql_tool</span> <span class=o>=</span> <span class=n>Tool</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>name</span><span class=o>=</span><span class=s1>&#39;Areac DB&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>func</span><span class=o>=</span><span class=n>sql_chain</span><span class=o>.</span><span class=n>run</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>description</span><span class=o>=</span><span class=s2>&#34;This database contains the data related to transits for all gates of the LTZ of Milan, which is called </span><span class=se>\&#34;</span><span class=s2>AreaC</span><span class=se>\&#34;</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34; The most important tables are: xtdpl1_ingressi_aggregated and xtdpl1_varchi.&#34;</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34; The table xtdpl1_ingressi_aggregated contains most important measures, like the number of all transits for each of the gates and for each day of the year.&#34;</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34; The field identifying the Time dimension is &#39;dat_year_month&#39; and it&#39;s NUMERIC with a standard YYYYMM format.&#34;</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34; The field &#39;flg_areac&#39; is BOOLEAN (true/false) and it&#39;s used to identify the actual </span><span class=se>\&#34;</span><span class=s2>AreaC</span><span class=se>\&#34;</span><span class=s2> payed transits.&#34;</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34; The xtdpl1_varchi table contains the gates transcoding. The primary key is the &#39;id&#39; field, identifying the specific gate. The other fields are descriptive attributes.&#34;</span>
</span></span><span class=line><span class=cl><span class=p>)</span></span></span></code></pre></div></div><p>Since the agent uses the LLM to decide <strong>which</strong> tool to use and <strong>how</strong> to use it <strong>solely based on the tool description</strong>, this approach has the great advantage of improving performance just by adding an effective description of the DB within the tool, <strong>without modifying the LLM model at all</strong>.
In my case, for example, I&rsquo;ve incrementally added a large number of details, experiencing every time a concrete improvement in the tool performance.</p></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>Updated on August 13, 2023</span></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span><a href=javascript:void(0); title="Share on X" data-sharer=x data-url=https://c-daniele.github.io/en/posts/2023-08-13-langchain-agents/ data-title="Langchain pt. 2 - Data Analysis through Agents" data-hashtags=GenAI,Langchain,Agents><i class="fab fa-x-twitter fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Threads" data-sharer=threads data-url=https://c-daniele.github.io/en/posts/2023-08-13-langchain-agents/ data-title="Langchain pt. 2 - Data Analysis through Agents"><i class="fab fa-threads fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Facebook" data-sharer=facebook data-url=https://c-daniele.github.io/en/posts/2023-08-13-langchain-agents/ data-hashtag=GenAI><i class="fab fa-facebook-square fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Hacker News" data-sharer=hackernews data-url=https://c-daniele.github.io/en/posts/2023-08-13-langchain-agents/ data-title="Langchain pt. 2 - Data Analysis through Agents"><i class="fab fa-hacker-news fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Line" data-sharer=line data-url=https://c-daniele.github.io/en/posts/2023-08-13-langchain-agents/ data-title="Langchain pt. 2 - Data Analysis through Agents"><i data-svg-src=https://cdn.jsdelivr.net/npm/simple-icons@15.14.0/icons/line.svg aria-hidden=true></i></a><a href=javascript:void(0); title="Share on 微博" data-sharer=weibo data-url=https://c-daniele.github.io/en/posts/2023-08-13-langchain-agents/ data-title="Langchain pt. 2 - Data Analysis through Agents"><i class="fab fa-weibo fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Diaspora" data-sharer=diaspora data-url=https://c-daniele.github.io/en/posts/2023-08-13-langchain-agents/ data-title="Langchain pt. 2 - Data Analysis through Agents" data-description><i class="fab fa-diaspora fa-fw" aria-hidden=true></i></a><a href="https://t.me/share/url?url=https%3a%2f%2fc-daniele.github.io%2fen%2fposts%2f2023-08-13-langchain-agents%2f&amp;text=Langchain%20pt.%202%20-%20Data%20Analysis%20through%20Agents" target=_blank title="Share on Telegram"><i class="fab fa-telegram fa-fw" aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fas fa-tags fa-fw" aria-hidden=true></i>&nbsp;<a href=/en/tags/genai/>GenAI</a>,&nbsp;<a href=/en/tags/langchain/>Langchain</a>,&nbsp;<a href=/en/tags/agents/>Agents</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>Back</a></span>&nbsp;|&nbsp;<span><a href=/en/>Home</a></span></section></div><div class=post-nav><a href=/en/posts/2023-07-24-langchain-helloworld-pdf/ class=prev rel=prev title="LLM - Experimenting LangChain - Part 1"><i class="fas fa-angle-left fa-fw" aria-hidden=true></i>LLM - Experimenting LangChain - Part 1</a>
<a href=/en/posts/2024-04-20-langchain-api/ class=next rel=next title="Langchain pt. 3 - How to call Rest API in natural language">Langchain pt. 3 - How to call Rest API in natural language<i class="fas fa-angle-right fa-fw" aria-hidden=true></i></a></div></div></article></div></main></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title="Back to Top"><i class="fas fa-arrow-up fa-fw" aria-hidden=true></i></a></div><div id=fixed-buttons-hidden><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment fa-fw" aria-hidden=true></i></a></div><script src=https://cdn.jsdelivr.net/npm/autocomplete.js@0.38.1/dist/autocomplete.min.js></script><script src=https://cdn.jsdelivr.net/npm/lunr@2.3.9/lunr.min.js></script><script src=https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js></script><script src=https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js></script><script src=https://cdn.jsdelivr.net/npm/sharer.js@0.5.2/sharer.min.js></script><script>window.config={comment:{},search:{highlightTag:"em",lunrIndexURL:"/en/index.json",maxResultLength:10,noResultsFound:"No results found",snippetLength:30,type:"lunr"}}</script><script src=/js/theme.min.js></script><script>var dnt,doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-8NZQZ3Z1RN")}</script><script src="https://www.googletagmanager.com/gtag/js?id=G-8NZQZ3Z1RN" async></script></body></html>