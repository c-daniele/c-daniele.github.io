<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Langchain pt. 2 - Data Analysis through Agents | Cdani's Blog</title><meta name=keywords content="ai,langchain,agents"><meta name=description content="Intro In the previous article I gave a very brief overview of LangChain, describing its main concepts with some examples with unstructured data in pdf format.
Following the same approach, in this article we will give a brief introduction to Agents and proceed by trying to answer an ambitious question:
leveraging these new AI tools, can we carry out data analysis on our DB without any knowledge of SQL nor of the data model, simply starting from a text prompt in natural language?"><meta name=author content="Me"><link rel=canonical href=https://c-daniele.github.io/en/posts/2023-08-13-langchain-agents/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.3598bbf45621a4ad34d093926efeb15d6df27175e085d2f069483f14ad39d7fa.css integrity="sha256-NZi79FYhpK000JOSbv6xXW3ycXXghdLwaUg/FK051/o=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://c-daniele.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://c-daniele.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://c-daniele.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://c-daniele.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://c-daniele.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=it href=https://c-daniele.github.io/it/posts/2023-08-13-langchain-agents/><link rel=alternate hreflang=en href=https://c-daniele.github.io/en/posts/2023-08-13-langchain-agents/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-8NZQZ3Z1RN"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-8NZQZ3Z1RN",{anonymize_ip:!1})}</script><meta property="og:title" content="Langchain pt. 2 - Data Analysis through Agents"><meta property="og:description" content="Intro In the previous article I gave a very brief overview of LangChain, describing its main concepts with some examples with unstructured data in pdf format.
Following the same approach, in this article we will give a brief introduction to Agents and proceed by trying to answer an ambitious question:
leveraging these new AI tools, can we carry out data analysis on our DB without any knowledge of SQL nor of the data model, simply starting from a text prompt in natural language?"><meta property="og:type" content="article"><meta property="og:url" content="https://c-daniele.github.io/en/posts/2023-08-13-langchain-agents/"><meta property="og:image" content="https://c-daniele.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-08-13T19:00:00+02:00"><meta property="article:modified_time" content="2023-08-13T19:00:00+02:00"><meta property="og:site_name" content="Cdani's Blog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://c-daniele.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Langchain pt. 2 - Data Analysis through Agents"><meta name=twitter:description content="Intro In the previous article I gave a very brief overview of LangChain, describing its main concepts with some examples with unstructured data in pdf format.
Following the same approach, in this article we will give a brief introduction to Agents and proceed by trying to answer an ambitious question:
leveraging these new AI tools, can we carry out data analysis on our DB without any knowledge of SQL nor of the data model, simply starting from a text prompt in natural language?"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":4,"name":"Langchain pt. 2 - Data Analysis through Agents","item":"https://c-daniele.github.io/en/posts/2023-08-13-langchain-agents/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Langchain pt. 2 - Data Analysis through Agents","name":"Langchain pt. 2 - Data Analysis through Agents","description":"Intro In the previous article I gave a very brief overview of LangChain, describing its main concepts with some examples with unstructured data in pdf format.\nFollowing the same approach, in this article we will give a brief introduction to Agents and proceed by trying to answer an ambitious question:\nleveraging these new AI tools, can we carry out data analysis on our DB without any knowledge of SQL nor of the data model, simply starting from a text prompt in natural language?","keywords":["ai","langchain","agents"],"articleBody":"Intro In the previous article I gave a very brief overview of LangChain, describing its main concepts with some examples with unstructured data in pdf format.\nFollowing the same approach, in this article we will give a brief introduction to Agents and proceed by trying to answer an ambitious question:\nleveraging these new AI tools, can we carry out data analysis on our DB without any knowledge of SQL nor of the data model, simply starting from a text prompt in natural language?\nAgents LLMs are extremely powerful, but they seem to be completely ineffective in answering questions that require detailed knowledge not tightly integrated into model training. Over the internet there are dozens of examples that manage to catch ChatGPT out through hallucinations or lack of response (eg: weather forecasts, latest news, gossip or even specific mathematical operations).\nFrameworks like LangChain can overcome these limitations by defining specific and data-aware components, but usually the actions performed by the framework are predetermined. In other words, the framework uses a Language Model to perform some actions, but they are “hardcoded” and in many cases this can make AI models completely ineffective, because you can’t drive specific actions based on user input.\nThat’s where agents come into play Agents are components that have a series of tools available to perform specific actions, such as doing a search on Wikipedia or Google, or executing Python code or even accessing the local file system.\nAgents use an LLM to understand user input and decide how to proceed accordingly:\nWhich tool among the provided set should be the most appropriate to use? What’s the input text to be passed as input to the tool? Have we reached the goal thus answering the initial question or should we repeat step 1 and 2 again? This approach was inspired from a framework called ReAct which has been defined at the end of 2022 by a joint team of researchers from Google and Princeton University. You can find here the original paper. In LangChain, there are several implementations of such approach, but the most common is called “Zero-shot ReAct” and can be described at a high level with the following workflow.\nFigure 1 - Simplified workflow for \"Zero-shot ReAct\" agents Please note that this type of agents have no memory and so discriminate their actions only on the basis of the input text and the description of the tool. It is therefore very important that the tools also include an effective description for the purpose of a correct interpretation by the LLM.\nLangChain tools are sometimes grouped into groups called “Toolkits”. In the official documentation you will find a default toolkit called “SQLDatabaseToolkit”, to configure a SQL agent.\nThe scenario As I said at the beginning of the article, we want to do a real analysis on the data present in a relational DB, assuming we have no knowledge of the data model nor SQL skills. The starting point will be a text prompt in natural language.\nFrom a technical standpoint, the task is very easy because, in addition to the toolkit, LangChain provides a utility method for defining a SqlAgent to which we only have to provide some parameters such as the DB connection, the type of LLM, etc..\nAt first sight the examples given in the official documentation look already very interesting and complete. In addition to trivial use cases (eg DESCRIBE a table), it is shown how the agent is able to make inferences on metadata to understand how to aggregate data or JOIN 2 or more tables.\nIn order to not repeat the same example taken from the documentation and introduce some more complications, I’ve decided to create an enhanced version of the standard toolkit, which is also able to do searches over the internet.\nThe dataset The official documentation includes examples that make use of a test DB based on SqlLite and called “Chinook”, which simulates a media store and which you can also download from the official SqlLite site.\nTaking a look at the data model and the data itself, I was suspicious of the exciting results they reported, because the DB is in my opinion not representative of a real case, because:\nthe names of the tables and columns are all defined in English and self-describing, moreover no naming convention has been used the DB seems practically in 3NF and this is pretty unlikely in scenarios where you want to do pure data analysis local SqlLite .db file? This is a case very far from reality! From past personal projects, I have made available an Athena DB on my AWS account with some data structures that in my opinion are more representative of real use-cases. The data is related to OpenData of the Municipality of Milan, relating to transits within the AreaC gates. AreaC is the the main LTZ (Limited Traffic Zone) for the city of Milan. Actually Athena is not a real DB, but rather a SQL-Engine based on Presto, but with the appropriate configurations, AWS provides an endpoint that allows you to access it as if it were a real DBMS.\nThe Data Model is very simple: it’s made of 2 fact tables, containing the AreaC crossing events (detail + aggregate), both linked to a decoding table of the entrances, in which some attributes are indicated, including the exact geographical position of the passage. In all 3 cases, these are Iceberg tables stored on S3 and mapped to Athena via the Glue catalog.\nThe original datasets have been taken from the official OpenData portal. This is about 4 years of data (about 101 million records in the biggest fact table).\nPlease find below the DDLs of the tables with some comments that I have added here for simplicity (and which therefore the agent did not have available…).\nFigure 2 - Detailed fact table DDL Figure 3 - Aggregated fact table DDL Figure 2 - Gate decoding table DDL In the aggregate table, in addition to removing some attributes, I’ve made a sort of pivot on the type of power supply, calculating the different transits in COLUMN instead of ROW, reducing the cardinality by about 92%. Other than that, the 2 fact tables are pretty much identical.\nThe Gate decoding table contains the descriptive name and the geographical coordinates.\nAs you can see, I’ve used a naming convention, but this is deliberately imperfect, for example it is a mix of English and Italian.\nThe software setup Please find below the basic imports and configurations of the main python code:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 from langchain.agents import create_sql_agent from langchain.sql_database import SQLDatabase from langchain.llms.openai import OpenAI from langchain.agents.agent_types import AgentType import os from urllib.parse import quote_plus from ExtendedSqlDatabaseToolkit import * # set the environment variables from dotenv import load_dotenv load_dotenv() # connection string conn_str = ( \"awsathena+rest://{aws_access_key_id}:{aws_secret_access_key}@\" \"athena.{region_name}.amazonaws.com:443/\" \"{schema_name}?s3_staging_dir={s3_staging_dir}\u0026work_group={wg}\" ) # database initialization db = SQLDatabase.from_uri(conn_str.format( aws_access_key_id=quote_plus(os.environ['AWS_AK']), aws_secret_access_key=quote_plus(os.environ['AWS_SAK']), region_name=os.environ['AWS_REGION'], schema_name=os.environ['AWS_ATHENA_SCHEMA'], s3_staging_dir=quote_plus(os.environ['AWS_S3_OUT']), wg=os.environ['AWS_ATHENA_WG'] ) , include_tables=['xtdpl1_ingressi_detailed', 'xtdpl1_ingressi_aggregated', 'xtdpl1_varchi'] , sample_rows_in_table_info=2) # toolkit definition through Custom Class toolkit = ExtendedSqlDatabaseToolkit(db=db, llm=OpenAI(temperature=0)) # Agent initialization agent_executor = create_sql_agent( llm=OpenAI(temperature=0), toolkit=toolkit, verbose=True, agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION ) LangChain makes use of SQLAlchemy so it already allows accessing to a large number of DBMSs without the need of reinventing the wheel.\nNote that in addition to the AWS-related environment variables explicitly referenced above, you also need to set the following variables:\nOPENAI_API_KEY: associated with the OpenAI account, which is mandatory to invoke their LLM APIs SERPAPI_API_KEY: associated with the SerpApi account, in order to programmatically search on Google. There is a FREE version with a 100 monthly calls limit The options at lines 29 and 30 has been provided to limit the agent’s range of action and prevent it from making too extensive reasoning on the whole catalog and dataset. Without these options, it’s pretty easy to overcome the tokens limit in the OpenAI API call.\nThe toolkit instantiated at line 34 is my custom class, extending the standard SQLToolkit made available by LangChain. Being a few lines of code, I’m also adding this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 \"\"\"Enhanced Toolkit for interacting with SQL databases and search over the internet\"\"\" from typing import List from langchain.agents.agent_toolkits import SQLDatabaseToolkit from langchain.tools import BaseTool from langchain.agents import load_tools class ExtendedSqlDatabaseToolkit(SQLDatabaseToolkit): \"\"\"Enhanced Toolkit for interacting with SQL databases and search over the internet\"\"\" def get_tools(self) -\u003e List[BaseTool]: sqlTools = super().get_tools() additionalTools = load_tools([\"serpapi\"], llm=self.llm) return additionalTools+sqlTools Please note that, in addition to the explicitly referenced libraries, you also need to install the “openai” and “pyathena” libraries.\nThe challenges I’ve asked the agent several questions, trying to stress-test different components (eg: identifying the semantics of the data, understand what to search over the internet, when/if it’s better switching from the aggregated table to the detailed one, etc etc).\nHere I’m just going to describe a couple of examples, but I will make some general considerations first.\nThe default language model from the OpenAI libraries is Text-davinci-003. This model is much larger and more expensive (about 10 times more!) than the one used by ChatGPT (GPT-3.5-Turbo). There are a lot of articles and papers describing the effectiveness of both in different use cases. Despite being smaller (6 vs 175 billion parameters), the latter one can have the same or in some cases even better performances.\nI’ve almost exclusively used the first of the 2 and the few tests I did with GPT-3.5-Turbo had much worse results. I didn’t spend any time for trying to understand the reason for such performance gap. Maybe I will dedicate another post to this topic.\nUse case A - Simple KPI Evaluation look for the coordinates and the descriptive name of the Gate (‘varco’) with the hightest sum of diesel vehicle transits during the whole month of August 2020. Only consider the real AreaC transits and exclude the service vehicles\nThe returned output is represented in the following picture. If you take a look at the lines starting with “Action”, “Observation” and “Thought”, you will see that we’ve got what is expected according to the “Zero-shot ReAct” model.\nFigure 3 - Use case A output The agent starts with the identification of the Action (sql_db_list_tables) and of the input (no input in this case), obtaining (Observation) the 3 tables on which we programmatically restricted the its visibility. In theory, the tool could explore the entire catalog but, as mentioned above, I wanted to narrow the scope to avoid exceeding the OpenAI tokens threshold.\nNow the agent gives control to the LLM (Thought) to identify the next action by which it states that the only 2 interesting tables are the aggregate fact table and the gate decoding table.\nPlease note that it’s assuming to query the aggregated table over the detail one, but I am a little surprised that this deduction has been made solely on the table naming, since metadata and data fetching will be made later. From this point of view, the final result might not be the correct one if the 2 tables had a different data perimeter (for example if the aggregated table only contained the last year).\nAfter fetching the metedata and extracting some sample data rows, the LLM builds the query. In this specific case you’ll see that the model guesses the syntax of the query on the first attempt, but I have experienced several cases in which it tries, correcting the syntax each time until it reaches the definitive query.\nThe rest is self-described in the image.\nA couple of comments:\nthe model was able to perfectly implement the filters I had in mind in the prompt, through naming and/or data inference I’ve made other few attempts by removing the aggregate table and leaving only the detail one and I got the same result. However, it should be noted that the detailed table has the KPI represented in a ROW instead of a COLUMN, so in that case the model understood that the filter “des_tipo_alimentazione = ‘diesel’” was to be applied as expected, no google search was done, because it was obviously not needed Use case B - further info requested look for the coordinates and the descriptive name of the Gate (‘varco’) with the hightest sum of diesel vehicle transits during the whole month of August 2020, including only areac entrances and excluding service vehicles. Also give me back the 3 gates with the smallest distance from it\nHere, the LLM surprised me: I’ve added the final sentence to force the Agent doing a search over the internet, but I forgot that the distance could be also evaluated with mathematical operations using just geographical coordinates, therefore the tool (namely the LLM model behind it) performed the whole task within the DB as shown in Figure 8.\nI’ve removed the first part of the output as this is identical to use case A.\nFigure 4 - Use case B output Use case C - combining query+search The extreme simplicity of the data model didn’t help me so much in creating a meaningful request, so I had to do some prompt engineering in order to force a web search. Finally I managed to get something relevant with a prompt like this:\nlook for the coordinates and the descriptive name of the Gate (‘varco’) with the hightest sum of diesel vehicle transits during the whole month of August 2020, including only areac entrances and excluding service vehicles. Also give me back the bus stop closest to this gate\nFigure 4 - Use case C output Here I’ve experienced some differences in the Agent behaviour between the Italian and English input prompt, but in general, it’s doing the expected job.\nConclusions As I already wrote in the previous article, the learning curve to adopt LangChain is quite shallow. A few lines of code are enough to obtain a “wow” effect and allow anyone to implement their own custom solution, also integrated with the rest of the enterprise ecosystem (repositories, Data APIs, mail servers, shared file systems, …) andß with their own LLM (for example, you can integrate your own installation of Llama 2 on-premise) where you don’t want to share data outside the Organization.\nOn the other hand, the examples I have given above are to be considered as simplified tutorials to familiarize yourself with the framework.\nTo get real solutions, a more structured approach is needed, which better exploits the characteristics of the framework and takes into account the detailed capabilities of the models.\nFor example, I’ve realized that it was not a wise choice to combine SQL and SerpApi search functionality in a single toolkit and that it would have been better to integrate those 2 capabilities through separate agents/chains.\nAs another example, I’ve noticed that in the “experimental” package there is a class called “SQLDatabaseChain” which allows you to develop a Tool Sql from scratch, with a few lines of code. This way, you can completely avoid the usage of the standard toolkit and choose a more tailored solution:\n1 2 3 4 5 6 7 8 9 10 11 12 sql_chain = SQLDatabaseChain.from_llm(llm=llm, db=db, verbose=True) sql_tool = Tool( name='Areac DB', func=sql_chain.run, description=\"This database contains the data related to transits for all gates of the LTZ of Milan, which is called \\\"AreaC\\\"\" \" The most important tables are: xtdpl1_ingressi_aggregated and xtdpl1_varchi.\" \" The table xtdpl1_ingressi_aggregated contains most important measures, like the number of all transits for each of the gates and for each day of the year.\" \" The field identifying the Time dimension is 'dat_year_month' and it's NUMERIC with a standard YYYYMM format.\" \" The field 'flg_areac' is BOOLEAN (true/false) and it's used to identify the actual \\\"AreaC\\\" payed transits.\" \" The xtdpl1_varchi table contains the gates transcoding. The primary key is the 'id' field, identifying the specific gate. The other fields are descriptive attributes.\" ) Since the agent uses the LLM to decide which tool to use and how to use it solely based on the tool description, this approach has the great advantage of improving performance just by adding an effective description of the DB within the tool, without modifying the LLM model at all. In my case, for example, I’ve incrementally added a large number of details, experiencing every time a concrete improvement in the tool performance.\n","wordCount":"2749","inLanguage":"en","datePublished":"2023-08-13T19:00:00+02:00","dateModified":"2023-08-13T19:00:00+02:00","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://c-daniele.github.io/en/posts/2023-08-13-langchain-agents/"},"publisher":{"@type":"Organization","name":"Cdani's Blog","logo":{"@type":"ImageObject","url":"https://c-daniele.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://c-daniele.github.io/en/ accesskey=h title="Home (Alt + H)"><img src=https://c-daniele.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://c-daniele.github.io/it/ title=Italiano aria-label=Italiano>It</a></li></ul></div></div><ul id=menu><li><a href=https://c-daniele.github.io/en/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://c-daniele.github.io/en/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://c-daniele.github.io/en/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://c-daniele.github.io/en/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://c-daniele.github.io/en/>Home</a></div><h1 class=post-title>Langchain pt. 2 - Data Analysis through Agents</h1><div class=post-meta><span title='2023-08-13 19:00:00 +0200 +0200'>August 13, 2023</span>&nbsp;·&nbsp;13 min&nbsp;·&nbsp;2749 words&nbsp;·&nbsp;Me&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://c-daniele.github.io/it/posts/2023-08-13-langchain-agents/>It</a></li></ul></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#intro>Intro</a></li><li><a href=#agents>Agents</a></li><li><a href=#thats-where-agents-come-into-play>That&rsquo;s where agents come into play</a></li><li><a href=#the-scenario>The scenario</a><ul><li><a href=#the-dataset>The dataset</a></li><li><a href=#the-software-setup>The software setup</a></li></ul></li><li><a href=#the-challenges>The challenges</a><ul><li><a href=#use-case-a---simple-kpi-evaluation>Use case A - Simple KPI Evaluation</a></li><li><a href=#use-case-b---further-info-requested>Use case B - further info requested</a></li><li><a href=#use-case-c---combining-querysearch>Use case C - combining query+search</a></li></ul></li><li><a href=#conclusions>Conclusions</a></li></ul></nav></div></details></div><div class=post-content><h2 id=intro>Intro<a hidden class=anchor aria-hidden=true href=#intro>#</a></h2><p>In the previous article I gave a very brief overview of LangChain, describing its main concepts with some examples with unstructured data in pdf format.</p><p>Following the same approach, in this article we will give a brief introduction to Agents and proceed by trying to answer an ambitious question:</p><blockquote><p>leveraging these new AI tools, can we carry out data analysis on our DB without any knowledge of SQL nor of the data model, simply starting from a text prompt in natural language?</p></blockquote><h2 id=agents>Agents<a hidden class=anchor aria-hidden=true href=#agents>#</a></h2><p>LLMs are extremely powerful, but they seem to be completely ineffective in answering questions that require detailed knowledge not tightly integrated into model training. Over the internet there are dozens of examples that manage to catch ChatGPT out through hallucinations or lack of response (eg: weather forecasts, latest news, gossip or even specific mathematical operations).</p><p>Frameworks like LangChain can overcome these limitations by defining specific and data-aware components, but usually the actions performed by the framework are predetermined. In other words, the framework uses a Language Model to perform some actions, <strong>but they are &ldquo;hardcoded&rdquo;</strong> and in many cases this can make AI models completely ineffective, because you can&rsquo;t drive specific actions based on user input.</p><h2 id=thats-where-agents-come-into-play>That&rsquo;s where agents come into play<a hidden class=anchor aria-hidden=true href=#thats-where-agents-come-into-play>#</a></h2><p>Agents are components that have a series of tools available to perform specific actions, such as doing a search on Wikipedia or Google, or executing Python code or even accessing the local file system.</p><p>Agents use an LLM to understand user input and decide how to proceed accordingly:</p><ol><li>Which tool among the provided set should be the most appropriate to use?</li><li>What&rsquo;s the input text to be passed as input to the tool?</li><li>Have we reached the goal thus answering the initial question or should we repeat step 1 and 2 again?</li></ol><p>This approach was inspired from a framework called <em>ReAct</em> which has been defined at the end of 2022 by a joint team of researchers from Google and Princeton University. You can find <a href=https://arxiv.org/abs/2210.03629>here</a> the original paper. In LangChain, there are several implementations of such approach, but the most common is called &ldquo;<em>Zero-shot ReAct</em>&rdquo; and can be described at a high level with the following workflow.</p><figure><img loading=lazy src=/images/20230813/Agent_Diagram_v2.png><figcaption>Figure 1 - Simplified workflow for "Zero-shot ReAct" agents</figcaption></figure><p>Please note that this type of agents have no memory and so discriminate their actions <strong>only on the basis of the input text and the description of the tool</strong>. It is therefore very important that the tools also include an effective description for the purpose of a correct interpretation by the LLM.</p><p>LangChain tools are sometimes grouped into groups called &ldquo;Toolkits&rdquo;. In the official documentation you will find a default toolkit called &ldquo;SQLDatabaseToolkit&rdquo;, to configure a SQL agent.</p><h2 id=the-scenario>The scenario<a hidden class=anchor aria-hidden=true href=#the-scenario>#</a></h2><p>As I said at the beginning of the article, we want to do a real analysis on the data present in a relational DB, assuming we have no knowledge of the data model nor SQL skills. The starting point will be a text prompt in natural language.</p><p>From a technical standpoint, the task is very easy because, in addition to the toolkit, LangChain provides a utility method for defining a SqlAgent to which we only have to provide some parameters such as the DB connection, the type of LLM, etc..</p><p>At first sight <a href=https://python.langchain.com/docs/integrations/toolkits/sql_database>the examples given in the official documentation</a> look already very interesting and complete. In addition to trivial use cases (eg <em>DESCRIBE</em> a table), it is shown how the agent is able to make inferences on metadata to understand how to aggregate data or JOIN 2 or more tables.</p><p>In order to not repeat the same example taken from the documentation and introduce some more complications, I&rsquo;ve decided to create an enhanced version of the standard toolkit, which is also able to do searches over the internet.</p><h3 id=the-dataset>The dataset<a hidden class=anchor aria-hidden=true href=#the-dataset>#</a></h3><p>The official documentation includes examples that make use of a test DB based on SqlLite and called &ldquo;Chinook&rdquo;, which simulates a media store and which you can also download from the official SqlLite site.</p><p>Taking a look at the data model and the data itself, I was suspicious of the exciting results they reported, because the DB is in my opinion not representative of a real case, because:</p><ul><li>the names of the tables and columns are all defined in English and self-describing, moreover no naming convention has been used</li><li>the DB seems practically in 3NF and this is pretty unlikely in scenarios where you want to do pure data analysis</li><li>local SqlLite .db file? This is a case very far from reality!</li></ul><p>From past personal projects, I have made available an Athena DB on my AWS account with some data structures that in my opinion are more representative of real use-cases. The data is related to OpenData of the Municipality of Milan, relating to <strong>transits within the AreaC gates</strong>. AreaC is the the main LTZ (<em>Limited Traffic Zone</em>) for the city of Milan. Actually Athena is not a real DB, but rather a SQL-Engine based on Presto, but with the appropriate configurations, AWS provides an endpoint that allows you to access it as if it were a real DBMS.</p><p>The Data Model is very simple: it&rsquo;s made of 2 fact tables, containing the AreaC crossing events (detail + aggregate), both linked to a decoding table of the entrances, in which some attributes are indicated, including the exact geographical position of the passage. In all 3 cases, these are Iceberg tables stored on S3 and mapped to Athena via the Glue catalog.</p><p>The original datasets have been taken from the <a href=https://dati.comune.milano.it/group/tran>official OpenData portal</a>. This is about 4 years of data (about 101 million records in the biggest fact table).</p><p>Please find below the DDLs of the tables with some comments that I have added here for simplicity (and which therefore the agent did not have available&mldr;).</p><figure><img loading=lazy src=/images/20230813/screenshot1.png><figcaption>Figure 2 - Detailed fact table DDL</figcaption></figure><figure><img loading=lazy src=/images/20230813/screenshot2.png><figcaption>Figure 3 - Aggregated fact table DDL</figcaption></figure><figure><img loading=lazy src=/images/20230813/screenshot3.png><figcaption>Figure 2 - Gate decoding table DDL</figcaption></figure><p>In the aggregate table, in addition to removing some attributes, I&rsquo;ve made a sort of pivot on the type of power supply, calculating the different transits in COLUMN instead of ROW, reducing the cardinality by about 92%. Other than that, the 2 fact tables are pretty much identical.</p><p>The Gate decoding table contains the descriptive name and the geographical coordinates.</p><p>As you can see, I&rsquo;ve used a naming convention, but this is <strong>deliberately imperfect</strong>, for example it is a mix of English and Italian.</p><h3 id=the-software-setup>The software setup<a hidden class=anchor aria-hidden=true href=#the-software-setup>#</a></h3><p>Please find below the basic imports and configurations of the main python code:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>langchain.agents</span> <span class=kn>import</span> <span class=n>create_sql_agent</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>langchain.sql_database</span> <span class=kn>import</span> <span class=n>SQLDatabase</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>langchain.llms.openai</span> <span class=kn>import</span> <span class=n>OpenAI</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>langchain.agents.agent_types</span> <span class=kn>import</span> <span class=n>AgentType</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>os</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>urllib.parse</span> <span class=kn>import</span> <span class=n>quote_plus</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>ExtendedSqlDatabaseToolkit</span> <span class=kn>import</span> <span class=o>*</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># set the environment variables</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>dotenv</span> <span class=kn>import</span> <span class=n>load_dotenv</span>
</span></span><span class=line><span class=cl><span class=n>load_dotenv</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># connection string</span>
</span></span><span class=line><span class=cl><span class=n>conn_str</span> <span class=o>=</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;awsathena+rest://</span><span class=si>{aws_access_key_id}</span><span class=s2>:</span><span class=si>{aws_secret_access_key}</span><span class=s2>@&#34;</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;athena.</span><span class=si>{region_name}</span><span class=s2>.amazonaws.com:443/&#34;</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;</span><span class=si>{schema_name}</span><span class=s2>?s3_staging_dir=</span><span class=si>{s3_staging_dir}</span><span class=s2>&amp;work_group=</span><span class=si>{wg}</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># database initialization</span>
</span></span><span class=line><span class=cl><span class=n>db</span> <span class=o>=</span> <span class=n>SQLDatabase</span><span class=o>.</span><span class=n>from_uri</span><span class=p>(</span><span class=n>conn_str</span><span class=o>.</span><span class=n>format</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>aws_access_key_id</span><span class=o>=</span><span class=n>quote_plus</span><span class=p>(</span><span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=s1>&#39;AWS_AK&#39;</span><span class=p>]),</span>
</span></span><span class=line><span class=cl>    <span class=n>aws_secret_access_key</span><span class=o>=</span><span class=n>quote_plus</span><span class=p>(</span><span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=s1>&#39;AWS_SAK&#39;</span><span class=p>]),</span>
</span></span><span class=line><span class=cl>    <span class=n>region_name</span><span class=o>=</span><span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=s1>&#39;AWS_REGION&#39;</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=n>schema_name</span><span class=o>=</span><span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=s1>&#39;AWS_ATHENA_SCHEMA&#39;</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=n>s3_staging_dir</span><span class=o>=</span><span class=n>quote_plus</span><span class=p>(</span><span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=s1>&#39;AWS_S3_OUT&#39;</span><span class=p>]),</span>
</span></span><span class=line><span class=cl>    <span class=n>wg</span><span class=o>=</span><span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=s1>&#39;AWS_ATHENA_WG&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl> <span class=p>,</span> <span class=n>include_tables</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;xtdpl1_ingressi_detailed&#39;</span><span class=p>,</span> <span class=s1>&#39;xtdpl1_ingressi_aggregated&#39;</span><span class=p>,</span> <span class=s1>&#39;xtdpl1_varchi&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl> <span class=p>,</span> <span class=n>sample_rows_in_table_info</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># toolkit definition through Custom Class</span>
</span></span><span class=line><span class=cl><span class=n>toolkit</span> <span class=o>=</span> <span class=n>ExtendedSqlDatabaseToolkit</span><span class=p>(</span><span class=n>db</span><span class=o>=</span><span class=n>db</span><span class=p>,</span> <span class=n>llm</span><span class=o>=</span><span class=n>OpenAI</span><span class=p>(</span><span class=n>temperature</span><span class=o>=</span><span class=mi>0</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Agent initialization</span>
</span></span><span class=line><span class=cl><span class=n>agent_executor</span> <span class=o>=</span> <span class=n>create_sql_agent</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>llm</span><span class=o>=</span><span class=n>OpenAI</span><span class=p>(</span><span class=n>temperature</span><span class=o>=</span><span class=mi>0</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>toolkit</span><span class=o>=</span><span class=n>toolkit</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>verbose</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>agent_type</span><span class=o>=</span><span class=n>AgentType</span><span class=o>.</span><span class=n>ZERO_SHOT_REACT_DESCRIPTION</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>LangChain makes use of SQLAlchemy so it already allows accessing to a large number of DBMSs without the need of reinventing the wheel.</p><p>Note that in addition to the AWS-related environment variables explicitly referenced above, you also need to set the following variables:</p><ul><li>OPENAI_API_KEY: associated with the OpenAI account, which is mandatory to invoke their LLM APIs</li><li>SERPAPI_API_KEY: associated with the SerpApi account, in order to programmatically search on Google. There is a FREE version with a 100 monthly calls limit</li></ul><p>The options at lines 29 and 30 has been provided to limit the agent&rsquo;s range of action and prevent it from making too extensive reasoning on the whole catalog and dataset. Without these options, it&rsquo;s pretty easy to overcome the tokens limit in the OpenAI API call.</p><p>The toolkit instantiated at line 34 is my custom class, extending the standard SQLToolkit made available by LangChain. Being a few lines of code, I&rsquo;m also adding this:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=s2>&#34;&#34;&#34;Enhanced Toolkit for interacting with SQL databases and search over the internet&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>typing</span> <span class=kn>import</span> <span class=n>List</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>langchain.agents.agent_toolkits</span> <span class=kn>import</span> <span class=n>SQLDatabaseToolkit</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>langchain.tools</span> <span class=kn>import</span> <span class=n>BaseTool</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>langchain.agents</span> <span class=kn>import</span> <span class=n>load_tools</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>ExtendedSqlDatabaseToolkit</span><span class=p>(</span><span class=n>SQLDatabaseToolkit</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Enhanced Toolkit for interacting with SQL databases and search over the internet&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>get_tools</span><span class=p>(</span><span class=bp>self</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>List</span><span class=p>[</span><span class=n>BaseTool</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>sqlTools</span> <span class=o>=</span> <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=n>get_tools</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>additionalTools</span> <span class=o>=</span> <span class=n>load_tools</span><span class=p>([</span><span class=s2>&#34;serpapi&#34;</span><span class=p>],</span> <span class=n>llm</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>llm</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>additionalTools</span><span class=o>+</span><span class=n>sqlTools</span>
</span></span></code></pre></td></tr></table></div></div><p>Please note that, in addition to the explicitly referenced libraries, you also need to install the &ldquo;openai&rdquo; and &ldquo;pyathena&rdquo; libraries.</p><h2 id=the-challenges>The challenges<a hidden class=anchor aria-hidden=true href=#the-challenges>#</a></h2><p>I&rsquo;ve asked the agent several questions, trying to stress-test different components (eg: identifying the semantics of the data, understand what to search over the internet, when/if it&rsquo;s better switching from the aggregated table to the detailed one, etc etc).</p><p>Here I&rsquo;m just going to describe a couple of examples, but I will make some general considerations first.</p><p>The default language model from the OpenAI libraries is <em>Text-davinci-003</em>. This model is much larger and more expensive (about 10 times more!) than the one used by ChatGPT (<em>GPT-3.5-Turbo</em>). There are a lot of articles and papers describing the effectiveness of both in different use cases.
Despite being smaller (6 vs 175 billion parameters), the latter one can have the same or in some cases even better performances.</p><p>I&rsquo;ve almost exclusively used the first of the 2 and the few tests I did with GPT-3.5-Turbo had much worse results.
I didn&rsquo;t spend any time for trying to understand the reason for such performance gap. Maybe I will dedicate another post to this topic.</p><h3 id=use-case-a---simple-kpi-evaluation>Use case A - Simple KPI Evaluation<a hidden class=anchor aria-hidden=true href=#use-case-a---simple-kpi-evaluation>#</a></h3><blockquote><p><em>look for the coordinates and the descriptive name of the Gate (&lsquo;varco&rsquo;) with the hightest sum of diesel vehicle transits during the whole month of August 2020. Only consider the real AreaC transits and exclude the service vehicles</em></p></blockquote><p>The returned output is represented in the following picture. If you take a look at the lines starting with &ldquo;Action&rdquo;, &ldquo;Observation&rdquo; and &ldquo;Thought&rdquo;, you will see that we&rsquo;ve got what is expected according to the &ldquo;<em>Zero-shot ReAct</em>&rdquo; model.</p><figure><img loading=lazy src=/images/20230813/screenshot6_result_1_en.png><figcaption>Figure 3 - Use case A output</figcaption></figure><p>The agent starts with the identification of the <strong>Action</strong> (sql_db_list_tables) and of the input (no input in this case), obtaining (<strong>Observation</strong>) the 3 tables on which we programmatically restricted the its visibility. In theory, the tool could explore the entire catalog but, as mentioned above, I wanted to narrow the scope to avoid exceeding the OpenAI tokens threshold.</p><p>Now the agent gives control to the LLM (<strong>Thought</strong>) to identify the next action by which it states that the only 2 interesting tables are the aggregate fact table and the gate decoding table.</p><p>Please note that it&rsquo;s assuming to query the aggregated table over the detail one, but I am a little surprised that this deduction has been made <strong>solely on the table naming</strong>, since metadata and data fetching will be made later. From this point of view, the final result might not be the correct one if the 2 tables had a different data perimeter (for example if the aggregated table only contained the last year).</p><p>After fetching the metedata and extracting some sample data rows, the LLM builds the query. In this specific case you&rsquo;ll see that the model guesses the syntax of the query on the first attempt, but I have experienced several cases in which it tries, correcting the syntax each time until it reaches the definitive query.</p><p>The rest is self-described in the image.</p><p>A couple of comments:</p><ul><li>the model was able to perfectly implement the filters I had in mind in the prompt, through naming and/or data inference</li><li>I&rsquo;ve made other few attempts by removing the aggregate table and leaving only the detail one and I got the same result. However, it should be noted that the detailed table has the KPI represented in a ROW instead of a COLUMN, so in that case the model understood that the filter &ldquo;<em>des_tipo_alimentazione = &lsquo;diesel&rsquo;</em>&rdquo; was to be applied</li><li>as expected, no google search was done, because it was obviously not needed</li></ul><h3 id=use-case-b---further-info-requested>Use case B - further info requested<a hidden class=anchor aria-hidden=true href=#use-case-b---further-info-requested>#</a></h3><blockquote><p><em>look for the coordinates and the descriptive name of the Gate (&lsquo;varco&rsquo;) with the hightest sum of diesel vehicle transits during the whole month of August 2020, including only areac entrances and excluding service vehicles. Also give me back the 3 gates with the smallest distance from it</em></p></blockquote><p>Here, the LLM surprised me: I&rsquo;ve added the final sentence to force the Agent doing a search over the internet, but I forgot that the distance could be also evaluated with mathematical operations using just geographical coordinates, therefore the tool (namely the LLM model behind it) performed the whole task within the DB as shown in Figure 8.</p><p>I&rsquo;ve removed the first part of the output as this is identical to use case A.</p><figure><img loading=lazy src=/images/20230813/screenshot7_result_6_en.png><figcaption>Figure 4 - Use case B output</figcaption></figure><h3 id=use-case-c---combining-querysearch>Use case C - combining query+search<a hidden class=anchor aria-hidden=true href=#use-case-c---combining-querysearch>#</a></h3><p>The extreme simplicity of the data model didn&rsquo;t help me so much in creating a meaningful request, so I had to do some prompt engineering in order to force a web search. Finally I managed to get something relevant with a prompt like this:</p><blockquote><p>look for the coordinates and the descriptive name of the Gate (&lsquo;varco&rsquo;) with the hightest sum of diesel vehicle transits during the whole month of August 2020, including only areac entrances and excluding service vehicles. Also give me back the bus stop closest to this gate</p></blockquote><figure><img loading=lazy src=/images/20230813/screenshot8_result_7_en.png><figcaption>Figure 4 - Use case C output</figcaption></figure><p>Here I&rsquo;ve experienced some differences in the Agent behaviour between the Italian and English input prompt, but in general, it&rsquo;s doing the expected job.</p><h2 id=conclusions>Conclusions<a hidden class=anchor aria-hidden=true href=#conclusions>#</a></h2><p>As I already wrote in the previous article, the learning curve to adopt LangChain is quite shallow. A few lines of code are enough to obtain a &ldquo;wow&rdquo; effect and allow anyone to implement their own custom solution, also integrated with the rest of the enterprise ecosystem (repositories, Data APIs, mail servers, shared file systems, &mldr;) andß with their own LLM (for example, you can integrate your own installation of Llama 2 on-premise) where you don&rsquo;t want to share data outside the Organization.</p><p>On the other hand, the examples I have given above are to be considered as simplified tutorials to familiarize yourself with the framework.</p><p>To get real solutions, a more structured approach is needed, which better exploits the characteristics of the framework and takes into account the detailed capabilities of the models.</p><p>For example, I&rsquo;ve realized that it was not a wise choice to combine SQL and SerpApi search functionality in a single toolkit and that it would have been better to integrate those 2 capabilities through separate agents/chains.</p><p>As another example, I&rsquo;ve noticed that in the &ldquo;experimental&rdquo; package there is a class called &ldquo;<em>SQLDatabaseChain</em>&rdquo; which allows you to develop a Tool Sql from scratch, with a few lines of code. This way, you can completely avoid the usage of the standard toolkit and choose a more tailored solution:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>sql_chain</span> <span class=o>=</span> <span class=n>SQLDatabaseChain</span><span class=o>.</span><span class=n>from_llm</span><span class=p>(</span><span class=n>llm</span><span class=o>=</span><span class=n>llm</span><span class=p>,</span> <span class=n>db</span><span class=o>=</span><span class=n>db</span><span class=p>,</span> <span class=n>verbose</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>sql_tool</span> <span class=o>=</span> <span class=n>Tool</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>name</span><span class=o>=</span><span class=s1>&#39;Areac DB&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>func</span><span class=o>=</span><span class=n>sql_chain</span><span class=o>.</span><span class=n>run</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>description</span><span class=o>=</span><span class=s2>&#34;This database contains the data related to transits for all gates of the LTZ of Milan, which is called </span><span class=se>\&#34;</span><span class=s2>AreaC</span><span class=se>\&#34;</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34; The most important tables are: xtdpl1_ingressi_aggregated and xtdpl1_varchi.&#34;</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34; The table xtdpl1_ingressi_aggregated contains most important measures, like the number of all transits for each of the gates and for each day of the year.&#34;</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34; The field identifying the Time dimension is &#39;dat_year_month&#39; and it&#39;s NUMERIC with a standard YYYYMM format.&#34;</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34; The field &#39;flg_areac&#39; is BOOLEAN (true/false) and it&#39;s used to identify the actual </span><span class=se>\&#34;</span><span class=s2>AreaC</span><span class=se>\&#34;</span><span class=s2> payed transits.&#34;</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34; The xtdpl1_varchi table contains the gates transcoding. The primary key is the &#39;id&#39; field, identifying the specific gate. The other fields are descriptive attributes.&#34;</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>Since the agent uses the LLM to decide <strong>which</strong> tool to use and <strong>how</strong> to use it <strong>solely based on the tool description</strong>, this approach has the great advantage of improving performance just by adding an effective description of the DB within the tool, <strong>without modifying the LLM model at all</strong>.
In my case, for example, I&rsquo;ve incrementally added a large number of details, experiencing every time a concrete improvement in the tool performance.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://c-daniele.github.io/en/tags/ai/>ai</a></li><li><a href=https://c-daniele.github.io/en/tags/langchain/>langchain</a></li><li><a href=https://c-daniele.github.io/en/tags/agents/>agents</a></li></ul><nav class=paginav><a class=next href=https://c-daniele.github.io/en/posts/2023-07-24-langchain-helloworld-pdf/><span class=title>Next »</span><br><span>LLM - Experimenting LangChain - Part 1</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Langchain pt. 2 - Data Analysis through Agents on twitter" href="https://twitter.com/intent/tweet/?text=Langchain%20pt.%202%20-%20Data%20Analysis%20through%20Agents&amp;url=https%3a%2f%2fc-daniele.github.io%2fen%2fposts%2f2023-08-13-langchain-agents%2f&amp;hashtags=ai%2clangchain%2cagents"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Langchain pt. 2 - Data Analysis through Agents on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fc-daniele.github.io%2fen%2fposts%2f2023-08-13-langchain-agents%2f&amp;title=Langchain%20pt.%202%20-%20Data%20Analysis%20through%20Agents&amp;summary=Langchain%20pt.%202%20-%20Data%20Analysis%20through%20Agents&amp;source=https%3a%2f%2fc-daniele.github.io%2fen%2fposts%2f2023-08-13-langchain-agents%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Langchain pt. 2 - Data Analysis through Agents on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fc-daniele.github.io%2fen%2fposts%2f2023-08-13-langchain-agents%2f&title=Langchain%20pt.%202%20-%20Data%20Analysis%20through%20Agents"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Langchain pt. 2 - Data Analysis through Agents on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fc-daniele.github.io%2fen%2fposts%2f2023-08-13-langchain-agents%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Langchain pt. 2 - Data Analysis through Agents on whatsapp" href="https://api.whatsapp.com/send?text=Langchain%20pt.%202%20-%20Data%20Analysis%20through%20Agents%20-%20https%3a%2f%2fc-daniele.github.io%2fen%2fposts%2f2023-08-13-langchain-agents%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Langchain pt. 2 - Data Analysis through Agents on telegram" href="https://telegram.me/share/url?text=Langchain%20pt.%202%20-%20Data%20Analysis%20through%20Agents&amp;url=https%3a%2f%2fc-daniele.github.io%2fen%2fposts%2f2023-08-13-langchain-agents%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Langchain pt. 2 - Data Analysis through Agents on ycombinator" href="https://news.ycombinator.com/submitlink?t=Langchain%20pt.%202%20-%20Data%20Analysis%20through%20Agents&u=https%3a%2f%2fc-daniele.github.io%2fen%2fposts%2f2023-08-13-langchain-agents%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://c-daniele.github.io/en/>Cdani's Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>