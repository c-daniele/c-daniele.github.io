<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Langchain pt. 3 - Integration of APIs into an AI-powered application | Cdani's Blog</title>
<meta name=keywords content="ai,langchain,api,rest,swagger,openapi"><meta name=description content="Intro Last year, Gartner put Generative AI at the peak of inflated expectations in its AI Hype Cycle.
Recently, big tech leaders compared the hype around GenAI to the dotcom bubble. Furthermore, according to some rumors, the main Cloud Providers are even giving instructions to their Sales Teams to slow down the enthusiasm towards customers regarding GenAI initiatives and promoting cost-vs-benefits awareness. Has the drop into the trough of disillusionment already begun?"><meta name=author content="Me"><link rel=canonical href=https://c-daniele.github.io/en/posts/2024-04-20-langchain-api/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.3598bbf45621a4ad34d093926efeb15d6df27175e085d2f069483f14ad39d7fa.css integrity="sha256-NZi79FYhpK000JOSbv6xXW3ycXXghdLwaUg/FK051/o=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://c-daniele.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://c-daniele.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://c-daniele.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://c-daniele.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://c-daniele.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=it href=https://c-daniele.github.io/it/posts/2024-04-20-langchain-api/><link rel=alternate hreflang=en href=https://c-daniele.github.io/en/posts/2024-04-20-langchain-api/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-8NZQZ3Z1RN"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-8NZQZ3Z1RN",{anonymize_ip:!1})}</script><meta property="og:title" content="Langchain pt. 3 - Integration of APIs into an AI-powered application"><meta property="og:description" content="Intro Last year, Gartner put Generative AI at the peak of inflated expectations in its AI Hype Cycle.
Recently, big tech leaders compared the hype around GenAI to the dotcom bubble. Furthermore, according to some rumors, the main Cloud Providers are even giving instructions to their Sales Teams to slow down the enthusiasm towards customers regarding GenAI initiatives and promoting cost-vs-benefits awareness. Has the drop into the trough of disillusionment already begun?"><meta property="og:type" content="article"><meta property="og:url" content="https://c-daniele.github.io/en/posts/2024-04-20-langchain-api/"><meta property="og:image" content="https://c-daniele.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-04-20T19:00:00+02:00"><meta property="article:modified_time" content="2024-04-20T19:00:00+02:00"><meta property="og:site_name" content="Cdani's blog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://c-daniele.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Langchain pt. 3 - Integration of APIs into an AI-powered application"><meta name=twitter:description content="Intro Last year, Gartner put Generative AI at the peak of inflated expectations in its AI Hype Cycle.
Recently, big tech leaders compared the hype around GenAI to the dotcom bubble. Furthermore, according to some rumors, the main Cloud Providers are even giving instructions to their Sales Teams to slow down the enthusiasm towards customers regarding GenAI initiatives and promoting cost-vs-benefits awareness. Has the drop into the trough of disillusionment already begun?"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":4,"name":"Langchain pt. 3 - Integration of APIs into an AI-powered application","item":"https://c-daniele.github.io/en/posts/2024-04-20-langchain-api/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Langchain pt. 3 - Integration of APIs into an AI-powered application","name":"Langchain pt. 3 - Integration of APIs into an AI-powered application","description":"Intro Last year, Gartner put Generative AI at the peak of inflated expectations in its AI Hype Cycle.\nRecently, big tech leaders compared the hype around GenAI to the dotcom bubble. Furthermore, according to some rumors, the main Cloud Providers are even giving instructions to their Sales Teams to slow down the enthusiasm towards customers regarding GenAI initiatives and promoting cost-vs-benefits awareness. Has the drop into the trough of disillusionment already begun?","keywords":["ai","langchain","api","rest","swagger","openapi"],"articleBody":"Intro Last year, Gartner put Generative AI at the peak of inflated expectations in its AI Hype Cycle.\nRecently, big tech leaders compared the hype around GenAI to the dotcom bubble. Furthermore, according to some rumors, the main Cloud Providers are even giving instructions to their Sales Teams to slow down the enthusiasm towards customers regarding GenAI initiatives and promoting cost-vs-benefits awareness. Has the drop into the trough of disillusionment already begun?\nFigure 1 - Hype Cycle Model Maybe the classic Hype Cycle model is not applicable this time. Compared to other transformative and technological trends, we are moving very quickly towards a phase of awareness and maturity. The market is moving beyond the race for the most powerful model in terms of “brute force” and new market trends arise:\nMany vendors are working on relatively small models that can also be run locally, for example: Meta and Qualcomm have recently accounced a collaboration aimed to optimize the Llama3 models in order to make them executed directly on devices equipped with future top-of-the-range Snapdragon platforms H2O launched a super tiny language model called Danube, which is a fork from Llama2 designed to be to be executed on mobile devices Rumors on Apple reported that they are working on a “on-device” language model which will also be available offline All the big players in the AI market are introducing multi-modal products Several frameworks are emerging for designing modular solutions, using LLM models as building blocks to build complex and vendor-agnostic “AI-powered” applications In other words, to draw a parallel with what happened many years ago with the birth of software engineering, these products are paving the way for “AI Engineering” LangChain is going precisely in this direction. It’s one of the most complete and powerful AI Open Source frameworks at the moment. It provides great control and flexibility for various use cases and offers greater granularity than other frameworks such as, for example, LlamaIndex. One of the features I have tested in recent days is the Rest-API integration, using well-defined standard specifications (e.g. Swagger, OpenApi) or even described in natural language.\nIn this article, I will show how to integrate a third-party API “at runtime” into a very simple chatbot, and query the API in natural language without any prior knowledge about the API specifications.\nTechnical preamble The code shown below, available on GitHub is making use of OpenAI and Bedrock. The latter, for those who don’t know it, is the AWS service that gives access to various models including Llama2, Claude, Mistral and the AWS proprietary model called Titan. The code is extremely simple and can be summarized as the following logical steps:\nEnvironment variable settings LLM initialization API specifications dynamic retrieval Setup and invoke of the APIChain component. This component applies some simple Prompt Engineering techniques to perform the following 3 actions: Take the user’s question in natural language as input and construct, via the LLM, the URL to be invoked Invoke the URL thus built via an HTTP call Wrap the response obtained from the HTTP call into a new LLM invocation and obtain the information requested by the user in terms of natural language. The overall process is summarized into the following flow diagram:\nFigure 2 - Flow Diagram For sake of simplicity, in the code that follows I have hard-coded the user interactions parts, but it’s easy to obtain these inputs dynamically via a dialogic user interation in a Chatbot application. In such a scenario, you could also configure the APIs specifications using a well-defined administration interface and then plug\u0026play directly the API into the chatbot to add features at-runtime.\nIn other words, with very small effort, you can build a chatbot that is completely agnostic with respect to the API specifications and dynamically adapts to the user needs, adding references to new APIs on the fly.\nAs real use case, you can imagine a customer care tool that integrates with company APIs to directly return information related to the customer orders, products, reports, etc.. You can thus develop these features incrementally, while enhancing the capabilities exposed by the chatbot and use a plug\u0026play approach, adding new APIs within the existing dialogic process.\nBroadening the discussion and moving towards a more Enterprise context, we can imagine the scenario of a modern Data Platform that makes the company KPIs available in the form of Data-APIs thus allowing anyone in the company accessing such KPIs via the enterprise chatbot.\nThe APIs The APIs I’ve used to test are the following:\nklarna.com for those who don’t know the brand, Klarna is a Swedish fintech that offers payment processing services for the e-commerce industry. Klarma payment options are usually available on most common online shopping websites. The Klarna API can be accessed for free and allows searching for products based on text description, price, brand, etc.. It is only available in a few countries (US, GB, DE, SE, DK). open-meteo It’s a free API that makes meteorological data available. The most common use case is when we query the API to obtain the weather conditions in a certain city, in terms of temperature, precipitation, visibility, etc. APIChain The main component we are going to use within the LangChain suite is called APIChain. Under the hood, the chain is made of:\nAn instance of an LLMChain, which is used to build the URL and the HTTP parameters from the natural language question A wrapper of the request component, which is used to send the HTTP request An instance of an LLMChain that is used to build the response in natural language, starting from the raw HTTP Response payload Some pre-built prompts that are used to prepare the context and effectively implement invocations of the LLM As regards the prompts that the APIChain component makes available, during the tests I realized that they did not work correctly with all LLMs (for example: they worked with OpenAI, but not with Llama2, Claude, etc). Therefore, I’ve built a slightly better version of such prompts and proposed the change on the official repo (we’ll see if they accept it 😃 ).\nThe test You can find the full source code in the GitHub repository.\nIn the first part of the code I’ve initialized the basic components and created the models.\nSome notes:\nThe environment variables related to integration with OPEN_AI and AWS must be configured in the .env file I’ve created a wrapper for instantiating the LLM model (see the “libs.py” file) Some of the involved AWS services are currently only available in some Regions. Therefore you need to pay attention to the region settings and the costs associated with use 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 from langchain.chains import APIChain from dotenv import load_dotenv import httpx import logging as logger import sys # see \"libs.py\" file from libs import * # see \"prompt_improved.py\" file from prompt_improved import * # Set WARNING Logger levels help print only meaningful text logger.basicConfig(stream=sys.stdout, level=logger.WARNING) logger.getLogger('botocore').setLevel(logger.WARNING) logger.getLogger('httpx').setLevel(logger.WARNING) # loading ENV variables load_dotenv() # Initialize Models gpt35 = create_llm(model={\"provider\":\"OpenAI\", \"value\": \"gpt-3.5-turbo\"}, model_kwargs={\"temperature\": 0.1}) gpt4 = create_llm(model={\"provider\":\"OpenAI\", \"value\": \"gpt-4\"}, model_kwargs={\"temperature\": 0.1}) claude3 = create_llm(model={\"provider\":\"Anthropic\", \"value\": \"anthropic.claude-3-sonnet-20240229-v1:0\"}, model_kwargs={\"temperature\": 0.1}) llama2 = create_llm(model={\"provider\":\"Meta\", \"value\": \"meta.llama2-70b-chat-v1\"}, model_kwargs=None) Ok, now let’s see how to dynamically integrate the interface descriptor and pass it to the APIChain component. The “limit_to_domains” variable is used to introduce a security mechanism that limits the domains to which requests can be directed. You could also set it to “None” to remove such constraints (not recommended). The variables api_url_prompt and api_response_prompt allow you to customize the prompts to be passed to the LLM. As I mentioned previously, I’ve set up 2 custom prompts that proved to be more robust than the default ones.\n26 27 28 29 30 31 32 33 34 35 36 37 38 # Dynamically retrieve swagger output = httpx.get(\"https://www.klarna.com/us/shopping/public/openai/v0/api-docs/\") swagger = output.text # build the APIChain chain = APIChain.from_llm_and_api_docs( llm=gpt4, api_docs=swagger, verbose=False, limit_to_domains=[\"klarna.com\", \"https://www.klarna.com/\", \"https://www.klarna.com\"], api_url_prompt=FINE_TUNED_API_URL_PROMPT, api_response_prompt=FINE_TUNED_API_RESPONSE_PROMPT ) At this point everything is set. We can ask a question and pass it to the framework and then return the output to the end user. I’ve asked to look for 3 t-shirts with a maximum price of 50 dollars and return price, description and the source link.\n39 40 41 42 43 44 45 # Ask a question to the Chain response = chain.invoke( \"Find 3 t-shirts, max 50 USD. For each Product print the Description, the Price and the corresponding URL\" ) # Print the Chain Output print(response['output']) This is the output I got on the first try:\n1. *Product: Polo Ralph Lauren Men's Slim Fit Wicking Crew Undershirts 3-pack - White* *Price: $37.99* *URL: https://www.klarna.com/us/shopping/pl/cl10001/3207134809/Clothing/Polo-Ralph-Lauren-Men-s-Slim-Fit-Wicking-Crew-Undershirts-3-pack-White/?utm_source=openai\u0026ref-site=openai_plugin* 2. *Product: Lacoste Men's T-shirts 3-pack - Black* *Price: $31.90* *URL: https://www.klarna.com/us/shopping/pl/cl10001/3202043025/Clothing/Lacoste-Men-s-T-shirts-3-pack-Black/?utm_source=openai\u0026ref-site=openai_plugin* 3. *Product: SKIMS Cotton Jersey T-shirt* *Price: $48.00* *URL: https://www.klarna.com/us/shopping/pl/cl10001/3202929904/Clothing/SKIMS-Cotton-Jersey-T-shirt/?utm_source=openai\u0026ref-site=openai_plugin* Not bad!\nI did several other tests with the other models and obtained similar performances although, as I expected, GPT4 and Claude3 are on average more precise.\nAs for the second API, the code is practically the same. You just have to modify the reference to the URL descriptor (swagger), the limit_to_domains variable which must be consistent with the API and the user’s question. So, I’m omitting the first part of the Python script.\nWarning: There is no official swagger for this API, so I’ve used the YAML file I’ve found on GitHub. I have noticed that sometimes HTTP calls to GitHub fail. In that case I suggest to try again a couple of times.\n26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 # Dynamically retrieve swagger output = httpx.get(\"https://raw.githubusercontent.com/open-meteo/open-meteo/main/openapi.yml\") meteo_swagger = output.text # build the APIChain chain = APIChain.from_llm_and_api_docs( llm=claude3, api_docs=meteo_swagger, verbose=True, limit_to_domains=None, api_url_prompt=FINE_TUNED_API_URL_PROMPT, api_response_prompt=FINE_TUNED_API_RESPONSE_PROMPT ) # Ask a question to the Chain response = chain.invoke( \"What is the weather like right now in Munich, Germany in degrees Fahrenheit?\" ) # Print the Chain Output print(response['output']) The output with Claude, GPT 3.5 and GPT4 is good as expected. The 2 Langchain calls have built the URL and processed the response, transforming it into natural language.\nThe current weather in Munich, Germany is 45.7°F with a wind speed of 17.7 km/h coming from 264° direction. The same test with Llama2 was unsuccessful as it hallucinated the first call, in which LangChain creates the URL, adding some unexpected parameters.\nBehind the scenes Another super interesting tool from the LangChain suite is called LangSmith, which allows you to monitor and profile all model invocations. In addition to this, it allows you to do many other things, such as:\nadvanced debugging continuous evaluation of the AI application through pre-defined datasets and evaluation criteria tracing annotations, in order to collect user feedback within the application many other features for monitoring and improving LangChain applications Using LangSmith, you can see the overall process and the underlying LLM invocations.\nFigure 3 - Underlying LangChain invocations In the image above you can clearly see the invocation tree, identified by the root “APIChain”, which is made of 2 LLM child chains, each one calling the OpenAI Chain. You can also see useful information like the number of used tokens and the estimated cost for each LLM invocation.\nIf you click on the items, you can also see the actual prompt and the response for each LLM invocation.\nFigure 4 - URL building step Figure 5 - Final prompt and summary of the response in natural language Final thoughts If you take a look at the LangChain source code and LangSmith profiling tools you can clearly see there is no rocket science under the hood, cause it’s mostly implemented through Prompt Engineering techniques. Nevertheless these tecniques allow extremely powerful integration between new AI applications and traditional systems.\nIn my opinion, it is one of the clearest examples of how today we can (and perhaps we should) review the human/machine interaction in terms of integration between well-specified formal systems with predictable behavior (e.g. any traditional software system in the company) and natural language.\nLangChain and other frameworks allow you to do something similar even at a lower level, for example by querying a database in natural language and using an LLM to generate the underlying queries. Even ignoring performance and scalability issues, this approach is good in theory but, based on my experience, there are several practical problems that make me think it is not really applicable but in some specific scenarios, since in most cases you’ll find application layering and poor or missing data catalogs. Conversely, enterprise APIs usually speak a Business-related language and have self-descriptive metadatas.\n","wordCount":"2109","inLanguage":"en","datePublished":"2024-04-20T19:00:00+02:00","dateModified":"2024-04-20T19:00:00+02:00","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://c-daniele.github.io/en/posts/2024-04-20-langchain-api/"},"publisher":{"@type":"Organization","name":"Cdani's Blog","logo":{"@type":"ImageObject","url":"https://c-daniele.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://c-daniele.github.io/en/ accesskey=h title="Home (Alt + H)"><img src=https://c-daniele.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://c-daniele.github.io/it/ title=Italiano aria-label=Italiano>It</a></li></ul></div></div><ul id=menu><li><a href=https://c-daniele.github.io/en/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://c-daniele.github.io/en/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://c-daniele.github.io/en/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://c-daniele.github.io/en/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://c-daniele.github.io/en/>Home</a></div><h1 class=post-title>Langchain pt. 3 - Integration of APIs into an AI-powered application</h1><div class=post-meta>&lt;span title='2024-04-20 19:00:00 +0200 CEST'>April 20, 2024&lt;/span>&amp;nbsp;·&amp;nbsp;10 min&amp;nbsp;·&amp;nbsp;2109 words&amp;nbsp;·&amp;nbsp;Me&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://c-daniele.github.io/it/posts/2024-04-20-langchain-api/>It</a></li></ul></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#intro>Intro</a></li><li><a href=#technical-preamble>Technical preamble</a></li><li><a href=#the-apis>The APIs</a></li><li><a href=#apichain>APIChain</a></li><li><a href=#the-test>The test</a></li><li><a href=#behind-the-scenes>Behind the scenes</a></li><li><a href=#final-thoughts>Final thoughts</a></li></ul></nav></div></details></div><div class=post-content><h2 id=intro>Intro<a hidden class=anchor aria-hidden=true href=#intro>#</a></h2><p>Last year, Gartner put Generative AI at the peak of inflated expectations in its AI <a href=https://www.gartner.com/en/articles/what-s-new-in-artificial-intelligence-from-the-2023-gartner-hype-cycle>Hype Cycle</a>.</p><p>Recently, big tech leaders <a href=https://www.wired.com/story/amazons-cloud-boss-selipsky-generative-ai-hype/>compared the hype around GenAI to the <em>dotcom</em> bubble</a>.
Furthermore, according to some <a href="https://www.theinformation.com/articles/generative-ai-providers-quietly-tamp-down-expectations?ref=wheresyoured.at">rumors</a>, the main Cloud Providers are even giving instructions to their Sales Teams to slow down the enthusiasm towards customers regarding GenAI initiatives and promoting cost-vs-benefits awareness.
Has the drop into the <a href=https://it.wikipedia.org/wiki/Hype_cycle>trough of disillusionment</a> already begun?</p><p><figure><a href=/images/20240420/Hype-Cycle-General.png><img src=/images/20240420/Hype-Cycle-General.png alt="Figure 1 - Hype Cycle Model"></a><figcaption>Figure 1 - Hype Cycle Model</figcaption></figure></p><p>Maybe the classic Hype Cycle model is not applicable this time.
Compared to other transformative and technological trends, we are moving very quickly towards a phase of awareness and maturity.
The market is moving beyond the race for the most powerful model in terms of &ldquo;brute force&rdquo; and new market trends arise:</p><ul><li>Many vendors are working on relatively small models that can also be run locally, for example:<ul><li>Meta and Qualcomm have recently <a href=https://www.qualcomm.com/news/releases/2024/04/qualcomm-enables-meta-llama-3-to-run-on-devices-powered-by-snapd>accounced a collaboration</a> aimed to optimize the <em>Llama3</em> models in order to make them executed directly on devices equipped with future top-of-the-range Snapdragon platforms</li><li>H2O launched a super tiny language model called <a href=https://venturebeat.com/ai/h2o-ai-releases-danube-a-super-tiny-llm-for-mobile-applications/><em>Danube</em></a>, which is a fork from <em>Llama2</em> designed to be to be executed on mobile devices</li><li><a href=https://www.bloomberg.com/news/newsletters/2024-04-21/apple-aapl-growth-opportunities-southeast-asia-and-africa-lower-end-iphone-lv9itkna>Rumors on Apple</a> reported that they are working on a &ldquo;<em>on-device</em>&rdquo; language model which will also be available offline</li></ul></li><li>All the big players in the AI market are introducing multi-modal products</li><li>Several frameworks are emerging for designing modular solutions, using LLM models as building blocks to build complex and vendor-agnostic <em>&ldquo;AI-powered&rdquo;</em> applications<ul><li>In other words, to draw a parallel with what happened many years ago with the birth of software engineering, these products are paving the way for <strong>&ldquo;AI Engineering&rdquo;</strong></li></ul></li></ul><p>LangChain is going precisely in this direction.
It&rsquo;s one of the most complete and powerful AI Open Source frameworks at the moment. It provides great control and flexibility for various use cases and offers greater granularity than other frameworks such as, for example, <em>LlamaIndex</em>.
One of the features I have tested in recent days is the Rest-API integration, using well-defined standard specifications (e.g. <em>Swagger</em>, <em>OpenApi</em>) or even described in natural language.</p><p>In this article, I will show how to integrate a third-party API &ldquo;at runtime&rdquo; into a very simple chatbot, and query the API in natural language <strong>without any prior knowledge</strong> about the API specifications.</p><h2 id=technical-preamble>Technical preamble<a hidden class=anchor aria-hidden=true href=#technical-preamble>#</a></h2><p>The code shown below, <a href=https://github.com/c-daniele/langchain_tests/tree/main/01.APIChain>available on GitHub</a> is making use of <em>OpenAI</em> and <em>Bedrock</em>. The latter, for those who don&rsquo;t know it, is the AWS service that gives access to various models including <em>Llama2</em>, <em>Claude</em>, <em>Mistral</em> and the AWS proprietary model called <em>Titan</em>.
The code is extremely simple and can be summarized as the following logical steps:</p><ol><li>Environment variable settings</li><li>LLM initialization</li><li>API specifications dynamic retrieval</li><li>Setup and invoke of the <em>APIChain</em> component. This component applies some simple Prompt Engineering techniques to perform the following 3 actions:<ol><li>Take the user&rsquo;s question in natural language as input and construct, via the LLM, the URL to be invoked</li><li>Invoke the URL thus built via an HTTP call</li><li>Wrap the response obtained from the HTTP call into a new LLM invocation and obtain the information requested by the user in terms of natural language.</li></ol></li></ol><p>The overall process is summarized into the following flow diagram:</p><p><figure><a href=/images/20240420/FlowChart-APIChain-v2.png><img src=/images/20240420/FlowChart-APIChain-v2.png alt="Figure 2 - Flow Diagram"></a><figcaption>Figure 2 - Flow Diagram</figcaption></figure></p><p>For sake of simplicity, in the code that follows I have hard-coded the user interactions parts, but it&rsquo;s easy to obtain these inputs dynamically via a dialogic user interation in a Chatbot application.
In such a scenario, you could also configure the APIs specifications using a well-defined administration interface and then plug&amp;play directly the API into the chatbot to <strong>add features at-runtime</strong>.</p><p>In other words, with very small effort, you can build a <strong>chatbot that is completely agnostic</strong> with respect to the API specifications and dynamically adapts to the user needs, adding references to new APIs on the fly.</p><p>As real use case, you can imagine a <strong>customer care</strong> tool that integrates with company APIs to directly return information related to the customer orders, products, reports, etc.. You can thus develop these features incrementally, while enhancing the capabilities exposed by the chatbot and use a <strong>plug&amp;play</strong> approach, adding new APIs within the existing dialogic process.</p><p>Broadening the discussion and moving towards a more Enterprise context, we can imagine the scenario of a modern Data Platform that makes the company KPIs available in the form of <em>Data-APIs</em> thus allowing anyone in the company accessing such KPIs via the enterprise chatbot.</p><h2 id=the-apis>The APIs<a hidden class=anchor aria-hidden=true href=#the-apis>#</a></h2><p>The APIs I&rsquo;ve used to test are the following:</p><ul><li><em><a href=https://www.klarna.com/us/shopping/public/openai/v0/api-docs/>klarna.com</a></em><ul><li>for those who don&rsquo;t know the brand, Klarna is a Swedish fintech that offers payment processing services for the e-commerce industry. Klarma payment options are usually available on most common online shopping websites. The Klarna API can be accessed for free and allows searching for products based on text description, price, brand, etc.. It is only available in a few countries (US, GB, DE, SE, DK).</li></ul></li><li><em><a href=https://raw.githubusercontent.com/open-meteo/open-meteo/main/openapi.yml>open-meteo</a></em><ul><li>It&rsquo;s a free API that makes meteorological data available. The most common use case is when we query the API to obtain the weather conditions in a certain city, in terms of temperature, precipitation, visibility, etc.</li></ul></li></ul><h2 id=apichain>APIChain<a hidden class=anchor aria-hidden=true href=#apichain>#</a></h2><p>The main component we are going to use within the LangChain suite is called <em>APIChain</em>. Under the hood, the chain is made of:</p><ul><li>An instance of an <em>LLMChain</em>, which is used to build the URL and the HTTP parameters from the natural language question</li><li>A wrapper of the <em>request</em> component, which is used to send the HTTP request</li><li>An instance of an <em>LLMChain</em> that is used to build the response in natural language, starting from the raw HTTP Response payload</li><li>Some pre-built prompts that are used to prepare the context and effectively implement invocations of the LLM</li></ul><p>As regards the prompts that the APIChain component makes available, during the tests I realized that they did not work correctly with all LLMs (for example: they worked with OpenAI, but not with Llama2, Claude, etc). Therefore, I&rsquo;ve built a slightly better version of such prompts and proposed the change on the official repo (we&rsquo;ll see if they accept it &#x1f603; ).</p><h2 id=the-test>The test<a hidden class=anchor aria-hidden=true href=#the-test>#</a></h2><p>You can find the full <a href=https://github.com/c-daniele/langchain_tests/tree/main/01.APIChain>source code in the GitHub</a> repository.</p><p>In the first part of the code I&rsquo;ve initialized the basic components and created the models.</p><p>Some notes:</p><ul><li>The environment variables related to integration with OPEN_AI and AWS must be configured in the <em>.env</em> file</li><li>I&rsquo;ve created a wrapper for instantiating the LLM model (see the <em>&ldquo;libs.py&rdquo;</em> file)</li><li>Some of the involved AWS services are currently only available in some Regions. Therefore you need to pay attention to the region settings and the costs associated with use</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>langchain.chains</span> <span class=kn>import</span> <span class=n>APIChain</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>dotenv</span> <span class=kn>import</span> <span class=n>load_dotenv</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>httpx</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>logging</span> <span class=k>as</span> <span class=nn>logger</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>sys</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># see &#34;libs.py&#34; file</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>libs</span> <span class=kn>import</span> <span class=o>*</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># see &#34;prompt_improved.py&#34; file</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>prompt_improved</span> <span class=kn>import</span> <span class=o>*</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Set WARNING Logger levels help print only meaningful text</span>
</span></span><span class=line><span class=cl><span class=n>logger</span><span class=o>.</span><span class=n>basicConfig</span><span class=p>(</span><span class=n>stream</span><span class=o>=</span><span class=n>sys</span><span class=o>.</span><span class=n>stdout</span><span class=p>,</span> <span class=n>level</span><span class=o>=</span><span class=n>logger</span><span class=o>.</span><span class=n>WARNING</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>logger</span><span class=o>.</span><span class=n>getLogger</span><span class=p>(</span><span class=s1>&#39;botocore&#39;</span><span class=p>)</span><span class=o>.</span><span class=n>setLevel</span><span class=p>(</span><span class=n>logger</span><span class=o>.</span><span class=n>WARNING</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>logger</span><span class=o>.</span><span class=n>getLogger</span><span class=p>(</span><span class=s1>&#39;httpx&#39;</span><span class=p>)</span><span class=o>.</span><span class=n>setLevel</span><span class=p>(</span><span class=n>logger</span><span class=o>.</span><span class=n>WARNING</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># loading ENV variables</span>
</span></span><span class=line><span class=cl><span class=n>load_dotenv</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Initialize Models</span>
</span></span><span class=line><span class=cl><span class=n>gpt35</span> <span class=o>=</span> <span class=n>create_llm</span><span class=p>(</span><span class=n>model</span><span class=o>=</span><span class=p>{</span><span class=s2>&#34;provider&#34;</span><span class=p>:</span><span class=s2>&#34;OpenAI&#34;</span><span class=p>,</span> <span class=s2>&#34;value&#34;</span><span class=p>:</span> <span class=s2>&#34;gpt-3.5-turbo&#34;</span><span class=p>},</span> <span class=n>model_kwargs</span><span class=o>=</span><span class=p>{</span><span class=s2>&#34;temperature&#34;</span><span class=p>:</span> <span class=mf>0.1</span><span class=p>})</span>
</span></span><span class=line><span class=cl><span class=n>gpt4</span> <span class=o>=</span> <span class=n>create_llm</span><span class=p>(</span><span class=n>model</span><span class=o>=</span><span class=p>{</span><span class=s2>&#34;provider&#34;</span><span class=p>:</span><span class=s2>&#34;OpenAI&#34;</span><span class=p>,</span> <span class=s2>&#34;value&#34;</span><span class=p>:</span> <span class=s2>&#34;gpt-4&#34;</span><span class=p>},</span> <span class=n>model_kwargs</span><span class=o>=</span><span class=p>{</span><span class=s2>&#34;temperature&#34;</span><span class=p>:</span> <span class=mf>0.1</span><span class=p>})</span>
</span></span><span class=line><span class=cl><span class=n>claude3</span> <span class=o>=</span> <span class=n>create_llm</span><span class=p>(</span><span class=n>model</span><span class=o>=</span><span class=p>{</span><span class=s2>&#34;provider&#34;</span><span class=p>:</span><span class=s2>&#34;Anthropic&#34;</span><span class=p>,</span> <span class=s2>&#34;value&#34;</span><span class=p>:</span> <span class=s2>&#34;anthropic.claude-3-sonnet-20240229-v1:0&#34;</span><span class=p>},</span> <span class=n>model_kwargs</span><span class=o>=</span><span class=p>{</span><span class=s2>&#34;temperature&#34;</span><span class=p>:</span> <span class=mf>0.1</span><span class=p>})</span>
</span></span><span class=line><span class=cl><span class=n>llama2</span> <span class=o>=</span> <span class=n>create_llm</span><span class=p>(</span><span class=n>model</span><span class=o>=</span><span class=p>{</span><span class=s2>&#34;provider&#34;</span><span class=p>:</span><span class=s2>&#34;Meta&#34;</span><span class=p>,</span> <span class=s2>&#34;value&#34;</span><span class=p>:</span> <span class=s2>&#34;meta.llama2-70b-chat-v1&#34;</span><span class=p>},</span> <span class=n>model_kwargs</span><span class=o>=</span><span class=kc>None</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>Ok, now let&rsquo;s see how to dynamically integrate the interface descriptor and pass it to the APIChain component.
The <em>&ldquo;limit_to_domains&rdquo;</em> variable is used to introduce a security mechanism that limits the domains to which requests can be directed. You could also set it to &ldquo;None&rdquo; to remove such constraints (not recommended).
The variables <em>api_url_prompt</em> and <em>api_response_prompt</em> allow you to customize the prompts to be passed to the LLM. As I mentioned previously, I&rsquo;ve set up 2 custom prompts that proved to be more robust than the default ones.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Dynamically retrieve swagger</span>
</span></span><span class=line><span class=cl><span class=n>output</span> <span class=o>=</span> <span class=n>httpx</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;https://www.klarna.com/us/shopping/public/openai/v0/api-docs/&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>swagger</span> <span class=o>=</span> <span class=n>output</span><span class=o>.</span><span class=n>text</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># build the APIChain </span>
</span></span><span class=line><span class=cl><span class=n>chain</span> <span class=o>=</span> <span class=n>APIChain</span><span class=o>.</span><span class=n>from_llm_and_api_docs</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>llm</span><span class=o>=</span><span class=n>gpt4</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>api_docs</span><span class=o>=</span><span class=n>swagger</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>verbose</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>limit_to_domains</span><span class=o>=</span><span class=p>[</span><span class=s2>&#34;klarna.com&#34;</span><span class=p>,</span> <span class=s2>&#34;https://www.klarna.com/&#34;</span><span class=p>,</span> <span class=s2>&#34;https://www.klarna.com&#34;</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=n>api_url_prompt</span><span class=o>=</span><span class=n>FINE_TUNED_API_URL_PROMPT</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>api_response_prompt</span><span class=o>=</span><span class=n>FINE_TUNED_API_RESPONSE_PROMPT</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>At this point everything is set. We can ask a question and pass it to the framework and then return the output to the end user.
I&rsquo;ve asked to look for 3 t-shirts with a maximum price of 50 dollars and return price, description and the source link.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Ask a question to the Chain</span>
</span></span><span class=line><span class=cl><span class=n>response</span> <span class=o>=</span> <span class=n>chain</span><span class=o>.</span><span class=n>invoke</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;Find 3 t-shirts, max 50 USD. For each Product print the Description, the Price and the corresponding URL&#34;</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Print the Chain Output</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>response</span><span class=p>[</span><span class=s1>&#39;output&#39;</span><span class=p>])</span>
</span></span></code></pre></td></tr></table></div></div><p>This is the output I got on the first try:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>1. *Product: Polo Ralph Lauren Men&#39;s Slim Fit Wicking Crew Undershirts 3-pack - White*
</span></span><span class=line><span class=cl>   *Price: $37.99*
</span></span><span class=line><span class=cl>   *URL: https://www.klarna.com/us/shopping/pl/cl10001/3207134809/Clothing/Polo-Ralph-Lauren-Men-s-Slim-Fit-Wicking-Crew-Undershirts-3-pack-White/?utm_source=openai&amp;ref-site=openai_plugin*
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>2. *Product: Lacoste Men&#39;s T-shirts 3-pack - Black*
</span></span><span class=line><span class=cl>   *Price: $31.90*
</span></span><span class=line><span class=cl>   *URL: https://www.klarna.com/us/shopping/pl/cl10001/3202043025/Clothing/Lacoste-Men-s-T-shirts-3-pack-Black/?utm_source=openai&amp;ref-site=openai_plugin*
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>3. *Product: SKIMS Cotton Jersey T-shirt*
</span></span><span class=line><span class=cl>   *Price: $48.00*
</span></span><span class=line><span class=cl>   *URL: https://www.klarna.com/us/shopping/pl/cl10001/3202929904/Clothing/SKIMS-Cotton-Jersey-T-shirt/?utm_source=openai&amp;ref-site=openai_plugin*
</span></span></code></pre></div><p>Not bad!</p><p>I did several other tests with the other models and obtained similar performances although, as I expected, GPT4 and Claude3 are on average more precise.</p><p>As for the second API, the code is practically the same. You just have to modify the reference to the URL descriptor (swagger), the <em>limit_to_domains</em> variable which must be consistent with the API and the user&rsquo;s question.
So, I&rsquo;m omitting the first part of the Python script.</p><p><em>Warning</em>: There is no official swagger for this API, so I&rsquo;ve used the YAML file I&rsquo;ve found on GitHub. I have noticed that sometimes HTTP calls to GitHub fail. In that case I suggest to try again a couple of times.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Dynamically retrieve swagger</span>
</span></span><span class=line><span class=cl><span class=n>output</span> <span class=o>=</span> <span class=n>httpx</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;https://raw.githubusercontent.com/open-meteo/open-meteo/main/openapi.yml&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>meteo_swagger</span> <span class=o>=</span> <span class=n>output</span><span class=o>.</span><span class=n>text</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># build the APIChain </span>
</span></span><span class=line><span class=cl><span class=n>chain</span> <span class=o>=</span> <span class=n>APIChain</span><span class=o>.</span><span class=n>from_llm_and_api_docs</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>llm</span><span class=o>=</span><span class=n>claude3</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>api_docs</span><span class=o>=</span><span class=n>meteo_swagger</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>verbose</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>limit_to_domains</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>api_url_prompt</span><span class=o>=</span><span class=n>FINE_TUNED_API_URL_PROMPT</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>api_response_prompt</span><span class=o>=</span><span class=n>FINE_TUNED_API_RESPONSE_PROMPT</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># Ask a question to the Chain</span>
</span></span><span class=line><span class=cl><span class=n>response</span> <span class=o>=</span> <span class=n>chain</span><span class=o>.</span><span class=n>invoke</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;What is the weather like right now in Munich, Germany in degrees Fahrenheit?&#34;</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Print the Chain Output</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>response</span><span class=p>[</span><span class=s1>&#39;output&#39;</span><span class=p>])</span>
</span></span></code></pre></td></tr></table></div></div><p>The output with Claude, GPT 3.5 and GPT4 is good as expected. The 2 Langchain calls have built the URL and processed the response, transforming it into natural language.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>The current weather in Munich, Germany is 45.7°F with a wind speed of 17.7 km/h coming from 264° direction.
</span></span></code></pre></div><p>The same test with Llama2 was unsuccessful as it hallucinated the first call, in which LangChain creates the URL, adding some unexpected parameters.</p><h2 id=behind-the-scenes>Behind the scenes<a hidden class=anchor aria-hidden=true href=#behind-the-scenes>#</a></h2><p>Another super interesting tool from the LangChain suite is called <a href=https://www.langchain.com/langsmith><em>LangSmith</em></a>, which allows you to monitor and profile all model invocations.
In addition to this, it allows you to do many other things, such as:</p><ul><li>advanced debugging</li><li>continuous evaluation of the AI application through pre-defined datasets and evaluation criteria</li><li>tracing annotations, in order to collect user feedback within the application</li><li>many other features for monitoring and improving LangChain applications</li></ul><p>Using LangSmith, you can see the overall process and the underlying LLM invocations.</p><p><figure><a href=/images/20240420/langsmith_input_root_calls.png><img src=/images/20240420/langsmith_input_root_calls.png alt="Figure 3 - Underlying LangChain invocations"></a><figcaption>Figure 3 - Underlying LangChain invocations</figcaption></figure></p><p>In the image above you can clearly see the invocation tree, identified by the root <em>&ldquo;APIChain&rdquo;</em>, which is made of 2 LLM child chains, each one calling the OpenAI Chain.
You can also see useful information like the <em>number of used tokens</em> and the <em>estimated cost</em> for each LLM invocation.</p><p>If you click on the items, you can also see the actual prompt and the response for each LLM invocation.</p><p><figure><a href=/images/20240420/langsmith_input_00.png><img src=/images/20240420/langsmith_input_00.png alt="Figure 4 - URL building step"></a><figcaption>Figure 4 - URL building step</figcaption></figure></p><p><figure><a href=/images/20240420/langsmith_input_01.png><img src=/images/20240420/langsmith_input_01.png alt="Figure 5 - Final prompt and summary of the response in natural language"></a><figcaption>Figure 5 - Final prompt and summary of the response in natural language</figcaption></figure></p><h2 id=final-thoughts>Final thoughts<a hidden class=anchor aria-hidden=true href=#final-thoughts>#</a></h2><p>If you take a look at the LangChain source code and LangSmith profiling tools you can clearly see there is no rocket science under the hood, cause it&rsquo;s mostly implemented through Prompt Engineering techniques. Nevertheless these tecniques allow extremely powerful integration between new AI applications and traditional systems.</p><p>In my opinion, it is one of the clearest examples of how today we can (and perhaps we should) <strong>review the human/machine interaction</strong> in terms of integration between well-specified formal systems with predictable behavior (e.g. any traditional software system in the company) and natural language.</p><p>LangChain and other frameworks allow you to do something similar even at a lower level, for example by querying a database in natural language and using an LLM to generate the underlying queries.
Even ignoring performance and scalability issues, this approach is good in theory but, based on my experience, there are several practical problems that make me think it is not really applicable but in some specific scenarios, since in most cases you&rsquo;ll find application layering and poor or missing data catalogs.
Conversely, enterprise APIs usually speak a Business-related language and have self-descriptive metadatas.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://c-daniele.github.io/en/tags/ai/>ai</a></li><li><a href=https://c-daniele.github.io/en/tags/langchain/>langchain</a></li><li><a href=https://c-daniele.github.io/en/tags/api/>api</a></li><li><a href=https://c-daniele.github.io/en/tags/rest/>rest</a></li><li><a href=https://c-daniele.github.io/en/tags/swagger/>swagger</a></li><li><a href=https://c-daniele.github.io/en/tags/openapi/>openapi</a></li></ul><nav class=paginav><a class=next href=https://c-daniele.github.io/en/posts/2023-08-13-langchain-agents/><span class=title>Next »</span><br><span>Langchain pt. 2 - Data Analysis through Agents</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Langchain pt. 3 - Integration of APIs into an AI-powered application on twitter" href="https://twitter.com/intent/tweet/?text=Langchain%20pt.%203%20-%20Integration%20of%20APIs%20into%20an%20AI-powered%20application&amp;url=https%3a%2f%2fc-daniele.github.io%2fen%2fposts%2f2024-04-20-langchain-api%2f&amp;hashtags=ai%2clangchain%2capi%2crest%2cswagger%2copenapi"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Langchain pt. 3 - Integration of APIs into an AI-powered application on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fc-daniele.github.io%2fen%2fposts%2f2024-04-20-langchain-api%2f&amp;title=Langchain%20pt.%203%20-%20Integration%20of%20APIs%20into%20an%20AI-powered%20application&amp;summary=Langchain%20pt.%203%20-%20Integration%20of%20APIs%20into%20an%20AI-powered%20application&amp;source=https%3a%2f%2fc-daniele.github.io%2fen%2fposts%2f2024-04-20-langchain-api%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Langchain pt. 3 - Integration of APIs into an AI-powered application on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fc-daniele.github.io%2fen%2fposts%2f2024-04-20-langchain-api%2f&title=Langchain%20pt.%203%20-%20Integration%20of%20APIs%20into%20an%20AI-powered%20application"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Langchain pt. 3 - Integration of APIs into an AI-powered application on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fc-daniele.github.io%2fen%2fposts%2f2024-04-20-langchain-api%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Langchain pt. 3 - Integration of APIs into an AI-powered application on whatsapp" href="https://api.whatsapp.com/send?text=Langchain%20pt.%203%20-%20Integration%20of%20APIs%20into%20an%20AI-powered%20application%20-%20https%3a%2f%2fc-daniele.github.io%2fen%2fposts%2f2024-04-20-langchain-api%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Langchain pt. 3 - Integration of APIs into an AI-powered application on telegram" href="https://telegram.me/share/url?text=Langchain%20pt.%203%20-%20Integration%20of%20APIs%20into%20an%20AI-powered%20application&amp;url=https%3a%2f%2fc-daniele.github.io%2fen%2fposts%2f2024-04-20-langchain-api%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Langchain pt. 3 - Integration of APIs into an AI-powered application on ycombinator" href="https://news.ycombinator.com/submitlink?t=Langchain%20pt.%203%20-%20Integration%20of%20APIs%20into%20an%20AI-powered%20application&u=https%3a%2f%2fc-daniele.github.io%2fen%2fposts%2f2024-04-20-langchain-api%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://c-daniele.github.io/en/>Cdani's Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>